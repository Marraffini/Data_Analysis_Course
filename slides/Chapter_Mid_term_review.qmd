---
title: "Review Chapters 1-5"
image: img/reg.png
format: 
  revealjs:
    width: 1050
    height:	700
    scrollable: true
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: img/L_Color.png
    footer: "[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)"
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
    
execute:
  echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = FALSE, 
                      include=TRUE, 
                      message=FALSE, 
                      comment = NA, 
                      warn=-1, 
                      warn.conflicts = FALSE, 
                      quietly=TRUE,
                      fig.align="center")
```
## Chapter 1: Data Collection
-   Types of data
-   Data collection/sampling design
-   Sample vs population
-   Bias

## Types of data

![](img/data_types.png){.preview-image}

## Data Collection: Survey, Experiment, Census 

::::{.columns}
:::{.column width="65%"}
-   Most datasets contain a subset, a **sample**, of a much bigger population of interest.
      
      +   We may conduct a **survey** to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.
      
      +   We might conduct an **experiment** where we take a sample of elements (people, organisms, objects) and apply some **treatment** in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.

-   **Census**: *every* element of the population of interest is represented in the dataset.

:::

:::{.column width="35%"}

![](img/sampling_variation2.gif)

:::

::::

## Sampling Design
-   Simple random sampling 
-   Stratified random sampling
-   Cluster sampling
-   Systematic random sampling

## Summary

-   Issues to address

    -   WHAT are collected?
    -   WHO does the data collection?
    -   HOW are the data collected?

-   Bias occurs due to

    -   SELECTION
    -   COLLECTION
    -   NON-RESPONSE (the single largest cause of bias!)

-   A sample may have the same biases as a census along with *sampling errors*

## Chapter 2: Exploratory Data Analysis
-   Hypothesis generative vs testing
-   Plots

## Categorical Data
-   Bar plots
-   Histograms
-   Frequency ploygon/kernal density
-   Boxplots
-   Cumulative frequency graphs

## Summary Statistics
-   Five number: min, lower hinge, median, upper hinge, max

## Bivatiate relationships
-   Scatter plots
-   Marginal plots
-   pairs
-   Correlation plots
-   Network plots
-   Contour plots

## More R resources 

-   R cookbook https://rc2e.com/
-   https://r-graph-gallery.com/
-   https://bookdown.org/ansellbr/WEHI_tidyR_course_book/

And so many more on the internet

## Summary

-   **Size**

    -   For small datasets, we cannot be too confident in any patterns we see. More likely for patterns to occur 'by chance'.
    -   Some displays are more affected by sample size than others

-   **Shape**

    -   In can be interesting to display the overall shape of distribution.
    -   Are there gaps and/or many peaks (modes)?
    -   Is the distribution `symmetrical`? Is the distribution `normal`?

-   **Outliers**

    -   Boxplots & scatterplots can reveal outliers
    -   More influential than points in the middle
    

-   Graphs should be simple and informative; certainly not misleading!

## Chapter 3: Probability Concepts and Distributions

## Probability and randomness

**Probability and randomness are placeholders for incomplete knowledge.** 

-   A probability is a number between 0 and 1 that quantifies the uncertainty in the occurance of an event

  -   0 = impossibility
  -   1 = certainty 

## Relative frequency of an event

Toss a coin

-   The possible outcomes are heads or tails
-   If you toss a coin 3 times what is the probability of getting 2 heads?
  -   possible ways to get 2 heads: HHT, HTH, THH 
  -   possible combinations of 3 tosses: HHT, HTH, THH, TTH, THT, HTT
  -   probability of getting 2 heads out of 3 tosses = 3/6 = 1/2 = 0.5
  
Roll a 6 sided die

-   There are 6 possible outcomes
-   What is the probability of getting a number 4 or higher?
  - of the 6 possible rolls want 4, 5, or 6
  - probability is 3/6 p = 0.5

When the sample space is finite and outcomes are equally likely classical probability will be the same as empirical probability

-    i.e we don't have to actually toss the coin or roll the die

## Probability Rules

For any event A, probability of A P(A) must satisfy:

-   $0 \le P(A) \le 1 $
-   P(S) = 1 for sample space S
-   Complement of event A is the event that A does not occur $P(A^c) = 1 -P(A)$
-   Events A and B are mutually exclusive if they have no outcomes in common (cannot occur together) $P(A \cup B) = P(A) + P(B)$, where $P(A \cup B)$ is the event A *OR* B occurs
-   If event A and B are not mutually exclusive $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, where $P(A \cap B)$ is the event A *AND* B occurs

## Independence 
-   Events A and B are independent if the occurence of one tells you nothing about whether the other occurred
-   $P(A \cap B)$ = $P(A)P(B)$
-   Two events are independent if $P(A|B) = P(A)$ and $P(B|A) = B$
-   This can extend to three or more events
-   The probability of observing two independent events $P(A and B) = P(A|B) X P(B)$ and $P(A and B) = P(B|A) X P(A)$

## Conditional Probability
-   Probability of an event conditional on occurrence of another event
-   $P(A|B)$ read as 'the probability of A given that B has occurred'
-   Properties:
  -   $P(A \cap B)$ = $P(A|B)P(B)$, where $P(A \cap B)$ is the probability of both A and B occurring
  -   Little bit of rearranging: $P(A|B) =\frac{P(A \cap B)}{P(B)}$ as long as $P(B)>0$
  -   If A and B are independent then $P(A \cap B)$ = $P(A)P(B)$ and so $P(A|B) = P(A)$. If $P(A|B) \ne P(A)$ then A and B *are* dependent. 

## Bayes rule

::: {align="center"}
$P(A|B) =\frac{P(B|A)P(A)}{P(B)}$
:::

## Garden of Forking Data

**Rules of the Probability Tree**

1.  Within each level, all branches are mutually exclusive events.

2.  The tree covers all possibilities (i.e., the entire sample space).

3.  We multiply as we move along branches.

4.  We add when we move across branches.



## Probability Distributions

**Discrete**

-   Binomial: collection of Bernoulli trials; success/failure
-   Poisson: integers; counts of relatively rare events that occure independently

**Continuous**

-   Normal/Gaussian: parameterized by mean and variance
  -   Standard Normal Distribution $\mu = 0$ and $\sigma = 0$
-   T distrinution/Students T: like a normal but with fatter tails
-   Log-normal: log(x) follows a normal distribution
-   Weibull: uses a scale parameter and shape parameter 
-   Gamma: also uses a scale parameter and shape parameter, bounded by 0 
-   Beta: bounded [0,1] and parameterized by two positive shape parameters

## What distribution is most appropriate?

-   Remember, theoretical distributions aren't real—they're just models—but they can be useful. Keep your purpose in mind. 

-   Choose the simplest distribution that provides an adequate fit. 

-   Data may be best served by a mixture of two or more distributions rather than a single distribution.

## Chapter 4 Statistical Inference

The term *statistical inference* means that we are using properties of a sample to make statements about the population from which the sample was drawn. 

![](img/inference.png)

## Summary statistics

-   Sample mean 
-   Sample variation 
-   Confidence intervals

## Testing Hypotheses

- A hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.
- For logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:
   + specify a ***null hypotheses***, which must be true if our hypothesis of interest is false, and 
   + measure the evidence *against* the null hypothesis, usually in the form of a *p*-value.

## Truth and outcomes

In reality, the null hypothesis is either true or false. 

The outcome of a test is either we reject the null hypothesis, or we fail to reject the null hypothesis (note, we never "confirm" or "accept" the null hypothesis – absence of evidence is not evidence of absence!). 

This gives us four possibilities:

|                          | *$H_0$ is true*  | *$H_0$ is false*  |
|--------------------------|------------------|-------------------|
| **Reject $H_0$**         | Type I error     | Correct result    |
| **Do not reject $H_0$**  | Correct result   | Type II error     |


With probabilities usually represented by:

|                          | *$H_0$ is true*  | *$H_0$ is false*    |
|--------------------------|------------------|---------------------|
| **Reject $H_0$**         | $\alpha$         | $1-\beta$ = "power" |
| **Do not reject $H_0$**  | $1 - \alpha$     | $\beta$             |

## Sampling distributions and test statistics

-   T distribution
-   Chi^2
-   F distribution

# T Tests

-   One sample: Can be used to test what the mean is or if that distribution is normally distributed\
        Null hypothesis: true mean *is* equal to 0
        Alternative hypothesis: true mean *is not* equal to 0
-   Two sample: two poputlations each with a mean and variance
    - We wish to test whether the two population means are equal.\
        Null hypothesis: $H_0:\mu=\mu_1=\mu_2$\
        Two-sided alternative hypothesis: $H_1:\mu_1 \neq \mu_2$
    - Equal variance or unequal variance
-   Paired: observations are paired in some way \
        Null hypothesis: true mean difference *is* equal to 0
        Alternative hypothesis: true mean difference *is not* equal to 0

## Transformations

-   Right skewed data needs a shrinking transformation
-   Left skewed data needs a stretching transformation
-   The strength or power of the transformation depends on the degree of
    skew.\

| POWER | Formula               | Name            | Result                 |
|:------|:----------------------|:----------------|:-----------------------|
| 3     | $x^3$                 | cube            | stretches large values |
| 2     | $x^2$                 | square          | stretches large values |
| 1     | $x$                   | raw             | No change              |
| 1/2   | $\sqrt{x}$            | square root     | squashes large values  |
| 0     | $\log{x}$             | logarithm       | squashes large values  |
| -1/2  | $\frac{-1}{\sqrt{x}}$ | reciprocal root | squashes large values  |
| -1    | $\frac{-1}{x}$        | reciprocal      | squashes large values  |

## Non-parametric tests

-   Spearman's Rank Correlation 
-   Wilcoxon signed rank test: alternative to one-sample t-test
-   Mann-Whitney Test: for two group comparison
    Null Hypothesis: two group medians are the same 
-   Kruskal-Wallis: rank sum test testing whether samples are from the same distrbution; alternative to ANOVA

## Summary

Basic methods of inference include:

- estimating a population parameter (e.g. mean) using a sample of values,
- estimating standard deviation of a sample estimate using the standard error,
- constructing confidence intervals for an estimate, and 
- testing hypotheses about particular values of the population parameter.

Inference is relatively easy for normally distributed populations.

Student's *t*-tests include:

- one-sample *t*-test, including paired-sample *t*-test for a difference of zero
- two-sample *t*-test assuming equal variances (estimating pooled variance)
- two-sample *t*-test not assuming equal variances (Welch test)

The *t*-test is generally robust for non-normal populations (especially for large samples).

Power transformations, such as square-root, log, or Box-Cox aim to reduce skewness.

Non-parametric tests can be used for non-normal data, but they are usually less powerful than parametric tests.

## Chapter 5: Tabulated counts

Tables and frequencies

-   We often have hypotheses regarding the frequencies of levels of a factor or group of factors.

    - For a single two-level factor (e.g., male vs female, survived vs died), we might wish to know how likely the data are to have come from a population with equal proportions (or some other specified proportion). 

    - For two factors, we might wish to know whether they are independent. 

- In either case, we can specify a null hypothesis and test it using data. 

- To do so, compare *observed* counts with *expected* counts, where the *expected* counts are derived from our null model about the population.


## Goodness of fit test

Hypotheses of this sort can be tested using the **Chi-squared test statistic** ($\chi ^ 2$ = Ki Sq.)

$$
\chi ^{2} =\sum _{1}^{c}\frac{\left({\rm Observed-Expected}\right)^{{\rm 2}} }{{\rm Expected}}  =\sum _{1}^{c}\frac{\left( O-E\right)^{2}}{E}
$$

If the number of categories is $c$, then the degrees of freedom is $c-1$. 

$\chi ^ 2$ distribution is a continuous distribution. When used for
    discrete data, Chi-squared approximation is very good for $E>5$ and
    often good if some expected values are as small as 1.
    
Sometimes, one can merge/combine categories in case of small expected counts.


## Goodness of fit for distributions

-   Treat class intervals as *categories* and obtain the actual counts
    (O) 
-   The assumed distribution gives the expected counts (E)
-   Perform a goodness of fit and validate the assumed theoretical
    distribution
-   Adjust the degrees of freedom (df) for the number of estimated
    parameters of the theoretical distribution
    -   For example, assume that you have 10 class intervals and test
        for normal distribution, which has 2 parameters.\
    -   So the df for this test will be 10-1-2=7.

## Contingency table

-   Given a two way table of frequency counts, **we test whether the row
    and column variables are independent**

-   The expected count for cell $(i,j)$ is given by $E_{ij}$ =
    $(T_i \times T_j)/n$ where\
    $~~~~~T_i$, the total for row $i$;\
    $~~~~~T_j$, the total for column $j$\
    $~~~~~n$, the overall total count

-   Test statistic :
    $\chi ^{2} =\sum _{{i=1}}^{r}\sum _{{j=1}}^{{\rm c}}\frac{\left({ O}_{{ ij}} { -E}_{{ij}} \right)^{{ 2}} }{{ E}_{{ij}} }.$

-   degrees of freedom: $(r-1)(c-1)$

## Summary

-   Goodness of fit is for testing whether the observed counts are from
    the hypothesised population groups.

    -   For $c$ categories (groups), the test involves $c-1$ df.

-   Contingency Table ($r$ rows and $c$ columns) data are tested for the
    independence.

    -   For $r\times c$ cells the test involves $(r-1)(c-1)$ df

-   $\chi ^{2}$ test works well if $E> 5$. Some could be as low as 1.

-   Do correspondence analysis (symmetric plots) when independence is
    rejected.
