---
title: "Chapter 4:<br>Introduction to Statistical Inference"
image: img/inference.png
format: 
  revealjs:
    width: 1050
    height:	700
    scrollable: true
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: img/L_Color.png
    footer: "[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)"
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
    
execute:
  echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = FALSE, 
                      include=TRUE, 
                      message=FALSE, 
                      comment = NA, 
                      warn=-1, 
                      warn.conflicts = FALSE, 
                      quietly=TRUE, 
                      fig.align="center")
library(tidyverse)
library(patchwork)
library(kableExtra)
theme_set(theme_minimal())
```

# Statistical Inference {.background-black}

> “A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions.”
>
> — M.J. Moroney


## Statistical Inference

::::{.columns}

:::{.column width="65%"}

The term *statistical inference* means that we are using properties of a sample to make statements about the population from which the sample was drawn. 

For example, say you wanted to know the mean length of the leaves on a tree ($\mu$). You wouldn't want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths ($x_i$), calculate the sample mean ($\bar x$), and use $\bar x$ as an estimate of $\mu$.

:::

:::{.column width="35%"}

![](img/sampling_variation2.gif)

:::

::::


## Statistical Inference for a Population Mean

::::{.columns}

:::{.column width="60%"}

Consider a population $X$ with mean $\mu$,\
variance $\text{Var}(X)=\sigma^2$, and\
standard deviation $\sigma$.

We can estimate $\mu$ by: 

- taking a random sample of values $\{x_1, x_2, ..., x_n\}$ from the population, $X$, and
- calculating the sample mean $\bar x = \frac 1 n \sum_{i=1}^{n}x_i$.


:::

:::{.column width="40%"}

![](img/inference.png)

:::

::::

Importantly:

- The sample mean $\bar x$ is a single draw from a random variable $\bar X$. It will never be exactly equal to the population mean, $\mu$, unless you measure every single leaf. 
- There is ***uncertainty*** in our estimate of $\mu$. Each sample yields a different $\bar x$. This is called ***sampling error***. 


## Sampling Variation

![](img/samp_dist.gif)

## Sampling Error

The variability of the sample means from sample to sample, $\text{Var}(\bar X)$, depends on two things: 

- the variability of the population values, $\text{Var}(x)$, and 
- the size of the sample, $n$.

The variability of the sample estimate of a parameter is usually expressed as a ***standard error*** (SE), which is simply the theoretical standard deviation of $\bar X$ from sample to sample.

The equation is surprisingly simple!

$\text{Var}(\bar X) = \text{Var}(X)/n$

$\text{SE}(\bar X) = \text{SD}(\bar X) = \sigma/\sqrt n$


## Quetelet's dataset

::::{.columns}

:::{.column width="70%"}

In 1846, a Belgian scholar Adolphe Quetelet published an analysis of the chest sizes of a full population of 5,738 Scottish soldiers.

The distribution of the measurements (in inches) in his database has a mean of $\mu$ = 39.83 inches, a standard deviation of $\sigma$ = 2.05 inches, and is well approximated by a normal distribution.

:::

:::{.column width="30%"}

![Adolphe Quetelet<br>1796-1874](img/quetelet_face.jpg)
:::

::::

```{r}
#| echo: true
#| fig-width: 5
#| fig-height: 3
#| output-location: column

qd <- tibble(
  Chest = 33:48, 
  Count = c(3, 18, 81, 185, 420, 749, 1073, 1079, 
            934, 658, 370, 92, 50, 21, 4, 1),
  Prob = Count / sum(Count)
  )

qd |> 
  ggplot() +
  aes(x = Chest, y = Count) +
  geom_col() +
  xlab("") + ylab("") +
  ggtitle("Chest circumferences of Scottish soldiers")

```



## Quetelet's dataset

Every soldier was measured so we can treat this as the population, and $\mu$ = 39.83 and $\sigma$ = 2.05 as population parameters. Let's take some samples of size $n$ = 6 from this population.

```{r}
#| echo: true
# Convert to a long vector of values
qd_long <- rep(qd$Chest, qd$Count)
```

::: {.fragment}

```{r}
#| echo: true
# A single sample
sample(qd_long, size = 6)
```

:::

::: {.fragment}

```{r}
#| echo: true
# Ten samples
map(1:7, ~ sample(qd_long, size = 6))
```

:::

## Quetelet's dataset

Every soldier was measured so we can treat this as the population, and $\mu$ = 39.83 and $\sigma$ = 2.05 as population parameters. Let's take some samples of size $n$ = 6 from this population.

```{r}
#| echo: true
# Convert to a long vector of values
qd_long <- rep(qd$Chest, qd$Count)
```

::: {.fragment}

```{r}
#| echo: true
# Put ten samples in a tibble
q_samples <- tibble( sample = as_factor(1:10) ) |> 
  mutate( 
    values = map(sample, ~ sample(qd_long, size = 6)) 
    ) |> 
  unnest()

head(q_samples, 15)
```

:::


## Quetelet's dataset

Every soldier was measured so we can treat this as the population, and $\mu$ = 39.83 and $\sigma$ = 2.05 as population parameters. Let's take some samples of size $n$ = 6 from this population.

```{r}
#| echo: true
# Convert to a long vector of values
qd_long <- rep(qd$Chest, qd$Count)
```

```{r}
#| echo: true
# Put ten samples in a tibble
q_samples <- tibble( sample = as_factor(1:10) ) |> 
  mutate( 
    values = map(sample, ~ sample(qd_long, size = 6)) 
    ) |> 
  unnest()

```


::: {.fragment}

```{r}
#| echo: true
# Calculate means of each sample
sample_means <- q_samples |> 
  group_by(sample) |> 
  summarise(mean = mean(values))

sample_means
```

:::

## Quetelet's dataset


::::{.columns}

:::{.column width="55%"}

```{r}
#| echo: true
# Make a function to do it all and plot
plot_samples <- function(dat = qd_long,
                         no_samples = 12,
                         sample_size = 6) {

  # Put samples in a tibble
  q_samples <- tibble( 
    sample = as_factor(1:no_samples),
    values = map(sample, ~ sample(dat, size = sample_size))
    ) |> unnest()
  
  # Calculate means of each sample
  sample_means <- q_samples |> group_by(sample) |> 
    summarise(mean = mean(values))
  
  ggplot() + 
    xlim(min(dat), max(dat)) +
    geom_vline(xintercept = mean(dat), alpha = .4) +
    geom_jitter(
      data = q_samples,
      mapping = aes(y = sample, x = values),
      width = 0, height = 0.1, alpha = .8
      ) + 
    geom_point(
      data = sample_means,
      mapping = aes(y = sample, x = mean),
      shape = 15, size = 3, 
      colour = "dark orange"
      ) +
    ggtitle(paste("Sample size =", sample_size))
}
```

:::

:::{.column width="45%"}

```{r}
#| echo: true
#| eval: true
#| fig-width: 5
#| fig-height: 4.5
plot_samples(sample_size = 6)
```

:::
::::

## Sample means and sample sizes

::::{.columns}

:::{.column width="40%"}

Let's compare the distribution of means for different sample sizes *across the samples*.

```{r}
#| echo: true
#| eval: false

plot_samples(sample_size = 3)
plot_samples(sample_size = 5)
plot_samples(sample_size = 10)
plot_samples(sample_size = 20)
plot_samples(sample_size = 50)
plot_samples(sample_size = 100)
plot_samples(sample_size = 200)

```

:::
:::{.column width="60%"}

```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-show: animate

plot_samples(sample_size = 3)
plot_samples(sample_size = 5)
plot_samples(sample_size = 10)
plot_samples(sample_size = 20)
plot_samples(sample_size = 50)
plot_samples(sample_size = 100)
plot_samples(sample_size = 200)

```
:::
::::

The larger the $n$, the less the sample means vary.



## Standard error

If is distributed as a normal random variable with mean $\mu$ and variance $\sigma^2$,

$$X \sim \text{Normal}(\mu, \sigma)$$

then sample means of size $n$ are distributed as

$$\bar X \sim \text{Normal}(\mu_\bar{X} = \mu, \sigma_\bar{X} = \sigma/\sqrt{n})$$
$\text{SE}(\bar{X}) = \sigma_\bar{X}= \sigma/\sqrt{n})$ is known as the ***standard error of the sample mean***. 

More generally, a standard error is the standard deviation of an estimated parameter over $\infty$ theoretical samples. 

According to the Central Limit Theorem (CLT), raw $X$ values don't have to be normally distributed for the sample means to be normally distributed (for any decent sample size)!

## Quetelet's chests: Standard error of means of samples

**Question:**

If Quetelet's chest circumferences are distributed as $X \sim \text{N}(\mu=39.83, \sigma=2.05)$, then how variable are the means of samples of size $n$ = 6?

**Answer:**

$$
\begin{aligned}
SE(\bar{X}_{n=6})&=\sigma/\sqrt{n} \\
&=2.05/\sqrt{6} \\
&=0.8369 
\end{aligned}
$$
Sample means of size $n$ = 6 would be distributed as 

$\bar{X}_{n=6} \sim \text{N}(\mu=39.83, \sigma=0.8369)$


## Quetelet's chests: Distribution of means of samples

```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-show: animate


plot_samples_dist <- function(n) {
  
  require(patchwork)
  
  se = sd(qd_long) / sqrt(n)
  
  plot_samples(sample_size = n) / 
    ggplot(
      tibble(
        x = seq( mean(qd_long) - 3.5*se, 
                 mean(qd_long) + 3.5*se, 
                 length = 10 )
        )
      ) + 
    xlim(min(qd_long), max(qd_long)) + 
    aes(x) +
    stat_function(
      xlim = c(mean(qd_long) - 3.5*se, 
               mean(qd_long) + 3.5*se),
      geom = "area",
      fun = dnorm, 
      args = list(mean = mean(qd_long), sd = se),
      colour = "dark orange",
      fill = "dark orange",
      alpha = .4) + 
    theme_void() + 
    annotate("text", label = "Theoretical distribution\nof sample means", 
             x=36, 
             y=0.5 * dnorm(mean(qd_long), mean(qd_long), sd = se)
             ) +
    plot_layout(heights = c(5, 2))
  }
  
plot_samples_dist(5)
plot_samples_dist(10)
plot_samples_dist(20)
plot_samples_dist(50)
plot_samples_dist(100)
plot_samples_dist(200)

```
## Confidence intervals

:::{.incremental}

- Another common way of expressing uncertainty when making statistical inferences is to present:
  + a point estimate (e.g., the sample mean) and 
  + a confidence interval around that estimate.
      
- A confidence interval gives an indication of the sampling error. 

- A 95% confidence interval is constructed so that ***intervals from 95% of samples will contain the true population parameter***. 

- In reality the interval either contains the true value or not, so you must **not** interpret a confidence interval as "there's a 95% probability that the interval contains the true parameter". 

- We can say:
  + “95% of so-constructed intervals will contain the true value of the parameter” or
  + “with 95% confidence, the interval contains the true value of the parameter”.

:::

## Confidence intervals

::::{.columns}

:::{.column width="50%"}

Because we know the population parameters for the Quetelet dataset, we can calculate where 95% of sample means will lie for any particular sample size.

Recall that, for a normal distribution, 95% of values lie between $\mu \pm 1.96\times\sigma$ \
(i.e., $\mu - 1.96\times\sigma$ and $\mu + 1.96\times\sigma$). 

It follows that 95% of means of samples of size $n$ will lie within $\mu \pm 1.96 \times \sigma/\sqrt{n}$.

For example, for samples of size 6 from the Quetelet dataset, 95% of means will lie within $39.83 \pm 1.96\times 2.05 / \sqrt 6$, \
so $\{ 37.02 , 42.64\}$.

:::
:::{.column width="50%"}

```{r}
# Make a function to do it all and plot
plot_samples_intervals <- function(dat = qd_long,
                         no_samples = 12,
                         sample_size = 6) {

  # Put samples in a tibble
  q_samples <- tibble( 
    sample = as_factor(1:no_samples),
    values = map(sample, ~ sample(dat, size = sample_size))
    ) |> unnest()
  
  # Calculate means of each sample
  sample_means <- q_samples |> group_by(sample) |> 
    summarise(mean = mean(values))
  
  ggplot() + 
    xlim(min(dat), max(dat)) +
    annotate(
      "rect", 
      xmin = mean(dat) - 1.96 * sd(dat) / sqrt(sample_size),
      xmax = mean(dat) + 1.96 * sd(dat) / sqrt(sample_size),
      ymin = -Inf, ymax = Inf,
      fill = "dark orange", 
      alpha = .4) +
    geom_vline(xintercept = mean(dat)) +
    geom_jitter(
      data = q_samples,
      mapping = aes(y = sample, x = values),
      width = 0, height = 0.1, alpha = .8
      ) + 
    geom_point(
      data = sample_means,
      mapping = aes(y = sample, x = mean),
      shape = 15, size = 3, 
      colour = "dark orange"
      ) +
    ggtitle(
      "95% of sample means will be within the orange band",
      subtitle = paste("Sample size =", sample_size))
}
```

```{r}
#| fig-width: 5
#| fig-height: 6
set.seed(1232)
plot_samples_intervals(no_samples=20)
```
:::
::::

## Estimating the distribution of sample means from data

:::{.incremental}

- It's all very well deriving the distribution of sample means when we *know the population parameters* ($\mu$ and $\sigma$) but *in most cases we only have our one sample*.

- We don't know any of the population parameters. We have to *estimate* the population mean (usually denoted $\hat\mu$ or $\bar x$) and *estimate* of the population standard deviation (usually denoted $\hat\sigma$ or $s$) from the sample data.

- This additional uncertainty complicates things a bit. In fact, we can't even use the normal distribution any more!

:::

## The *t* distribution


::::{.columns}

:::{.column width="70%"}

- Introducing William Sealy Gosset, who described the *t* distribution after working with random numbers.

- Gosset was a chemist and mathematician who worked for the Guinness brewery in Dublin. 

- Guinness forbade anyone from publishing under their own names so that competing breweries wouldn’t know what they were up to, so he published his discovery in 1908 under the penname “Student”. Student’s identity was only revealed when he died. It is therefore often called “Student’s *t* distribution”. 

:::
:::{.column width="30%"}

![Willam Sealy Gosset<br>1876-1937](img/gosset_1876-1937.jpg)

:::
::::


## The *t* distribution


- The *t* distribution is like the standard normal. It is bell-shaped and symmetric with mean = 0, but it has fatter tails (greater uncertainty) due to the fact that we do not *know* $\sigma$; we have to *estimate* it.

- The *t* distribution is not just a single distribution. It is really an entire series of distributions which is indexed by something called the “degrees of freedom” (or $df$).

::::{.columns}

:::{.column width="40%"}

- As $df \rightarrow \infty$, $t \rightarrow Z$.

```{r}
#| echo: true

p <- expand_grid(
  df = c(2, 5, Inf),
  x = seq(-4, 4, by = .01)
  ) |> 
  mutate(
    Density = dt(x = x, df = df),
    `degrees of freedom` = as_factor(df)
  ) |> 
  ggplot() +
  aes(x = x, y = Density, 
      group = `degrees of freedom`, 
      colour = `degrees of freedom`) +
  geom_line()
```


:::
:::{.column width="60%"}

```{r}
#| fig-height: 3
#| fig-width: 6

p
```

:::
::::

## The *t* distribution

For a sample of size $n$, if

- $X$ is a normal random variable with mean $\mu$,
- $\bar X$ is the sample mean, and 
- $s$ is the sample standard deviation, 

then the variable:

$$
T = \frac{\bar X - \mu} {s/\sqrt{n}}
$$

is distributed as a $t$ distribution with $(n – 1)$ degrees of freedom.

## Confidence intervals and the *t* distribution

The process of using sample data to try and make useful statements about an unknown parameter, $\theta$, is called statistical inference.

A confidence interval for the true value of a parameter is often obtained by:

$$
\hat \theta \pm t \times \text{SE}(\hat \theta)
$$
where $\hat \theta$ is the sample estimate of $\theta$ and $t$ is a quantile from the $t$ distribution with the appropriate degrees of freedom. 

For example, to get the $t$ score for a 95% confidence interval with 9 degrees of freedom:

```{r}
#| echo: true
qt(p = 0.975, df = 9)
```

The piece that is being added and subtracted, $t \times \text{SE}(\hat \theta)$, is often called the *margin of error*.


## Confidence intervals for a sample mean

We can calculate a 95% confidence interval for a sample mean with the following information:

::: {style="font-size: 75%;"}
- sample mean $\bar x$,
- sample standard deviation $s$, 
- the sample size $n$, and 
- the 0.025th quantile of the $t$ distribution with degrees of freedom $df=n-1$.
:::


::::{.fragment}
:::{.absolute top="45%" left=0}

A sample of size $n$ = 6 from Quetelet data

```{r}
#| echo: true
n1 <- 6; df1 <- n1-1
( dq1 <- sample(qd_long, size = n1) )
( mean1 <- mean(dq1) )
( sd1 <- sd(dq1) )
( t1 <- qt(c(0.025, 0.975), df = df1) )
mean1 + t1 * sd1 / sqrt(n1)
```

:::
::::

::::{.fragment}
:::{.absolute top="45%" right=0}
Or, more simply...

```{r}
#| echo: true
t.test(dq1)
```

:::
::::

## Confidence intervals for sample means

```{r}
#| echo: false
# Make a function to do it all and plot
plot_samples_ci <- function(dat = qd_long,
                         no_samples = 12,
                         sample_size = 6) {

  # Put samples in a tibble
  q_samples <- tibble( 
    sample = as_factor(1:no_samples),
    values = map(sample, ~ sample(dat, size = sample_size))
    ) |> unnest()
  
  # Calculate means of each sample
  sample_means <- q_samples |> group_by(sample) |> 
    summarise(mean = mean(values),
              sd = sd(values)) |> 
    mutate(
      lower = mean + qt(0.025, df = sample_size-1) * sd / sqrt(sample_size),
      upper = mean + qt(0.975, df = sample_size-1) * sd / sqrt(sample_size)
      )
  
  ggplot() + 
    xlim(min(dat), max(dat)) +
    geom_vline(xintercept = mean(dat), alpha = .4) +
    geom_jitter(
      data = q_samples,
      mapping = aes(y = sample, x = values),
      width = 0, height = 0.1, alpha = .8
      ) + 
    geom_pointrange(
      data = sample_means,
      mapping = aes(
        y = sample, 
        x = mean,
        xmin = lower,
        xmax = upper
        ),
      shape = 15, size = 0.5, 
      colour = "dark orange"
      ) +
    ggtitle(paste("Sample size =", sample_size))
}
```

```{r}
#| fig-height: 5
#| fig-width: 10
#| output-location: fragment

set.seed(123)

plot_samples_ci(no_samples = 30, sample_size = 5) +
  plot_samples_ci(no_samples = 30, sample_size = 10) +
  plot_samples_ci(no_samples = 30, sample_size = 50)
```

## *t* as a test statistic

This method can be used when the estimator $\hat\theta$ is approximately normally distributed and

$$
 \frac{\hat\theta - \theta} {\text{SE}(\hat \theta)}
$$

has approximately a Student’s *t* distribution.

This paves the way for hypothesis testing for specific values of $\theta$. Many of the methods you will learn in this course are based on this general rule.


## Testing hypotheses

- Testing hypotheses underpins a lot of scientific work.
- A hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.
- For logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:
   + specify a ***null hypotheses***, which must be true if our hypothesis of interest is false, and 
   + measure the evidence *against* the null hypothesis, usually in the form of a *p*-value.

## Example: growth of alfalfa

Say a farmer has 9 paddocks of alfalfa. He's hired a new manager, and wants to know if, on average, this year's crop is different to last year's. He measures the yield for each paddock in year 1 and year 2, and calculates the difference.

```{r}
#| echo: true
year1 <- c(0.8, 1.3, 1.7, 1.7, 1.8, 2.0, 2.0, 2.0, 2.2)
year2 <- c(0.7, 1.4, 1.8, 1.8, 2.0, 2.0, 2.1, 2.1, 2.2)
diff <- year2 - year1
( xbar <- mean(diff) ) ; ( s <- sd(diff) )
```
The mean difference is `r xbar |> round(3)`. On average, yields were `r xbar |> round(3)` greater than last year. \
Is that convincing different from zero? 

How likely is such a difference to have arisen just by chance, and really, if we had a million paddocks, there would be no difference?

What is the probability of seeing a difference of `r xbar |> round(3)` or more in our dataset if the true mean were zero?

These questions can be addressed with a $p$-value from a hypothesis test.

## Example: growth of alfalfa

The hypothesis of interest (called the "alternative" hypothesis) is: 

$\ \ \ \ \ H_A: \mu \ne 0$, that is, the mean difference in yield $\mu$ between the two years is not zero. 

The **null** hypothesis (the case if the alternative hypothesis is wrong) is:

$\ \ \ \ \ H_0: \mu = 0$, that is, the mean difference is zero.

We can test the null hypothesis using a *t* statistic $t_0 = \frac{\bar x - \mu_0} {\text{SE}(\bar x)}$, where \
$\ \ \ \ \ \mu_0 = 0$ is the hypothesised value of the mean and\
$\ \ \ \ \ \text{SE}(\bar x) = s/\sqrt{n}$ is the (estimated) standard error of the sample mean. 

```{r}
#| echo: true
( t0 <- (xbar - 0) / ( s / sqrt(9) ) )
```
The *p*-value is $\text{Pr}(|t_{df=8}| > t_0)$

```{r}
#| echo: true
2 * pt(t0, df = 8, lower = F)
```

:::{.absolute bottom=0 right=0}

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 2.5
dfa=8

p <- ggplot() + 
  stat_function(
      xlim = c( -4, 4),
      geom = "area",
      fun = dt, 
      args = list(df = dfa),
      alpha = .4) +
  geom_area(
    data = tibble(
      t = seq(-4, -t0, by=.01),
      y = dt(t, df=dfa)
    ), 
    mapping = aes(t,y),
    fill = "dark orange"
    ) +
  geom_area(
    data = tibble(
      t = seq(t0, 4, by=.01),
      y = dt(t, df=dfa)
    ), 
    mapping = aes(t,y),
    fill = "dark orange"
    ) +
  geom_segment(
    aes(x = c(-t0,t0), 
        xend = c(-t0,t0),
        y = c(0,0),
        yend = c(0.1,0.1)
        )
    ) + 
  annotate(
    "text",
    x = c(-t0, t0),
    y = c(0.12,0.12),
    label = c(-t0 |> round(3), t0 |> round(3))
    ) +
  ggtitle("t distribution for df = 8") +
  ylab("")

p
```
:::

## Example: growth of alfalfa

The hypothesis of interest (called the "alternative" hypothesis) is: 

$\ \ \ \ \ H_A: \mu \ne 0$, that is, the mean difference in yield $\mu$ between the two years is not zero. 

The **null** hypothesis (the case if the alternative hypothesis is wrong) is:

$\ \ \ \ \ H_0: \mu = 0$, that is, the mean difference is zero.

This can be done more easily:

```{r}
#| echo: true
t.test(diff)
```

:::{.absolute bottom=0 right=0}

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 2.5
p
```
:::


## Example: growth of alfalfa

The $p$-value of 0.05 means that, if the null hypothesis were true, only 5% of sample means would be as or more extreme than the observed value of `r xbar |> round(3)`. It is the area in orange in the graph below.

We can therefore reject the null hypothesis at the conventional 5% level, and conclude that, on average, yields were indeed higher this year.

This is an example of a paired *t* test. They two samples (year 1 and year 2) are not independent of one another because we have the same paddocks in both years. So, we take the differences, treat them like a single sample, and do a one-sample *t* test for the mean difference being zero.

$p$-values are random variables too.\
<https://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/>

:::{.absolute bottom=0 right=0}

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 2.5
p
```
:::

## Truth and outcomes

In reality, the null hypothesis is either true or false. 

The outcome of a test is either we reject the null hypothesis, or we fail to reject the null hypothesis (note, we never "confirm" or "accept" the null hypothesis – absence of evidence is not evidence of absence!). 

This gives us four possibilities:

|                          | *$H_0$ is true*  | *$H_0$ is false*  |
|--------------------------|------------------|-------------------|
| **Reject $H_0$**         | Type I error     | Correct result    |
| **Do not reject $H_0$**  | Correct result   | Type II error     |


With probabilities usually represented by:

|                          | *$H_0$ is true*  | *$H_0$ is false*    |
|--------------------------|------------------|---------------------|
| **Reject $H_0$**         | $\alpha$         | $1-\beta$ = "power" |
| **Do not reject $H_0$**  | $1 - \alpha$     | $\beta$             |


## Truth and outcomes

The probability of making a Type I error, rejecting $H_0$ when $H_0$ is true, is set by the experimenter *a priori* (beforehand) as the significance level, $\alpha$.

- Say we set $\alpha$ at the conventional 0.05. We know that, if $H_0$ is true, we have a 5% chance of rejecting $H_0$ (the Type I error rate is 0.05) and a 95% chance of not rejecting $H_0$.

The probability of retaining (not rejecting) $H_0$ when $H_0$ is false is usually represented by $\beta$ (“beta”).

- We usually do not know $\beta$ because it depends on $\sigma$, $n$ and also the *effect size* under the alternative hypothesis. These must be asserted to calculate $\beta$ and power, $1-\beta$. 


## Calculating power

The ***power*** of a hypothesis test is the probability of rejecting the
null hypothesis if it is indeed false. Basically, *"if we're right, what is the probability that we'll be able to show it?"*

The power of the $t$ test can be evaluated using R if you assert the effect size $\delta$:

::::{.columns}

:::{.column}
```{r, echo=TRUE}
power.t.test(n = 10, delta = 1, sd = 1, sig.level = 0.05)
```
:::

:::{.column}
```{r, echo=TRUE}
power.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)
```
:::

::::

The bigger the sample size, the bigger more power we have to detect an effect.

Often, experimenters might aim to have a power of 80% or more, so they will most likely reject the null if it is false.

## Sampling distributions

The term *sampling distribution* means the distribution of the computed
statistic such as the sample mean when sampling is repeated many times.

For a normal population,

-   Student's $t$ distribution is the sampling distribution of the mean
    (after rescaling).

-   $\chi^2$ distribution is the sampling distribution of the sample
    variance $S^2$.

-   $(n-1)S^2/\sigma^2$ follows $\chi^2$ distribution.

$F$ distribution is ratio of two $\chi^2$ distributions.

-   It becomes the sampling distribution of the ratio of two sample
    variances $S_1^2/S_2^2$ from two normal populations (after scaling).

$t$ distribution is symmetric but $\chi^2$ and $F$ distributions are
right skewed. - For large samples, they become normal - For $n>30$, the
skew will diminish

For the three sampling distributions, the sample size $n$ becomes the
proxy parameter, called the degrees of freedom (df).

-   $t_{n-1}$, $\chi_{n-1}^2$ & $F_{(n_1-1),(n_2-1) }$

## Two sample t-test

- The two-sample *t*-test is a common statistical test. You have two populations, $X_1$ and $X_2$, with means $\mu_1$ and $\mu_2$ and variances $\sigma^2_1$ and $\sigma^2_2$, respectively. 

- We wish to test whether the two population means are equal.\
Null hypothesis: $H_0:\mu=\mu_1=\mu_2$\
Two-sided alternative hypothesis: $H_1:\mu_1 \neq \mu_2$

- The basic *t*-test ***assumes equal variances***; that is, $\sigma^2_1 = \sigma^2_2$.\
If we make this assumption and it is false, the test may give the wrong conclusion!

## Two sample t-test with equal variances

Under the assumption of equal variances $\sigma^2_1 = \sigma^2_2$, we can perform a **pooled-sample t-test**. 

We calculate the estimated pooled variance as:

$$s_{p}^{2} = w_{1} s_{1}^{2} +w_{2} s_{2}^{2}$$

where the *weights* are $w_{1} =\frac{n_{1}-1}{n_{1} +n_{2}-2}$ and $w_{2} =\frac{n_{2}-1}{n_{1} +n_{2}-2}$, \
and $n_1$ and $n_2$ are the sample sizes for samples 1 and 2, respectively. 

For the pooled case, the $df$ for the $t$-test is simply

$$df = n_{1}+n_{2}-2$$

## Two sample t-test with unequal variances

This is often called the "Welch test".

We do not need to calculated the pooled variance $s^2_p$;\
we simply use $s^2_1$ and $s^2_2$ as they are. 

For the unpooled case, the $df$ for the test is smaller:
    $$df=\frac{\left(\frac{s_{1}^{2}}{n_{1}} +\frac{s_{2}^{2} }{n_{2}} \right)^{2} }{\frac{1}{n_{1} -1} \left(\frac{s_{1}^{2}}{n_{1}}\right)^{2} +\frac{1}{n_{2} -1} \left(\frac{s_{2}^{2}}{n_{2} } \right)^{2}}$$
The $df$ doesn't have to be an integer in this case. 


## Validity of equal variance assumption 

We can test the null hypothesis that the variances are equal using either a Bartlett's test or Levene's test. Levene's is generally favourable over Bartlett's. 


```{r}
#| echo: true
tv = read_csv("https://www.massey.ac.nz/~anhsmith/data/tv.csv")
car::leveneTest(TELETIME~factor(SEX), data=tv)
```

Highish *p-value* means there's no strong evidence against the null hypothesis that the variances are equal. Therefore, we might feel happy to assume equal variances and use a pooled variance estimate.

## Validity of equal variance assumption 

*t*-test assuming equal variances:

```{r}
#| echo: true
t.test(TELETIME ~ factor(SEX), var.equal = TRUE, data=tv)
```

*t*-test *not* assuming equal variances (Welch test):

```{r}
#| echo: true
t.test(TELETIME ~ factor(SEX), data=tv)
```

## Shiny apps (some not currently working)

<https://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/>

<https://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/>

<https://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/>

For more on non-parametric tests, see Study Guide.

## Test of proportions

Testing the null hypothesis that a proportion $p=0.5$\
Alternative hypothesis: $p \ne 0.5$ 

Say a survey of 1000 beached whales had 450 females. 

::::{.columns}

:::{.column}

Is the population 50:50?

```{r}
#| echo: true
prop.test(450, 1000)
```

:::

:::{.column}

An exact version of the test

```{r}
#| echo: true
binom.test(c(450, 550))
```

:::

::::

To test for a proportion other than 0.5

```{r}
#| echo: true
binom.test(c(450, 550), p = 3/4)
```


## Comparing several proportions

Data on smokers in four group of patients\
(from Fleiss, 1981, *Statistical methods for rates and proportions*).

```{r, echo=TRUE}
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
```

More on chi-squared tests next week.

## Tests for normality

Testing the hypothesis that sample data came from a normally distributed population. $H_0: X \sim N(\mu, \sigma)$.

-   Kolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)
-   Shapiro-Wilk test (based on variance of the difference)

Example: a sample of $n$ = 50 from N(100,1).

::::{.columns}

:::{.column}

```{r, warning=FALSE, message=FALSE}
#| echo: true
set.seed(123)
shapiro.test(rnorm(50, mean=100))

```

:::

:::{.column}

```{r, warning=FALSE, message=FALSE}
#| echo: true
set.seed(123)
ks.test(rnorm(50), "pnorm")
```

:::
::::

The large p-values mean that there's no evidence of non-normality. 

Like all tests, they can give the wrong answer. Try with `set.seed(1234)`. 

## Can also use plots to assess normality

```{r, warning=FALSE, fig.height=3, fig.width=4}
#| echo: true
tv = read_csv("https://www.massey.ac.nz/~anhsmith/data/tv.csv")

ggplot(tv, aes(sample = TELETIME)) + 
  stat_qq() + stat_qq_line() +
  labs(title = "Normal quantile plot for TV viewing times") 
```


## Transformations

-   It can be useful to transform skewed data to make the distribution more like a 'normal' if that is required for a particular analysis. 

-   A linear transformation $Y^*= a+bY$ only changes the scale or centre, not the shape of the distribution.

-   Transformations can help to improve symmetry, normality, and stabilise the variance.

## Example

Right skewed distribution is made roughly symmetric using a log transformation for no. of vehicles variable (rangitikei.\* dataset)

```{r}
load("../data/rangitikei.Rdata")
p1 <- ggplot(rangitikei, aes(vehicle))+geom_boxplot()
p1 <- p1+labs(title = "Boxplot of vehicle (raw)")
p2 <- ggplot(rangitikei, aes(vehicle^2))+geom_boxplot()
p2 <- p2+labs(title = "Boxplot of vehicle squared")
p3 <- ggplot(rangitikei, aes(sqrt(vehicle)))+geom_boxplot()
p3 <- p3+labs(title = "Boxplot of square-root of vehicle")
p4 <- ggplot(rangitikei, aes(log(vehicle)))+geom_boxplot()
p4 <- p4+labs(title = "Boxplot of log vehicle")
library(patchwork)
p2+p1+p3+p4  
```

## A Ladder of Powers for Transforming Data

-   Right skewed data needs a shrinking transformation
-   Left skewed data needs a stretching transformation
-   The strength or power of the transformation depends on the degree of
    skew.\

| POWER | Formula               | Name            | Result                 |
|:------|:----------------------|:----------------|:-----------------------|
| 3     | $x^3$                 | cube            | stretches large values |
| 2     | $x^2$                 | square          | stretches large values |
| 1     | $x$                   | raw             | No change              |
| 1/2   | $\sqrt{x}$            | square root     | squashes large values  |
| 0     | $\log{x}$             | logarithm       | squashes large values  |
| -1/2  | $\frac{-1}{\sqrt{x}}$ | reciprocal root | squashes large values  |
| -1    | $\frac{-1}{x}$        | reciprocal      | squashes large values  |



## Box-Cox transformation

::::{.columns}

:::{.column width="40%"}

This is a normalising transformation based on the Box-Cox power parameter, $\lambda$.

R gives a point estimate of the Box-Cox power & a confidence interval.

Usually, we are concerned about whether the *residuals from a model* are normally distributed – put the model in the `lm` statement. 

Sometimes, no "good" value of $\lambda$ can be found.


:::

:::{.column width="60%"}
```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 4
library(lindia)
gg_boxcox(lm(rangitikei$vehicle ~ 1))
```

:::

::::

## Statistical inference based on transformed data

-   Obtain the confidence interval using transformed data and then back
    transform the limits (to keep the original scale)

    -   The confidence limits calculated on log-transformed data can be
        exponentiated back to the raw scale (i.e., $e^{\text{confidence limit}}$)
    -   The confidence limits of square-root-transformed data can be
        squared back to the raw scale (ie. ${\text{confidence limit}^2}$)

-   For hypothesis test, apply the same transformation on the value
    hypothesised under the null

-   Explore <https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/> app (not currently working)

## Example 

Brain weights of `Animals` data are right-skewed. 


```{r}
#| echo: true
#| tbl-cap: "Lower and upper confidence limits"
#| output-location: column
data(Animals, package = "MASS")

bind_cols( 
  # confidence intervals on raw data
  `raw x` = Animals |> 
    pull(brain) |> 
    t.test() |> 
    pluck("conf.int"),
  # confidence intervals on log-transformed data
  `log x` = Animals |> 
    pull(brain) |> 
    log() |> 
    t.test() |> 
    pluck("conf.int"),
  # confidence intervals on log-transformed data 
  # and then back-transformed
  `log x, CIs back-transformed` = Animals |> 
    pull(brain) |> 
    log() |> 
    t.test() |> 
    pluck("conf.int") |>
    exp()
  ) |> 
  kable()

```

## Non-parametric tests

Non-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations). 

Many non-parametric methods rely on replacing the observed data by their *ranks*. 

## Spearman's Rank Correlation


::::{.columns}

:::{.column}

Rank the $X$ and $Y$ variables, and then obtain usual Pearson correlation coefficient.

The plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.

```{r}
#| echo: true
library(GGally)

p <- ggpairs(
  trees, 
  upper = list(continuous = wrap('cor', method = "spearman")),
  lower = list(continuous = 'cor') 
  )
```

:::

:::{.column}

```{r}
#| label: fig-spear
#| echo: false
#| fig.cap: "Comparison of Pearsonian and Spearman's rank correlations"
#| fig-width: 5
#| fig-height: 5
p
```

:::
::::

## Wilcoxon signed rank test

A non-parametric alternative to the one-sample t-test

$H_0: \eta=\eta_0$ where $\eta$ (Greek letter 'eta') is the population median

Based on based on ranking $(|Y-\eta_0|)$, where the ranks for data with $Y<\eta_0$ are compared to the ranks for data with $Y>\eta_0$


::::{.columns}

:::{.column}

```{r}
#| echo: true
wilcox.test(tv$TELETIME, mu=1680, conf.int=T)
```

:::

:::{.column}

```{r}
#| echo: true
t.test(tv$TELETIME, mu=1680)
```

:::
::::

## Mann-Whitney test

For two group comparison, pool the two group responses and then rank the
pooled data

Ranks for the first group are compared to the ranks for the second group

The null hypothesis is that the two group medians are the same:     $H_0: \eta_1=\eta_2$.


::::{.columns}

:::{.column}


```{r}
#| echo: true
wilcox.test(rangitikei$people~rangitikei$time, conf.int=T)
```

:::

:::{.column}

```{r}
#| echo: true
t.test(rangitikei$people~rangitikei$time)
```

:::
::::

## Another form of test


::::{.columns}

:::{.column}

```{r}
#| echo: true
kruskal.test(rangitikei$people~rangitikei$time)
```

:::

:::{.column}

```{r}
#| echo: true
wilcox.test(rangitikei$people~rangitikei$time)
```


:::
::::

## Permutation tests

A permutation (or randomisation) test has the following steps:

1. Randomly permute the observed data *many times*, thereby destroying any real relationship, 
2. Recalculate the test statistic $T$  for each random permutation
3. Compare the observed value of $T$ (from the actual data) with the values of $T$ under permutation.

One sample hypothesis test example follows:

```{r}
#| echo: true
library(exactRankTests)
perm.test(tv$TELETIME, null.value=1500)
```

For small samples, this approach is not powerful.

## Two group comparison

```{r}
#| echo: true
perm.test(TELETIME~SEX, distribution ='exact', data=tv)
```

Also using a linear model fit (cover later)

```{r}
#| echo: true
library(lmPerm)
summary(lmp(TELETIME~SEX, data=tv))
```

Read the study guide example for bootstrap tests (not examined)

## Summary

Basic methods of inference include:

- estimating a population parameter (e.g. mean) using a sample of values,
- estimating standard deviation of a sample estimate using the standard error,
- constructing confidence intervals for an estimate, and 
- testing hypotheses about particular values of the population parameter.

Inference is relatively easy for normally distributed populations.

Student's *t*-tests include:

- one-sample *t*-test, including paired-sample *t*-test for a difference of zero
- two-sample *t*-test assuming equal variances (estimating pooled variance)
- two-sample *t*-test not assuming equal variances (Welch test)

The *t*-test is generally robust for non-normal populations (especially for large samples).

Power transformations, such as square-root, log, or Box-Cox aim to reduce skewness.

Non-parametric tests can be used for non-normal data, but they are usually less powerful than parametric tests.


<!-- ## Exercises -->

<!-- download.file("<https://www.massey.ac.nz/~kgovinda/220exer/Chap3moreexamples.R>", destfile="Chap3moreexamples.R") -->

<!-- download.file("<https://www.massey.ac.nz/~kgovinda/220exer/chapter-3-exercises.html>", destfile="chapter-3-exercises.html") -->

<!-- install.packages("remotes") -->

<!-- remotes::install_github("ricompute/ricomisc") -->

<!-- ricomisc::rstudio_viewer("chapter-3.html", file_path = NULL) -->
