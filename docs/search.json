[
  {
    "objectID": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "href": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "title": "Chapter 1:Data Collection",
    "section": "The nature of data: random variables",
    "text": "The nature of data: random variables\nData come from recording observations of the world. E.g.,\n\nrecording the number of heart beats per minute\ncounting birds in your yard\nmeasuring the lengths of fish you catch\n\nEach time you collect one of these observations, it is likely to be different.\n\nPulse may vary between 55 and 70 bpm, depending on when you record it\nNumber of birds varies at different times, and across different yards.\n\nWe call these random variables (often denoted with an upper case letter, \\(X\\)), because the particular value that a single observation or measurement will take is uncertain. The value varies across observations."
  },
  {
    "objectID": "slides/Chapter01.html#types-of-data",
    "href": "slides/Chapter01.html#types-of-data",
    "title": "Chapter 1:Data Collection",
    "section": "Types of data",
    "text": "Types of data"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "href": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of qualitative data",
    "text": "Subtypes of qualitative data\n\n\n\n\nNominal variables have no particular order (e.g., gender, colour, species, country)\n\n\n\nOrdinal variables can be ordered (e.g., altitude = {low, mid, high}, age group = {child, juvenile, adult} )"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "href": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of quantitative data",
    "text": "Subtypes of quantitative data\n\n\n\n\nContinuous variables have no gaps between possible values, as in measurements (e.g., weight, temperature, length)\n\n\n\nDiscrete variables have gaps between possible values, as in counts (e.g., number of siblings, number of flowers)"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-continuous-data",
    "href": "slides/Chapter01.html#subtypes-of-continuous-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of continuous data",
    "text": "Subtypes of continuous data\n\nInterval scale\n\nNo absolute zero\nDivision & subtraction may not be meaningful\nTemperature in degrees Celcius is interval because 20°C is not twice as hot as 10°C.\n\n\nRatio scale\n\nZero = zero\nAll arithmetic manipulation can be done\nLength is ratio because 20 mm is twice as long as 10 mm."
  },
  {
    "objectID": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "href": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "title": "Chapter 1:Data Collection",
    "section": "Data Collection: Survey, Experiment, Census",
    "text": "Data Collection: Survey, Experiment, Census\n\n\n\nWe collect data from the world to get information about patterns and processes.\nMost datasets contain a subset, a sample, of a much bigger population of interest.\n\nWe may conduct a survey to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.\nWe might conduct an experiment where we take a sample of elements (people, organisms, objects) and apply some treatment in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.\n\nIf we are not dealing with a sample, if every element of the population of interest is represented in the dataset, we call this a census rather than a sample."
  },
  {
    "objectID": "slides/Chapter01.html#measurement-issues",
    "href": "slides/Chapter01.html#measurement-issues",
    "title": "Chapter 1:Data Collection",
    "section": "Measurement issues",
    "text": "Measurement issues\n\nMeasuring Devices or Instruments\n\na physical device - measuring rule to gauge the heights of plants\na counting device - a Geiger- counter for measuring radioactive material\na questionnaire - requires a more subjective response.\n\nMeasurement Error\n\nmeasuring instrument may be faulty (bias)\nvalues recorded from the same object may vary from one measurement to another (variance)\n\nIndirect measures\n\nFor example, we use Body Mass Index (BMI) as a measure of condition, and we measure temperature with the expansion of mercury."
  },
  {
    "objectID": "slides/Chapter01.html#non-response",
    "href": "slides/Chapter01.html#non-response",
    "title": "Chapter 1:Data Collection",
    "section": "Non-response",
    "text": "Non-response\n\na non-sampling error\nSelection stage: an element may be selected but not found\n\ne.g. sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey.\n\nCollection stage: it may not be possible to take a measurement\n\nsome respondents may forget, or refuse, to answer the questionnaire\n\nDocumentation stage\n\nIncorrect record of measurement\n\nCall-backs reduce non-response"
  },
  {
    "objectID": "slides/Chapter01.html#census-related-concepts",
    "href": "slides/Chapter01.html#census-related-concepts",
    "title": "Chapter 1:Data Collection",
    "section": "Census related concepts",
    "text": "Census related concepts\nTARGET POPULATION the population under study\nFRAME operationalises data collection from a target population. e.g. listing of elements in population.\nACTUAL POPULATION is the resulting set of elements on which usable data have been collected."
  },
  {
    "objectID": "slides/Chapter01.html#sample-vs-population",
    "href": "slides/Chapter01.html#sample-vs-population",
    "title": "Chapter 1:Data Collection",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\n\nA sample is a subset of the population.\nDatasets usually only contain a sample from the population; rarely do we have the entire population of data!\nWhy sample?\n\nSampling conserves resources (money, time, etc.).\nA well collected sample is more useful than a badly designed census.\nCollecting data may be destructive.\nThe disadvantage: the statistics we calculate from sample data is subject to sampling variation, which introduces uncertainty* about their true values.\n\n\n\n\n\n“You don’t have to eat the whole ox to know that the meat is tough” – Samuel Johnson (1709-1784)"
  },
  {
    "objectID": "slides/Chapter01.html#population-frame-and-sample",
    "href": "slides/Chapter01.html#population-frame-and-sample",
    "title": "Chapter 1:Data Collection",
    "section": "Population, frame, and sample",
    "text": "Population, frame, and sample"
  },
  {
    "objectID": "slides/Chapter01.html#statistical-inference",
    "href": "slides/Chapter01.html#statistical-inference",
    "title": "Chapter 1:Data Collection",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the process of using information from sample data to make conclusions about the population.\n\nFor example, we want to know \\(\\mu\\), mean length of fish in a population. So, we collect a sample of fish, measure their lengths, calculate the mean \\(\\bar{x}\\), and use \\(\\bar{x}\\) as an estimate of \\(\\mu\\). This is statistical inference.\n\n\n\n\nThe sample mean \\(\\bar{x}\\) depends on which particular fish we happened to get in our sample.\nTherefore, the sample mean \\(\\bar{x}\\) itself is a random variable.\nIf we were to take 1000 different samples, we’d get 1000 different means."
  },
  {
    "objectID": "slides/Chapter01.html#bias-vs-sampling-variance",
    "href": "slides/Chapter01.html#bias-vs-sampling-variance",
    "title": "Chapter 1:Data Collection",
    "section": "Bias vs sampling variance",
    "text": "Bias vs sampling variance\n\n\nA method used to estimate \\(\\hat{\\theta}\\) a population parameter \\(\\theta\\) is called an estimator. An estimator includes the study design, methods of data collection, and mathematical operations.\nSampling variance is the sample-to-sample variation in an estimator.\nBias is when our estimator doesn’t get it right on average. That is, the average of estimates over \\(\\infty\\) samples is not centred on the population parameter; \\(\\text{Mean}(\\hat{\\theta}) \\neq \\theta\\).\n\n\n\n\nAn estimator can have high/low sampling variance and high/low bias."
  },
  {
    "objectID": "slides/Chapter01.html#principle-of-randomisation",
    "href": "slides/Chapter01.html#principle-of-randomisation",
    "title": "Chapter 1:Data Collection",
    "section": "Principle of randomisation",
    "text": "Principle of randomisation\n\n\n\nWe want our sample to be representative of (and have similar properties to) the population. The most straightforward way to do this is through randomisation.\nWe randomise the selection of objects for our sample to avoid bias. If we (consciously or subconsciously) tended to chose the largest fish for our sample, we’d get an upwardly biased estimate of the lengths.\nSimple random sampling or EPSEM (equal probability of selection) is the gold standard of random sampling."
  },
  {
    "objectID": "slides/Chapter01.html#simple-random-sampling-srs",
    "href": "slides/Chapter01.html#simple-random-sampling-srs",
    "title": "Chapter 1:Data Collection",
    "section": "Simple Random Sampling (SRS)",
    "text": "Simple Random Sampling (SRS)\n\n\n\nRandom selection of elements\n\n“Random” refers to the process not outcome\nEach (sampling) unit has same chance of being selected\nUnits can be selected with & without replacement\n\n\n\n\n\n\n\nSRS is easy to handle; suits even for a poor sampling frame\nSRS can be costly to implement\nSRS estimates are more variable than some alternatives\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#stratified-random-sampling-strs",
    "href": "slides/Chapter01.html#stratified-random-sampling-strs",
    "title": "Chapter 1:Data Collection",
    "section": "Stratified Random Sampling (STRS)",
    "text": "Stratified Random Sampling (STRS)\n\n\n\nSuitable for heterogeneous populations\nPopulation is divided into relatively homogeneous groups called strata and a random sample is taken from each stratum.\n\n\n\n\n\n\nSampling Approaches\n\nSample the larger strata more heavily (suits when all the strata are equally variable)\nSample the more varied strata are sampled\n\nAdvantages of STRS\n\nleads to efficient estimation That is, the variance (of an estimate) is usually less than that of SRS\nsample is spread throughout population\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#cluster-sampling",
    "href": "slides/Chapter01.html#cluster-sampling",
    "title": "Chapter 1:Data Collection",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\n\n\nA convenient method of sampling\npopulation is composed of clusters (groups)\nSelect certain clusters (randomly) and collect measurements from a random selection of the elements within the chosen clusters\nLarger variance than SRS!\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "href": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "title": "Chapter 1:Data Collection",
    "section": "Systematic Random sampling (SyRS)",
    "text": "Systematic Random sampling (SyRS)\n\n\n\nSelect every \\(k^{th}\\) element!\nRandom start within the first block of elements.\n\nConvenient and also the sample will be representative of population\nVariance of estimates - generally greater than those of SRS\nInefficient/inappropriate, if cycle or trend is present\n\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#other-sampling-methods",
    "href": "slides/Chapter01.html#other-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Other Sampling methods",
    "text": "Other Sampling methods\n\nProbability proportional to size (PPS)\n\ne.g., sampling high-value companies more likely than low-value companies\n\nMultistage\n\ne.g., first stage - cluster; second stage - SRS\n\nNon-probability sampling methods\n\nHaphazard / opportunistic / volunteer; take what you can get!\nSnowball; get your participants to find new participants\nPurposive; select items with certain characteristics; e.g., patients with particular symptoms\n\nNon-probability samples are often treated as random, requiring the assumption that the sample is representative. The validity of this assumption should be carefully considered."
  },
  {
    "objectID": "slides/Chapter01.html#some-sampling-methods",
    "href": "slides/Chapter01.html#some-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Some sampling methods",
    "text": "Some sampling methods"
  },
  {
    "objectID": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "href": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "title": "Chapter 1:Data Collection",
    "section": "Effective Sample size (thumb rule)",
    "text": "Effective Sample size (thumb rule)\n\n\n\n\n\n\n\n\nSample Design\nDesign Effect (\\(d\\))\nEffective Sample Size (\\(\\frac{n}{d}\\))\n\n\n\n\nSRS\n1.00\n\\(n\\)\n\n\nSTRS\n0.80 to 0.90\n\\(\\frac{n}{0.9}\\) to \\(\\frac{n}{0.8}\\)\n\n\nCluster\n1.02 to 1.26\n\\(\\frac{n}{1.26}\\) to \\(\\frac{n}{1.02}\\)\n\n\nSyRS\n1.05\n\\(\\frac{n}{1.05}\\)\n\n\nQuota\n2\n\\(\\frac{n}{2}\\)"
  },
  {
    "objectID": "slides/Chapter01.html#summary",
    "href": "slides/Chapter01.html#summary",
    "title": "Chapter 1:Data Collection",
    "section": "Summary",
    "text": "Summary\n\nIssues to address\n\nWHAT are collected?\nWHO does the data collection?\nHOW are the data collected?\n\nBias occurs due to\n\nSELECTION\nCOLLECTION\nNON-RESPONSE (the single largest cause of bias!)\n\nA sample may have the same biases as a census along with sampling errors\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter02.html#two-modes-of-data-analysis",
    "href": "slides/Chapter02.html#two-modes-of-data-analysis",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Two modes of data analysis",
    "text": "Two modes of data analysis\n\n\nHypothesis-generating\n\n“Exploratory analysis”\nAim is to explore data to discover new patterns\nResults must not be presented as formal tests of a priori hypotheses\nTesting a hypothesis using the same data that gave rise to the hypothesis is circular reasoning\n\n\nHypothesis-testing\n\n“Confirmatory analysis”\nAim is to evaluate evidence for specific a priori hypotheses\nThe hypotheses and ideas were conceived of before the data were observed\nCan be used for formal scientific inference\n\n\n\nPresenting hypothesis-generating analyses as hypothesis-testing analyses (i.e., pretending the hypotheses were conceived prior to the analysis) is scientifically dishonest, and a major contributor to the replication crisis in science."
  },
  {
    "objectID": "slides/Chapter02.html#plots-for-categorical-data",
    "href": "slides/Chapter02.html#plots-for-categorical-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Plots for categorical data",
    "text": "Plots for categorical data\nBar graphs\n\nShow the frequency of each category (level) in categorical variables\nThe height of each bar is proportional to the frequency\nCan be “stacked” or “clustered”"
  },
  {
    "objectID": "slides/Chapter02.html#tea-data",
    "href": "slides/Chapter02.html#tea-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Tea data",
    "text": "Tea data\nData from 300 individuals’ tea-drinking habits (18 questions), perceptions (12 questions), and personal details (4 questions).\n\ndata(tea, package = \"FactoMineR\")\nglimpse(tea)\n\nRows: 300\nColumns: 36\n$ breakfast        &lt;fct&gt; breakfast, breakfast, Not.breakfast, Not.breakfast, b…\n$ tea.time         &lt;fct&gt; Not.tea time, Not.tea time, tea time, Not.tea time, N…\n$ evening          &lt;fct&gt; Not.evening, Not.evening, evening, Not.evening, eveni…\n$ lunch            &lt;fct&gt; Not.lunch, Not.lunch, Not.lunch, Not.lunch, Not.lunch…\n$ dinner           &lt;fct&gt; Not.dinner, Not.dinner, dinner, dinner, Not.dinner, d…\n$ always           &lt;fct&gt; Not.always, Not.always, Not.always, Not.always, alway…\n$ home             &lt;fct&gt; home, home, home, home, home, home, home, home, home,…\n$ work             &lt;fct&gt; Not.work, Not.work, work, Not.work, Not.work, Not.wor…\n$ tearoom          &lt;fct&gt; Not.tearoom, Not.tearoom, Not.tearoom, Not.tearoom, N…\n$ friends          &lt;fct&gt; Not.friends, Not.friends, friends, Not.friends, Not.f…\n$ resto            &lt;fct&gt; Not.resto, Not.resto, resto, Not.resto, Not.resto, No…\n$ pub              &lt;fct&gt; Not.pub, Not.pub, Not.pub, Not.pub, Not.pub, Not.pub,…\n$ Tea              &lt;fct&gt; black, black, Earl Grey, Earl Grey, Earl Grey, Earl G…\n$ How              &lt;fct&gt; alone, milk, alone, alone, alone, alone, alone, milk,…\n$ sugar            &lt;fct&gt; sugar, No.sugar, No.sugar, sugar, No.sugar, No.sugar,…\n$ how              &lt;fct&gt; tea bag, tea bag, tea bag, tea bag, tea bag, tea bag,…\n$ where            &lt;fct&gt; chain store, chain store, chain store, chain store, c…\n$ price            &lt;fct&gt; p_unknown, p_variable, p_variable, p_variable, p_vari…\n$ age              &lt;int&gt; 39, 45, 47, 23, 48, 21, 37, 36, 40, 37, 32, 31, 56, 6…\n$ sex              &lt;fct&gt; M, F, F, M, M, M, M, F, M, M, M, M, M, M, M, M, M, F,…\n$ SPC              &lt;fct&gt; middle, middle, other worker, student, employee, stud…\n$ Sport            &lt;fct&gt; sportsman, sportsman, sportsman, Not.sportsman, sport…\n$ age_Q            &lt;fct&gt; 35-44, 45-59, 45-59, 15-24, 45-59, 15-24, 35-44, 35-4…\n$ frequency        &lt;fct&gt; 1/day, 1/day, +2/day, 1/day, +2/day, 1/day, 3 to 6/we…\n$ escape.exoticism &lt;fct&gt; Not.escape-exoticism, escape-exoticism, Not.escape-ex…\n$ spirituality     &lt;fct&gt; Not.spirituality, Not.spirituality, Not.spirituality,…\n$ healthy          &lt;fct&gt; healthy, healthy, healthy, healthy, Not.healthy, heal…\n$ diuretic         &lt;fct&gt; Not.diuretic, diuretic, diuretic, Not.diuretic, diure…\n$ friendliness     &lt;fct&gt; Not.friendliness, Not.friendliness, friendliness, Not…\n$ iron.absorption  &lt;fct&gt; Not.iron absorption, Not.iron absorption, Not.iron ab…\n$ feminine         &lt;fct&gt; Not.feminine, Not.feminine, Not.feminine, Not.feminin…\n$ sophisticated    &lt;fct&gt; Not.sophisticated, Not.sophisticated, Not.sophisticat…\n$ slimming         &lt;fct&gt; No.slimming, No.slimming, No.slimming, No.slimming, N…\n$ exciting         &lt;fct&gt; No.exciting, exciting, No.exciting, No.exciting, No.e…\n$ relaxing         &lt;fct&gt; No.relaxing, No.relaxing, relaxing, relaxing, relaxin…\n$ effect.on.health &lt;fct&gt; No.effect on health, No.effect on health, No.effect o…"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-one-variable",
    "href": "slides/Chapter02.html#bar-charts-one-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — one variable",
    "text": "Bar charts — one variable\n\n\nggplot(tea) +\n  geom_bar(aes(x = price)) + \n  ggtitle(\"Bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-two-variables",
    "href": "slides/Chapter02.html#bar-charts-two-variables",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — two variables",
    "text": "Bar charts — two variables\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts---flipped",
    "href": "slides/Chapter02.html#bar-charts---flipped",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts - flipped",
    "text": "Bar charts - flipped\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  aes(x = price) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nPie charts are popular but not usually the best way to show proportional data\nRequires comparison of angles or areas of different shapes\nBar charts are almost always better\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs",
    "href": "slides/Chapter02.html#one-dimensional-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nggplot(Animals) +\n  aes(x = brain) + \n  geom_dotplot() + \n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Dotplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs-1",
    "href": "slides/Chapter02.html#one-dimensional-graphs-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nAnimals |&gt; \n  mutate(\n    Animal = fct_reorder(\n      rownames(Animals), \n      brain )\n    ) |&gt; \n  ggplot() +\n  aes( y = Animal, \n       x = brain\n       ) + \n  geom_point() + \n  ylab(\"Animal\") + \n  ggtitle(\"Strip chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#histograms",
    "href": "slides/Chapter02.html#histograms",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nDivide the data range into “bins”, count the occurrences in each bin, and make a bar chart.\nY-axis can show raw counts, relative frequencies, or densities\n\nset.seed(1234); dfm &lt;- data.frame(X = rnorm(50, 100))\n\np1 &lt;- ggplot(dfm, aes(X)) + geom_histogram(bins = 20) + ylab(\"count\") + ggtitle(\"Frequency histogram\", \"Heights of the bars sum to n\")\np2 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(count/sum(count))) + geom_histogram(bins = 20) + ylab(\"relative frequency\") +\n  ggtitle(\"Relative frequency histogram\", \"Heights sum to 1\")\np3 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(density)) + geom_histogram(bins = 20) + \n  ggtitle(\"Density histogram\",\"Heights x widths sum to 1\")\n\nlibrary(patchwork); p1+p2+p3"
  },
  {
    "objectID": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "href": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Frequency polygon & kernel density plots",
    "text": "Frequency polygon & kernel density plots\n\n\nHistograma coarse visualisation of the distribution\n\nggplot(vital) + aes(Life_female) + \n  geom_histogram(bins = 12) +\n  geom_freqpoly(bins = 12)\n\n\n\n\n\n\n\n\n\nKernel densitya smooth approximation of the density\n\nggplot(vital) + aes(Life_female) +\n  geom_histogram(bins = 12, aes(y = after_stat(density))) + \n  geom_density()"
  },
  {
    "objectID": "slides/Chapter02.html#kernel-density-estimation-kde",
    "href": "slides/Chapter02.html#kernel-density-estimation-kde",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Kernel density estimation (KDE)",
    "text": "Kernel density estimation (KDE)"
  },
  {
    "objectID": "slides/Chapter02.html#summary-statistics-for-eda",
    "href": "slides/Chapter02.html#summary-statistics-for-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary statistics for EDA",
    "text": "Summary statistics for EDA"
  },
  {
    "objectID": "slides/Chapter02.html#five-number-summary",
    "href": "slides/Chapter02.html#five-number-summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Five-number summary",
    "text": "Five-number summary\nMinimum, lower hinge, median, upper hinge and maximum\n\nset.seed(1234)\nmy.data &lt;- rnorm(50, 100)\nfivenum(my.data)\n\n[1]  97.65430  99.00566  99.46477  99.98486 102.41584\n\nsummary(my.data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.65   99.01   99.46   99.55   99.96  102.42"
  },
  {
    "objectID": "slides/Chapter02.html#boxplots",
    "href": "slides/Chapter02.html#boxplots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Boxplots",
    "text": "Boxplots\n\nGraphical display of 5-number summary\nCan show several groups of data on the same graph"
  },
  {
    "objectID": "slides/Chapter02.html#cumulative-frequency-graphs",
    "href": "slides/Chapter02.html#cumulative-frequency-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Cumulative frequency graphs",
    "text": "Cumulative frequency graphs\n\nShow the left tail area\nUseful to obtain the quantiles (deciles, percentiles, quartiles etc)\n\n\n\nset.seed(123)\n\nd &lt;- data.frame(\n  x = rnorm(50, 100)\n  )\n\nggplot(d) + \n  aes(x) + \n  stat_ecdf()"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps",
    "href": "slides/Chapter02.html#shiny-apps",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nLots of examples are available\n\nIn the study guide and workshops for this course (though not all of them are working currently)\nOn the web\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.univariate.graphs/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.univariate.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "href": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) plot",
    "text": "Quantile-Quantile (Q-Q) plot\nQ-Q plots compare the distributions of two data sets by plotting their quantiles against each other.\n\n\nvital &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/vital.txt\", \n  header=TRUE, sep=\",\")\n\nquants &lt;- seq(0, 1, 0.05)\n\nvital |&gt; \n  summarise(\n    Female = quantile(Life_female, quants),\n    Male = quantile(Life_male, quants)\n  ) |&gt; \n  ggplot() +\n  aes(x = Female, y = Male) +\n  geom_point() + \n  geom_abline(slope=1, intercept=0) +\n  coord_fixed() +\n  ggtitle(\n    \"Quantiles of life expectancy\",\n    subtitle = \"are lower for males vs females\"\n    )"
  },
  {
    "objectID": "slides/Chapter02.html#some-q-q-plot-patterns",
    "href": "slides/Chapter02.html#some-q-q-plot-patterns",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Some Q-Q Plot patterns",
    "text": "Some Q-Q Plot patterns\n\nCase a: Quantiles of Y (mean/median etc) are higher than those of X\nCase b: Spread or SD of Y &gt; spread or SD of X\nCase c: X and Y follow different distributions \n\nR function: qqplot()."
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships",
    "href": "slides/Chapter02.html#bivariate-relationships",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-1",
    "href": "slides/Chapter02.html#bivariate-relationships-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_smooth(span = 0.8, se = FALSE) + \n  ggtitle(\"Scatterplot with lowess smoother\")"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-2",
    "href": "slides/Chapter02.html#bivariate-relationships-2",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_density_2d() +\n  ggtitle(\"Scatterplot with 2D density\")"
  },
  {
    "objectID": "slides/Chapter02.html#marginal-plot",
    "href": "slides/Chapter02.html#marginal-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Marginal Plot",
    "text": "Marginal Plot\nShows both bivariate relationships and univariate (marginal) distributions\n\n\np1 &lt;- ggplot(rangitikei) +\n  aes(x = people, y = vehicle) + \n  geom_point() + theme_bw()\n\nlibrary(ggExtra)\nggMarginal(p1, type=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "href": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot / scatterplot matrix",
    "text": "Pairs plot / scatterplot matrix\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1])"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "href": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot with a grouping variable",
    "text": "Pairs plot with a grouping variable\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1], \n        aes(colour = pinetree$Area))"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-coefficients",
    "href": "slides/Chapter02.html#correlation-coefficients",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation coefficients",
    "text": "Correlation coefficients\nThe Pearson correlation coefficient measures the linear association between two variables."
  },
  {
    "objectID": "slides/Chapter02.html#correlation-matrix",
    "href": "slides/Chapter02.html#correlation-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\nTo show all pairwise correlation coefficients\nUseful to explore the inter-relationship between variables \n\n\n\nlibrary(psych)\ncorr.test(pinetree[,-1])\n\n\nCall:corr.test(x = pinetree[, -1])\nCorrelation matrix \n        Top Third Second First\nTop    1.00  0.92   0.96  0.97\nThird  0.92  1.00   0.95  0.91\nSecond 0.96  0.95   1.00  0.97\nFirst  0.97  0.91   0.97  1.00\nSample Size \n[1] 60\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       Top Third Second First\nTop      0     0      0     0\nThird    0     0      0     0\nSecond   0     0      0     0\nFirst    0     0      0     0\n\n To see confidence intervals of the correlations, print with the short=FALSE option"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-plots",
    "href": "slides/Chapter02.html#correlation-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Plots",
    "text": "Correlation Plots\n\n\nlibrary(corrplot)\ncorrplot(\n  cor(pinetree[,-1]),  \n  type = \"upper\", \n  method=\"number\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#network-plots",
    "href": "slides/Chapter02.html#network-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Network plots",
    "text": "Network plots\n\n\nlibrary(corrr)\npinetree[,-1] |&gt; \n  correlate() |&gt; \n  network_plot(min_cor=0.2)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots",
    "href": "slides/Chapter02.html#d-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-1",
    "href": "slides/Chapter02.html#d-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1 + aes(colour = Area)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plotly\n\n\nlibrary(plotly)\n\nplot_ly(\n  pinetree, \n  x = ~First, \n  y = ~Second, \n  z = ~Top\n  ) |&gt; \n  add_markers()"
  },
  {
    "objectID": "slides/Chapter02.html#contour-plots",
    "href": "slides/Chapter02.html#contour-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Contour plots",
    "text": "Contour plots\n\n3D plots are difficult to interpret than 2D plots in general\nContour plots are another way of looking three variables in two dimensions\n\n\n\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots",
    "href": "slides/Chapter02.html#conditioning-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\ncoplot(Top ~ First | Second*Area, \n       data = pinetree)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots-1",
    "href": "slides/Chapter02.html#conditioning-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"mpru/ggcleveland\")\nlibrary(ggcleveland)\ngg_coplot(\n  pinetree, \n  x = First, \n  y = Top, \n  faceting = Second, \n  number_bins = 6, \n  overlap = 3/4\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#more-r-graphs",
    "href": "slides/Chapter02.html#more-r-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "More R graphs",
    "text": "More R graphs\nBuild plots in a single layout (R packages patchwork or gridExtra)\n\n\np1 &lt;- ggplot(testmarks) +\n  aes(y = English, x = Maths) + \n  geom_point()\n\np2 &lt;- p1 + \n  stat_density_2d(\n    geom = \"raster\",\n    aes(fill = after_stat(density)),\n    contour = FALSE) + \n  scale_fill_viridis_c() + \n  guides(fill=FALSE)\n\nlibrary(patchwork)\np1 / p2"
  },
  {
    "objectID": "slides/Chapter02.html#learning-eda",
    "href": "slides/Chapter02.html#learning-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Learning EDA",
    "text": "Learning EDA\n\nThe best way to learn EDA is to try many approaches and find which are informative and which are not.\n\nChatfield (1995) on tackling statistical problems:\n\nDo not attempt to analyse the data until you understand what is being measured and why. Find out whether there is prior information such as are there any likely effects.\nFind out how the data were collected.\nLook at the structure of the data.\nThe data then need to be carefully examined in an exploratory way before attempting a more sophisticated analysis.\nUse common sense, and be honest!"
  },
  {
    "objectID": "slides/Chapter02.html#summary",
    "href": "slides/Chapter02.html#summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary",
    "text": "Summary\n\nSize\n\nFor small datasets, we cannot be too confident in any patterns we see. More likely for patterns to occur ‘by chance’.\nSome displays are more affected by sample size than others\n\nShape\n\nIn can be interesting to display the overall shape of distribution.\nAre there gaps and/or many peaks (modes)?\nIs the distribution symmetrical? Is the distribution normal?\n\nOutliers\n\nBoxplots & scatterplots can reveal outliers\nMore influential than points in the middle\n\nGraphs should be simple and informative; certainly not misleading!\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter03.html#probability-and-randomness",
    "href": "slides/Chapter03.html#probability-and-randomness",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\n\n\n\nProbability and randomness are placeholders for incomplete knowledge.\nAfter I shuffled a deck of cards, you might consider the identity of the top card to be “random”.\nBut is it really?\nIf you knew the starting positions of the cards and a good HD video of my shuffling, you could surely know the positions of the cards, and which is on top.\nLikewise for rolling a die. If we know everything about the starting position, how it was thrown, the texture of the surface, humidity, etc., could we predict what it would roll?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "href": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability as a relative frequency",
    "text": "Probability as a relative frequency\n\n\nThe classical definition of probability is just the relative frequency of an event.\n\nIf a fair die is rolled, there are \\(n = 6\\) possible outcomes. Let the event of interest be getting a number 4 or more. The probability of this event is 3 out of 6 or \\(p=1/2\\).\n\nThe sample space or the set of all possible outcomes need not be finite.\n\nExample: Tossing a coin until the first head appears will result in an infinite sample space. The probability can be viewed as a limiting or long run fraction of \\(m/n\\) (i.e. when \\(n \\to \\infty\\)).\n\nWhen the sample space is finite and outcomes are equally likely, we can assume that classical probability will be the same as empirical probability.\n\nExample: To find the probability of a fair coin landing heads it is not necessary to toss the coin repeatedly and observe the proportion of heads.\n\nProbabilities can only be between \\(0\\) (impossible) and \\(1\\) (certain).\nProbabilities can be subjective (such as expert opinion, or a guess)"
  },
  {
    "objectID": "slides/Chapter03.html#mutually-exclusive-events",
    "href": "slides/Chapter03.html#mutually-exclusive-events",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\nFor mutually exclusive events,\n\nThe probability of any two events co-occurring is zero\nThe probability of one event or another event occurring is the sum of the two respective probabilities.\nThe probability of any one event not occurring is the sum of those remaining.\n\n\nExample: A randomly selected single digit can be either odd (Event \\(O\\)) or even (Event \\(E\\)).\nThe events \\(O\\) and \\(E\\) are mutually exclusive because a number cannot be both odd and even.\nThe sample space is \\(\\{0,1,2,3,4,5,6,7,8,9\\}\\).\n\n\n\n\\(\\rm{Pr(E~\\&~O)=0}\\)\n\\(\\rm{Pr(E~ or~O)=1}\\)\n\\(\\rm{Pr(E)=1-Pr(O)}\\) and \\(\\rm{Pr(O)=1-Pr(E)}\\)"
  },
  {
    "objectID": "slides/Chapter03.html#statistical-independence",
    "href": "slides/Chapter03.html#statistical-independence",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Statistical Independence",
    "text": "Statistical Independence\nIf events \\(A\\) and \\(B\\) are statistically independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\n\n\n\n\n\n\nConditional probability\n\n\n\n\\(P(A|B)\\) is the probability of event \\(A\\) occurring given that event \\(B\\) is has occurred.\nFor example, the probability of a card you’ve drawn being a 5, given that it is a spade.\nThe sample space is reduced to that where \\(B\\) (e.g. the card is a spade) has occurred.\n\n\n\n\n\nWe say that two events (\\(A\\) and \\(B\\)) are independent if \\(P(A | B) = P(A)\\) and \\(P(B | A) = P(B)\\).\nObserving event \\(A\\) doesn’t make event \\(B\\) any more or less likely, and vice versa.\nFor any two events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B ) = P(A|B) \\times P(B)\\) and \\(P(A \\textbf{ and } B ) = P(B|A) \\times P(A)\\)."
  },
  {
    "objectID": "slides/Chapter03.html#blood-group-example",
    "href": "slides/Chapter03.html#blood-group-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Blood Group Example",
    "text": "Blood Group Example\n\n\n\n Two systems for categorising blood are:\n\nthe Rh system (Rh+ and Rh–)\nthe Kell system (K+ and K–)\n\nFor any person, their blood type in any one system\nis independent of their blood type in any other.\nFor Europeans in New Zealand,\nabout 81% are Rh+ and about 8% are K+.\n\nFrom the table:\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\n\n0.0648 + 0.1748 = 0.2396\n\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\n\n0.0152"
  },
  {
    "objectID": "slides/Chapter03.html#bayes-rule",
    "href": "slides/Chapter03.html#bayes-rule",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[P(A\\mid B)=\\frac {P(B\\mid A)P(A)}{P(B)}~~~~~~~~\\rm{s.t}~~ P(B)&gt;0\\]\n\n\\(P(A\\mid B)\\) and \\(P(B\\mid A)\\) are conditional probabilities.\n\\(P(A)\\) and \\(P(B)\\) are marginal or prior probabilities."
  },
  {
    "objectID": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "href": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Prevalence, sensitivity, specificity, PPV, and NPV",
    "text": "Prevalence, sensitivity, specificity, PPV, and NPV\nLet \\(D\\) be the event of a person having the Disease and \\(H\\) be the event of a person being Healthy (i.e., not having the disease). The outcome of a test for the disease can be either positive \\((T_+)\\) or negative \\((T_-)\\).\nConsider the following definitions of conditional probabilities:\n\nprevalence is the overall probability one has the disease, or \\(P(D)\\).\nsensitivity the probability that one tests positive given one has the disease, or \\(P(T_+ | D)\\).\nspecificity the probability that one tests negative given one does not have the disease, or \\(P(T_- | H)\\).\npositive predictive value of a test is the probability one has the disease given that one has tested positive, or \\(P(D \\mid T_{+})\\)\nnegative predictive value of a test is the probability that one is healthy given that one has tested negative, or \\(P(H \\mid T_{-})\\)"
  },
  {
    "objectID": "slides/Chapter03.html#example",
    "href": "slides/Chapter03.html#example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nSay the following were true:\n\nPrevalence: \\(P(D) = 0.03\\) and \\(P(H) = 1-0.03=0.97\\)\nSensitivity: \\(P(T_+\\mid D) = 0.98\\)\nSpecificity: \\(P(T_{-}\\mid H) = 0.95\\)\n\nWe can use Bayes Rule to answer the following questions:\n\nWhat proportion of the overall population will test positive vs negative?\nWhat are the implications of a positive or negative test result?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-tree",
    "href": "slides/Chapter03.html#probability-tree",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability tree",
    "text": "Probability tree\nIt can be useful to visualise the probabilities of the four possible states using a tree diagram.\n\n\n\n\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\n\n\n\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued",
    "href": "slides/Chapter03.html#example-continued",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat proportion of the overall population will test positive vs negative?\nThe overall proportion of positive tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{+}) &= P(T_{+} \\& D) + P(T_{+} \\& H) \\\\\n&= P(T_{+} \\mid D)P(D) + P(T_{+} \\mid H)P(H) \\\\\n&= 0.98 \\times 0.03 + 0.05 \\times 0.97 \\\\\n&= 0.0779\n\\end{aligned}\n\\] The overall proportion of negative tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{-}) &= 1 - P(T_{+}) \\\\ &= 0.9221\n\\end{aligned}\n\\]\nComplete table of probabilities:\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97\n\n\n\n0.0779\n0.9221\n1"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued-1",
    "href": "slides/Chapter03.html#example-continued-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat are the implications of a positive or negative test result?\nAccording to Bayes rule, the probability of a random person having the disease given they’ve tested positive is given by:\n\\[\n\\begin{aligned}\nP(D\\mid T_{+}) &= \\frac {P(T_{+}\\mid D)P(D)} {P(T_{+})} \\\\\n&= \\frac{0.98 \\times 0.03}  {0.0779} \\\\\n&= 0.3774\n\\end{aligned}\n\\]\nAccording to Bayes rule, the probability of a random person not having the disease given they’ve tested negative is given by:\n\\[\n\\begin{aligned}\nP(H \\mid T_{-}) &= \\frac {P(T_{-} \\mid H)P(H)} {P(T_{-})} \\\\\n&= \\frac{0.95 \\times 0.97}  {0.9221} \\\\\n&= 0.9993\n\\end{aligned}\n\\]\nThe positive predictive value of the test is poor—only 38% of the subjects who tested positive will have the disease.\nThe negative predictive value is better—if a random subject tests negative, they’re very unlikely to have the disease."
  },
  {
    "objectID": "slides/Chapter03.html#discrete-probability-distributions-1",
    "href": "slides/Chapter03.html#discrete-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Discrete probability distributions",
    "text": "Discrete probability distributions\nConsider the number of eggs \\((X)\\) in an Adelie penguin’s nest. The values range from \\(1\\) to \\(5\\), each with a certain probability (or relative frequency) of occurrence.\n\n\nNote the probabilities add to \\(1\\) because \\({1,2,3,4,5}\\) is a complete sample space.\n\nThe population mean \\(\\mu_X\\) is simply the sum of each outcome multiplied by its probability.\n\\[\\mu_X = E(X)= \\sum xP(X=x)=\\sum xP(x)\\]\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\n(Mean=sum(X*P))\n\n[1] 3.15\n\n\nThe population variance is given by\n\\[Var(X)= \\sigma_X^2=\\sum (x-\\mu_X)^2 P(x)\\]\nThe population SD is simply the square-root of the variance.\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\nMean=sum(X*P)\n(Variance =sum((X-Mean)^2*P))\n\n[1] 1.4275\n\n(SD=sqrt(Variance))\n\n[1] 1.19478"
  },
  {
    "objectID": "slides/Chapter03.html#binomial-distribution",
    "href": "slides/Chapter03.html#binomial-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nConsider a variable that has two possible outcomes\n(say success and failure, with 50% probabilty each).\nThis can be described as a “Bernoulli” random variable.\nA “Binomial” is just a collection of Bernoulli trials.\nLet \\(X\\) be the number of heads when two coins are tossed.\nThe count of the number of successes \\(X\\) out of a fixed total of\n\\(n\\) independent trials follows the binomial distribution.\nThat is, \\(X \\sim Bin(n, p)\\), where \\(p\\) the probability of a success.\nThe binomial probability function \\(P(X=x)\\) or \\(P(x)\\)\nis given by \\[P(x)={n \\choose x}p^{x}(1-p)^{n-x}\\]\nFor \\(n=10\\), \\(p=0.3\\), the binomial probabilities,\n\\(P(x)\\) for \\(x=0,1,2, \\dots, 10\\), are plotted to the right.\nIf each of 10 basketball shots succeeded with probability 0.3, this describes the probability of your total score out of 10.\n\n\n\n\ndfm &lt;- data.frame(\n  x = as.factor(0:10), \n  Probability = dbinom(x = 0:10, size = 10, prob = 0.3))\nggplot(dfm) + aes(x = x, y = Probability) + geom_col() +\n  xlab(\"Number of successes (x)\") +\n  annotate(geom = \"table\", label = list(dfm), x=11, y=.05)"
  },
  {
    "objectID": "slides/Chapter03.html#example-1",
    "href": "slides/Chapter03.html#example-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nA microbiologist plates out certain bacteria on a plate, and picks out 10 colonies. She knows that the probability of successfully creating a recombinant is 0.15.\nWhat is the probability that if she mixes all 10 colonies in a growth medium with penicillin, something (anything) will grow?\nIn other words:\nIf \\(X \\sim Bin(n = 10, p = 0.15)\\), what is \\(P(x &gt; 0)\\)?\nNote \\(P(x &gt; 0)=1-P(x = 0)\\). So in R, compute this as follows:\n\n1 - dbinom(x=0, size=10, prob=.15)\n\n[1] 0.8031256\n\n\nor\n\n1-pbinom(q=0, size=10, prob=.15)\n\n[1] 0.8031256"
  },
  {
    "objectID": "slides/Chapter03.html#binomial",
    "href": "slides/Chapter03.html#binomial",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial",
    "text": "Binomial\nThe code pbinom(k,size=n,prob=p) gives the cumulative probabilities up to and including the quantile \\(k\\).\nThe Probability Mass Function (PMS) for a binomial random variable is:\n\\[P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}\\]\nThe mean and variance of the binomial random variable is given by\n\\[\\mu_X=np~~~~ \\sigma^2_X=np(1-p)\\]\nIn the last example, the expected number of recombinant strain of bacteria is\n\\[\\mu_X=np=10*0.15=1.5\\]\nwith standard deviation\n\\[\\sigma_X=\\sqrt {np(1-p)}=1.129159\\]"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution",
    "href": "slides/Chapter03.html#poisson-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to obtain the probabilities of counts of relatively rare events that occur independently in space or time.\nSome Examples:\n\nThe number of snails in a quadrat \\((1~m^2)\\)\nFish counts in a visual transect (25m x 5m)\nBacterial colonies in 2 litres of milk"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution-1",
    "href": "slides/Chapter03.html#poisson-distribution-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe random variable \\(X\\), the number of occurrences (count), often follows the Poisson distribution whose probability function is given by\n\\[\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty\\]\nThe parameter \\(\\lambda\\) is the mean which is also equal to the variance.\n\\[\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda\\]\nMain assumptions:\n\nThe events occur at a constant average rate of \\(\\lambda\\) per unit time or space.\nOccurrences are independent of one another as well as they do not happen at exactly the same unit time or space."
  },
  {
    "objectID": "slides/Chapter03.html#poisson-example",
    "href": "slides/Chapter03.html#poisson-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson example",
    "text": "Poisson example\n\n\n\n\n\n\n\n\n\n\n\n Consider the number of changes that accumulate along a\nstretch of a neutrally evolving gene over a given period of time.\nThis is a Poisson random variable with a\npopulation mean of \\(\\lambda=kt\\), where\n\\(k\\) is the number of mutations per generation, and\n\\(t\\) is the time in generations that has elapsed.\n     \nAssume that \\(k = 1\\times10^{-4}\\) and \\(t = 500\\).\nFor \\(\\lambda=kt=0.05\\), the Poisson probabilities are shown in the following plot.\nWhat is the probability that at least one mutation has occurred over this period?\n\\(P(x &gt; 0)=1-P(x = 0)\\) is found in R as follows:\n\n1 - dpois(x=0, lambda=0.05)\n\n[1] 0.04877058"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-1",
    "href": "slides/Chapter03.html#continuous-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nA discrete random variable takes values which are simply points on a real line. In other words, there is an inherent discontinuity in the values a discrete random variable can take.\nIf a random variable, \\(X\\), can take any value (i.e., not just integers) in some interval of the real line, it is called a continuous random variable.\nE.g., height, weight, length, percentage protein\nFor a discrete random variable \\(X\\), the associated probabilities \\(P(X=x)\\) are also just points or masses, and hence the probability function \\(P(x)\\) is also called as the probability mass function (PMF).\nFor continuous random variables, probabilities can be computed when the variable falls in an interval such as \\(5\\) to \\(15\\), but not when it takes a fixed value such as \\(10\\) (which is equal to zero).\nThe Probability Density Function (PDF) gives the relative likelihood of any particular value."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-2",
    "href": "slides/Chapter03.html#continuous-probability-distributions-2",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\n\nFor example, consider a random proportion \\((X)\\) between \\(0\\) and \\(1\\). Here \\(X\\) follows a (standard) continuous uniform distribution whose (probability) density function \\(f(x)\\) is defined as follows:\n\\[f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x&lt;0\\ \\mathrm {or} \\ x&gt;1\\end{cases}\\] This constant density function is the simple one in the graph to the right.\n\n\n\ntibble(x = seq(-.5, 1.5, length=1000),\n       `f(x)` = dunif(x, min=0, max=1)) |&gt; \n  ggplot() + \n  aes(x = x, y = `f(x)`) +\n  geom_area(colour = 1, alpha = .2)\n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-3",
    "href": "slides/Chapter03.html#continuous-probability-distributions-3",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n  \np\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-4",
    "href": "slides/Chapter03.html#continuous-probability-distributions-4",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  annotate(geom = \"path\", \n    x = c(19.3, 19.3, 13), \n    y = c(0, rep(dnorm(19.3,20,2),2) ),\n    arrow = arrow(),\n    colour = \"dodgerblue4\", size = 1.1)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe density at 19.3 is \\(f(19.3) = 0.1876\\)).\n\ndnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.1876202"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-5",
    "href": "slides/Chapter03.html#continuous-probability-distributions-5",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  geom_area(\n    data = d |&gt; filter(x &lt;= 19.3),\n    fill = \"dodgerblue4\",\n    size  = 1.1, alpha = .6)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe area under the curve to the left of the value 19.3 is given by the Cumulative Density Function (CDF), or \\(F(x)\\). It gives the probability that x &lt; 19.3; \\(F(19.3) = 0.3632\\).\n\npnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.3631693"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-6",
    "href": "slides/Chapter03.html#continuous-probability-distributions-6",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe cumulative distribution function, CDF, \\(F(x)\\) gives the left tail area or probability up to \\(x\\). This is probability is found as\n\\[F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt\\] The relationship between the density function \\(f(x)\\) and the distribution function \\(F(x)\\) is given by the Fundamental Theorem of Calculus.\n\\[f(x)={dF(x) \\over dx}\\]"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-7",
    "href": "slides/Chapter03.html#continuous-probability-distributions-7",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe total area under the PDF curve is \\(1\\). The probability of obtaining a value between two points (\\(a\\) and \\(b\\)) is the area under the PDF curve between those two points. This probability is given by \\(F(b)-F(a)\\).\nFor the uniform distribution \\(U(0,1)\\), \\(f(x)=1\\). So\n\\[F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x\\]\nFor example, the probability of a randomly drawn fraction from the interval \\([0,1]\\) to fall below \\(x=0.5\\) is 50%.\nThe probability of a random fraction falling between \\(a=0.2\\) and \\(b=0.8\\) is\n\\[F(b)-F(a)=0.8-0.2=0.6\\]\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "href": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "The Normal (Gaussian) Distribution",
    "text": "The Normal (Gaussian) Distribution\n\n\nThe Gaussian or Normal Distribution is parameterised in terms of the mean \\(\\mu\\) and the variance \\(\\sigma ^{2}\\) and its Probability Density Function (PDF) is given by\n\\[f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\] A Standard Normal Distribution has mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It has a simpler PDF:\n\\[f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}\\] If \\(X \\sim N(\\mu, \\sigma)\\), you can convert the \\(X\\) values into \\(Z\\)-scores by subtracting the mean \\(\\mu\\) and dividing by the standard deviation \\(\\sigma\\).\n\\[Z={\\frac {X-\\mu }{\\sigma }}\\]\nWe often deal with the standard normal because the symmetric bell shape of the normal distribution remains the same for all \\(\\mu\\) and \\(\\sigma\\).\n\n\n \n\ndfn &lt;- tibble(x=seq(-4,4,length=1000), \n              `f(x)` = dnorm(x), \n              `F(x)` = pnorm(x))\np1 &lt;- ggplot(dfn) + aes(x=x,y=`f(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np2 &lt;- ggplot(dfn) + aes(x=x,y=`F(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Cumulative Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#example-of-a-normal",
    "href": "slides/Chapter03.html#example-of-a-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example of a normal",
    "text": "Example of a normal\n\nThe weight of an individual of Amphibola crenata, a marine snail,\nis normally distributed with a mean of \\(40g\\) and variance of \\(20g^2\\).\n\n  \n\ndfs &lt;- tibble(x=seq(20, 60, length=1000), \n    `f(x)` = dnorm(x, mean=40, sd=sqrt(20)))\n\nps &lt;- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs between \\(35g\\) and \\(50g\\)?\n\n\n\n\nIn R, the function pnorm() gives the CDF.\n\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n\n[1] 0.8555501\n\n\n\n\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &lt; 50 & x &gt; 35),\n            fill=\"coral1\", alpha=.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs below \\(35g\\) or over \\(50g\\)?\n\n\n\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n\n[1] 0.1444499\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &gt; 50),\n            fill=\"coral1\", alpha=.5) + \n  geom_area(data = dfs |&gt; filter(x &lt; 35),\n            fill=\"coral1\", alpha=.5)"
  },
  {
    "objectID": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "href": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Areas (probabilities) under the standard normal",
    "text": "Areas (probabilities) under the standard normal\nUnder standard normal, the areas under the PDF curve are shown below for various situations.\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#small-sample-effect",
    "href": "slides/Chapter03.html#small-sample-effect",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Small sample effect",
    "text": "Small sample effect\nFor small samples, the shape might be difficult to judge.\n\n\nset.seed(1234)\ndfm &lt;- data.frame(\n  x=rnorm(50, \n          mean=80, \n          sd=12)\n  )\n\np1 &lt;- ggplot(dfm) + \n  geom_histogram(\n    aes(x=x, y=after_stat(density)), \n    colour=1\n    ) + \n  stat_function(\n    fun = dnorm, \n    args = list(mean = 80, sd = 12), \n    geom = \"line\"\n    ) +\n  xlim(min(dfm), max(dfm))\n\np2 &lt;- ggplot(dfm) + aes(x) + \n  geom_boxplot() +\n  xlim(min(dfm), max(dfm)) +\n  theme_void()\n\nlibrary(patchwork)\np1 / p2 + plot_layout(heights = c(5, 1))\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#normal-quantile-plots",
    "href": "slides/Chapter03.html#normal-quantile-plots",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Normal quantile plots",
    "text": "Normal quantile plots\nIn a normal quantile plot, the quantiles of the sample are plotted against the theoretical quantiles of the fitted normal distribution.\nThe points should roughly lie on a straight line\nWe can also compare the empirical and theoretical CDFs.\n\n\n\n\n\n\n\n\n\nTV viewing time data\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "href": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Which distribution is most appropriate?",
    "text": "Which distribution is most appropriate?\n\nRemember, theoretical distributions aren’t real—they’re just models—but they can be useful. Keep your purpose in mind.\nChoose the simplest distribution that provides an adequate fit.\nData may be best served by a mixture of two or more distributions rather than a single distribution.\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-1",
    "href": "slides/Chapter04.html#statistical-inference-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nThe term statistical inference means that we are using properties of a sample to make statements about the population from which the sample was drawn.\nFor example, say you wanted to know the mean length of the leaves on a tree (\\(\\mu\\)). You wouldn’t want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths (\\(x_i\\)), calculate the sample mean (\\(\\bar x\\)), and use \\(\\bar x\\) as an estimate of \\(\\mu\\)."
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "href": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference for a Population Mean",
    "text": "Statistical Inference for a Population Mean\n\n\nConsider a population \\(X\\) with mean \\(\\mu\\),\nvariance \\(\\text{Var}(X)=\\sigma^2\\), and\nstandard deviation \\(\\sigma\\).\nWe can estimate \\(\\mu\\) by:\n\ntaking a random sample of values \\(\\{x_1, x_2, ..., x_n\\}\\) from the population, \\(X\\), and\ncalculating the sample mean \\(\\bar x = \\frac 1 n \\sum_{i=1}^{n}x_i\\).\n\n\n\n\n\nImportantly:\n\nThe sample mean \\(\\bar x\\) is a single draw from a random variable \\(\\bar X\\). It will never be exactly equal to the population mean, \\(\\mu\\), unless you measure every single leaf.\nThere is uncertainty in our estimate of \\(\\mu\\). Each sample yields a different \\(\\bar x\\). This is called sampling error."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-variation",
    "href": "slides/Chapter04.html#sampling-variation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Variation",
    "text": "Sampling Variation"
  },
  {
    "objectID": "slides/Chapter04.html#sampling-error",
    "href": "slides/Chapter04.html#sampling-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Error",
    "text": "Sampling Error\nThe variability of the sample means from sample to sample, \\(\\text{Var}(\\bar X)\\), depends on two things:\n\nthe variability of the population values, \\(\\text{Var}(x)\\), and\nthe size of the sample, \\(n\\).\n\nThe variability of the sample estimate of a parameter is usually expressed as a standard error (SE), which is simply the theoretical standard deviation of \\(\\bar X\\) from sample to sample.\nThe equation is surprisingly simple!\n\\(\\text{Var}(\\bar X) = \\text{Var}(X)/n\\)\n\\(\\text{SE}(\\bar X) = \\text{SD}(\\bar X) = \\sigma/\\sqrt n\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset",
    "href": "slides/Chapter04.html#quetelets-dataset",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\nIn 1846, a Belgian scholar Adolphe Quetelet published an analysis of the chest sizes of a full population of 5,738 Scottish soldiers.\nThe distribution of the measurements (in inches) in his database has a mean of \\(\\mu\\) = 39.83 inches, a standard deviation of \\(\\sigma\\) = 2.05 inches, and is well approximated by a normal distribution.\n\n\n\n\nAdolphe Quetelet1796-1874\n\n\n\n\n\n\nqd &lt;- tibble(\n  Chest = 33:48, \n  Count = c(3, 18, 81, 185, 420, 749, 1073, 1079, \n            934, 658, 370, 92, 50, 21, 4, 1),\n  Prob = Count / sum(Count)\n  )\n\nqd |&gt; \n  ggplot() +\n  aes(x = Chest, y = Count) +\n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"Chest circumferences of Scottish soldiers\")"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-1",
    "href": "slides/Chapter04.html#quetelets-dataset-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# A single sample\nsample(qd_long, size = 6)\n\n[1] 39 40 42 40 37 37\n\n\n\n\n\n# Ten samples\nmap(1:7, ~ sample(qd_long, size = 6))\n\n[[1]]\n[1] 40 43 46 41 40 43\n\n[[2]]\n[1] 43 38 38 39 39 37\n\n[[3]]\n[1] 37 43 40 41 43 43\n\n[[4]]\n[1] 42 37 39 39 43 40\n\n[[5]]\n[1] 43 39 37 40 41 39\n\n[[6]]\n[1] 41 43 41 36 39 40\n\n[[7]]\n[1] 42 39 41 37 38 38"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-2",
    "href": "slides/Chapter04.html#quetelets-dataset-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\nhead(q_samples, 15)\n\n# A tibble: 15 × 2\n   sample values\n   &lt;fct&gt;   &lt;int&gt;\n 1 1          37\n 2 1          39\n 3 1          40\n 4 1          41\n 5 1          41\n 6 1          38\n 7 2          39\n 8 2          42\n 9 2          39\n10 2          41\n11 2          45\n12 2          38\n13 3          38\n14 3          41\n15 3          35"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-3",
    "href": "slides/Chapter04.html#quetelets-dataset-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\n\n\n# Calculate means of each sample\nsample_means &lt;- q_samples |&gt; \n  group_by(sample) |&gt; \n  summarise(mean = mean(values))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample  mean\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 1       40.3\n 2 2       40.2\n 3 3       40.7\n 4 4       39.7\n 5 5       40  \n 6 6       39.5\n 7 7       41.3\n 8 8       40.7\n 9 9       40.3\n10 10      39.5"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-4",
    "href": "slides/Chapter04.html#quetelets-dataset-4",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\n\n# Make a function to do it all and plot\nplot_samples &lt;- function(dat = qd_long,\n                         no_samples = 12,\n                         sample_size = 6) {\n\n  # Put samples in a tibble\n  q_samples &lt;- tibble( \n    sample = as_factor(1:no_samples),\n    values = map(sample, ~ sample(dat, size = sample_size))\n    ) |&gt; unnest()\n  \n  # Calculate means of each sample\n  sample_means &lt;- q_samples |&gt; group_by(sample) |&gt; \n    summarise(mean = mean(values))\n  \n  ggplot() + \n    xlim(min(dat), max(dat)) +\n    geom_vline(xintercept = mean(dat), alpha = .4) +\n    geom_jitter(\n      data = q_samples,\n      mapping = aes(y = sample, x = values),\n      width = 0, height = 0.1, alpha = .8\n      ) + \n    geom_point(\n      data = sample_means,\n      mapping = aes(y = sample, x = mean),\n      shape = 15, size = 3, \n      colour = \"dark orange\"\n      ) +\n    ggtitle(paste(\"Sample size =\", sample_size))\n}\n\n\n\nplot_samples(sample_size = 6)"
  },
  {
    "objectID": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "href": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sample means and sample sizes",
    "text": "Sample means and sample sizes\n\n\nLet’s compare the distribution of means for different sample sizes across the samples.\n\nplot_samples(sample_size = 3)\nplot_samples(sample_size = 5)\nplot_samples(sample_size = 10)\nplot_samples(sample_size = 20)\nplot_samples(sample_size = 50)\nplot_samples(sample_size = 100)\nplot_samples(sample_size = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe larger the \\(n\\), the less the sample means vary."
  },
  {
    "objectID": "slides/Chapter04.html#standard-error",
    "href": "slides/Chapter04.html#standard-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error",
    "text": "Standard error\nIf is distributed as a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[X \\sim \\text{Normal}(\\mu, \\sigma)\\]\nthen sample means of size \\(n\\) are distributed as\n\\[\\bar X \\sim \\text{Normal}(\\mu_\\bar{X} = \\mu, \\sigma_\\bar{X} = \\sigma/\\sqrt{n})\\] \\(\\text{SE}(\\bar{X}) = \\sigma_\\bar{X}= \\sigma/\\sqrt{n})\\) is known as the standard error of the sample mean.\nMore generally, a standard error is the standard deviation of an estimated parameter over \\(\\infty\\) theoretical samples.\nAccording to the Central Limit Theorem (CLT), raw \\(X\\) values don’t have to be normally distributed for the sample means to be normally distributed (for any decent sample size)!"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Standard error of means of samples",
    "text": "Quetelet’s chests: Standard error of means of samples\nQuestion:\nIf Quetelet’s chest circumferences are distributed as \\(X \\sim \\text{N}(\\mu=39.83, \\sigma=2.05)\\), then how variable are the means of samples of size \\(n\\) = 6?\nAnswer:\n\\[\n\\begin{aligned}\nSE(\\bar{X}_{n=6})&=\\sigma/\\sqrt{n} \\\\\n&=2.05/\\sqrt{6} \\\\\n&=0.8369\n\\end{aligned}\n\\] Sample means of size \\(n\\) = 6 would be distributed as\n\\(\\bar{X}_{n=6} \\sim \\text{N}(\\mu=39.83, \\sigma=0.8369)\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Distribution of means of samples",
    "text": "Quetelet’s chests: Distribution of means of samples"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals",
    "href": "slides/Chapter04.html#confidence-intervals",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nAnother common way of expressing uncertainty when making statistical inferences is to present:\n\na point estimate (e.g., the sample mean) and\na confidence interval around that estimate.\n\nA confidence interval gives an indication of the sampling error.\nA 95% confidence interval is constructed so that intervals from 95% of samples will contain the true population parameter.\nIn reality the interval either contains the true value or not, so you must not interpret a confidence interval as “there’s a 95% probability that the interval contains the true parameter”.\nWe can say:\n\n“95% of so-constructed intervals will contain the true value of the parameter” or\n“with 95% confidence, the interval contains the true value of the parameter”."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-1",
    "href": "slides/Chapter04.html#confidence-intervals-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nBecause we know the population parameters for the Quetelet dataset, we can calculate where 95% of sample means will lie for any particular sample size.\nRecall that, for a normal distribution, 95% of values lie between \\(\\mu \\pm 1.96\\times\\sigma\\)\n(i.e., \\(\\mu - 1.96\\times\\sigma\\) and \\(\\mu + 1.96\\times\\sigma\\)).\nIt follows that 95% of means of samples of size \\(n\\) will lie within \\(\\mu \\pm 1.96 \\times \\sigma/\\sqrt{n}\\).\nFor example, for samples of size 6 from the Quetelet dataset, 95% of means will lie within \\(39.83 \\pm 1.96\\times 2.05 / \\sqrt 6\\),\nso \\(\\{ 37.02 , 42.64\\}\\)."
  },
  {
    "objectID": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "href": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Estimating the distribution of sample means from data",
    "text": "Estimating the distribution of sample means from data\n\n\nIt’s all very well deriving the distribution of sample means when we know the population parameters (\\(\\mu\\) and \\(\\sigma\\)) but in most cases we only have our one sample.\nWe don’t know any of the population parameters. We have to estimate the population mean (usually denoted \\(\\hat\\mu\\) or \\(\\bar x\\)) and estimate of the population standard deviation (usually denoted \\(\\hat\\sigma\\) or \\(s\\)) from the sample data.\nThis additional uncertainty complicates things a bit. In fact, we can’t even use the normal distribution any more!"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution",
    "href": "slides/Chapter04.html#the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\n\n\nIntroducing William Sealy Gosset, who described the t distribution after working with random numbers.\nGosset was a chemist and mathematician who worked for the Guinness brewery in Dublin.\nGuinness forbade anyone from publishing under their own names so that competing breweries wouldn’t know what they were up to, so he published his discovery in 1908 under the penname “Student”. Student’s identity was only revealed when he died. It is therefore often called “Student’s t distribution”.\n\n\n\n\n\nWillam Sealy Gosset1876-1937"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-1",
    "href": "slides/Chapter04.html#the-t-distribution-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\nThe t distribution is like the standard normal. It is bell-shaped and symmetric with mean = 0, but it has fatter tails (greater uncertainty) due to the fact that we do not know \\(\\sigma\\); we have to estimate it.\nThe t distribution is not just a single distribution. It is really an entire series of distributions which is indexed by something called the “degrees of freedom” (or \\(df\\)).\n\n\n\n\nAs \\(df \\rightarrow \\infty\\), \\(t \\rightarrow Z\\).\n\n\np &lt;- expand_grid(\n  df = c(2, 5, Inf),\n  x = seq(-4, 4, by = .01)\n  ) |&gt; \n  mutate(\n    Density = dt(x = x, df = df),\n    `degrees of freedom` = as_factor(df)\n  ) |&gt; \n  ggplot() +\n  aes(x = x, y = Density, \n      group = `degrees of freedom`, \n      colour = `degrees of freedom`) +\n  geom_line()"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-2",
    "href": "slides/Chapter04.html#the-t-distribution-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\nFor a sample of size \\(n\\), if\n\n\\(X\\) is a normal random variable with mean \\(\\mu\\),\n\\(\\bar X\\) is the sample mean, and\n\\(s\\) is the sample standard deviation,\n\nthen the variable:\n\\[\nT = \\frac{\\bar X - \\mu} {s/\\sqrt{n}}\n\\]\nis distributed as a \\(t\\) distribution with \\((n – 1)\\) degrees of freedom."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "href": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals and the t distribution",
    "text": "Confidence intervals and the t distribution\nThe process of using sample data to try and make useful statements about an unknown parameter, \\(\\theta\\), is called statistical inference.\nA confidence interval for the true value of a parameter is often obtained by:\n\\[\n\\hat \\theta \\pm t \\times \\text{SE}(\\hat \\theta)\n\\] where \\(\\hat \\theta\\) is the sample estimate of \\(\\theta\\) and \\(t\\) is a quantile from the \\(t\\) distribution with the appropriate degrees of freedom.\nFor example, to get the \\(t\\) score for a 95% confidence interval with 9 degrees of freedom:\n\nqt(p = 0.975, df = 9)\n\n[1] 2.262157\n\n\nThe piece that is being added and subtracted, \\(t \\times \\text{SE}(\\hat \\theta)\\), is often called the margin of error."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean\nWe can calculate a 95% confidence interval for a sample mean with the following information:\n\n\nsample mean \\(\\bar x\\),\nsample standard deviation \\(s\\),\nthe sample size \\(n\\), and\nthe 0.025th quantile of the \\(t\\) distribution with degrees of freedom \\(df=n-1\\).\n\n\n\n\nA sample of size \\(n\\) = 6 from Quetelet data\n\nn1 &lt;- 6; df1 &lt;- n1-1\n( dq1 &lt;- sample(qd_long, size = n1) )\n\n[1] 39 42 38 36 40 38\n\n( mean1 &lt;- mean(dq1) )\n\n[1] 38.83333\n\n( sd1 &lt;- sd(dq1) )\n\n[1] 2.041241\n\n( t1 &lt;- qt(c(0.025, 0.975), df = df1) )\n\n[1] -2.570582  2.570582\n\nmean1 + t1 * sd1 / sqrt(n1)\n\n[1] 36.69118 40.97548\n\n\n\n\n\n\nOr, more simply…\n\nt.test(dq1)\n\n\n    One Sample t-test\n\ndata:  dq1\nt = 46.6, df = 5, p-value = 8.595e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.69118 40.97548\nsample estimates:\nmean of x \n 38.83333"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "href": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for sample means",
    "text": "Confidence intervals for sample means"
  },
  {
    "objectID": "slides/Chapter04.html#t-as-a-test-statistic",
    "href": "slides/Chapter04.html#t-as-a-test-statistic",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "t as a test statistic",
    "text": "t as a test statistic\nThis method can be used when the estimator \\(\\hat\\theta\\) is approximately normally distributed and\n\\[\n\\frac{\\hat\\theta - \\theta} {\\text{SE}(\\hat \\theta)}\n\\]\nhas approximately a Student’s t distribution.\nThis paves the way for hypothesis testing for specific values of \\(\\theta\\). Many of the methods you will learn in this course are based on this general rule."
  },
  {
    "objectID": "slides/Chapter04.html#testing-hypotheses",
    "href": "slides/Chapter04.html#testing-hypotheses",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Testing hypotheses",
    "text": "Testing hypotheses\n\nTesting hypotheses underpins a lot of scientific work.\nA hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.\nFor logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:\n\nspecify a null hypotheses, which must be true if our hypothesis of interest is false, and\nmeasure the evidence against the null hypothesis, usually in the form of a p-value."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nSay a farmer has 9 paddocks of alfalfa. He’s hired a new manager, and wants to know if, on average, this year’s crop is different to last year’s. He measures the yield for each paddock in year 1 and year 2, and calculates the difference.\n\nyear1 &lt;- c(0.8, 1.3, 1.7, 1.7, 1.8, 2.0, 2.0, 2.0, 2.2)\nyear2 &lt;- c(0.7, 1.4, 1.8, 1.8, 2.0, 2.0, 2.1, 2.1, 2.2)\ndiff &lt;- year2 - year1\n( xbar &lt;- mean(diff) ) ; ( s &lt;- sd(diff) )\n\n[1] 0.06666667\n\n\n[1] 0.08660254\n\n\nThe mean difference is 0.067. On average, yields were 0.067 greater than last year.\nIs that convincing different from zero?\nHow likely is such a difference to have arisen just by chance, and really, if we had a million paddocks, there would be no difference?\nWhat is the probability of seeing a difference of 0.067 or more in our dataset if the true mean were zero?\nThese questions can be addressed with a \\(p\\)-value from a hypothesis test."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nWe can test the null hypothesis using a t statistic \\(t_0 = \\frac{\\bar x - \\mu_0} {\\text{SE}(\\bar x)}\\), where\n\\(\\ \\ \\ \\ \\ \\mu_0 = 0\\) is the hypothesised value of the mean and\n\\(\\ \\ \\ \\ \\ \\text{SE}(\\bar x) = s/\\sqrt{n}\\) is the (estimated) standard error of the sample mean.\n\n( t0 &lt;- (xbar - 0) / ( s / sqrt(9) ) )\n\n[1] 2.309401\n\n\nThe p-value is \\(\\text{Pr}(|t_{df=8}| &gt; t_0)\\)\n\n2 * pt(t0, df = 8, lower = F)\n\n[1] 0.04973556"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nThis can be done more easily:\n\nt.test(diff)\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.3094, df = 8, p-value = 0.04974\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 9.806126e-05 1.332353e-01\nsample estimates:\n mean of x \n0.06666667"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe \\(p\\)-value of 0.05 means that, if the null hypothesis were true, only 5% of sample means would be as or more extreme than the observed value of 0.067. It is the area in orange in the graph below.\nWe can therefore reject the null hypothesis at the conventional 5% level, and conclude that, on average, yields were indeed higher this year.\nThis is an example of a paired t test. They two samples (year 1 and year 2) are not independent of one another because we have the same paddocks in both years. So, we take the differences, treat them like a single sample, and do a one-sample t test for the mean difference being zero.\n\\(p\\)-values are random variables too.\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes",
    "href": "slides/Chapter04.html#truth-and-outcomes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nIn reality, the null hypothesis is either true or false.\nThe outcome of a test is either we reject the null hypothesis, or we fail to reject the null hypothesis (note, we never “confirm” or “accept” the null hypothesis – absence of evidence is not evidence of absence!).\nThis gives us four possibilities:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect result\n\n\nDo not reject \\(H_0\\)\nCorrect result\nType II error\n\n\n\nWith probabilities usually represented by:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\n\\(\\alpha\\)\n\\(1-\\beta\\) = “power”\n\n\nDo not reject \\(H_0\\)\n\\(1 - \\alpha\\)\n\\(\\beta\\)"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes-1",
    "href": "slides/Chapter04.html#truth-and-outcomes-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nThe probability of making a Type I error, rejecting \\(H_0\\) when \\(H_0\\) is true, is set by the experimenter a priori (beforehand) as the significance level, \\(\\alpha\\).\n\nSay we set \\(\\alpha\\) at the conventional 0.05. We know that, if \\(H_0\\) is true, we have a 5% chance of rejecting \\(H_0\\) (the Type I error rate is 0.05) and a 95% chance of not rejecting \\(H_0\\).\n\nThe probability of retaining (not rejecting) \\(H_0\\) when \\(H_0\\) is false is usually represented by \\(\\beta\\) (“beta”).\n\nWe usually do not know \\(\\beta\\) because it depends on \\(\\sigma\\), \\(n\\) and also the effect size under the alternative hypothesis. These must be asserted to calculate \\(\\beta\\) and power, \\(1-\\beta\\)."
  },
  {
    "objectID": "slides/Chapter04.html#calculating-power",
    "href": "slides/Chapter04.html#calculating-power",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Calculating power",
    "text": "Calculating power\nThe power of a hypothesis test is the probability of rejecting the null hypothesis if it is indeed false. Basically, “if we’re right, what is the probability that we’ll be able to show it?”\nThe power of the \\(t\\) test can be evaluated using R if you assert the effect size \\(\\delta\\):\n\n\n\npower.t.test(n = 10, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 10\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.5619846\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.9986074\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nThe bigger the sample size, the bigger more power we have to detect an effect.\nOften, experimenters might aim to have a power of 80% or more, so they will most likely reject the null if it is false."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-distributions",
    "href": "slides/Chapter04.html#sampling-distributions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nThe term sampling distribution means the distribution of the computed statistic such as the sample mean when sampling is repeated many times.\nFor a normal population,\n\nStudent’s \\(t\\) distribution is the sampling distribution of the mean (after rescaling).\n\\(\\chi^2\\) distribution is the sampling distribution of the sample variance \\(S^2\\).\n\\((n-1)S^2/\\sigma^2\\) follows \\(\\chi^2\\) distribution.\n\n\\(F\\) distribution is ratio of two \\(\\chi^2\\) distributions.\n\nIt becomes the sampling distribution of the ratio of two sample variances \\(S_1^2/S_2^2\\) from two normal populations (after scaling).\n\n\\(t\\) distribution is symmetric but \\(\\chi^2\\) and \\(F\\) distributions are right skewed. - For large samples, they become normal - For \\(n&gt;30\\), the skew will diminish\nFor the three sampling distributions, the sample size \\(n\\) becomes the proxy parameter, called the degrees of freedom (df).\n\n\\(t_{n-1}\\), \\(\\chi_{n-1}^2\\) & \\(F_{(n_1-1),(n_2-1) }\\)"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test",
    "href": "slides/Chapter04.html#two-sample-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nThe two-sample t-test is a common statistical test. You have two populations, \\(X_1\\) and \\(X_2\\), with means \\(\\mu_1\\) and \\(\\mu_2\\) and variances \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\), respectively.\nWe wish to test whether the two population means are equal.\nNull hypothesis: \\(H_0:\\mu=\\mu_1=\\mu_2\\)\nTwo-sided alternative hypothesis: \\(H_1:\\mu_1 \\neq \\mu_2\\)\nThe basic t-test assumes equal variances; that is, \\(\\sigma^2_1 = \\sigma^2_2\\).\nIf we make this assumption and it is false, the test may give the wrong conclusion!"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with equal variances",
    "text": "Two sample t-test with equal variances\nUnder the assumption of equal variances \\(\\sigma^2_1 = \\sigma^2_2\\), we can perform a pooled-sample t-test.\nWe calculate the estimated pooled variance as:\n\\[s_{p}^{2} = w_{1} s_{1}^{2} +w_{2} s_{2}^{2}\\]\nwhere the weights are \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\),\nand \\(n_1\\) and \\(n_2\\) are the sample sizes for samples 1 and 2, respectively.\nFor the pooled case, the \\(df\\) for the \\(t\\)-test is simply\n\\[df = n_{1}+n_{2}-2\\]"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with unequal variances",
    "text": "Two sample t-test with unequal variances\nThis is often called the “Welch test”.\nWe do not need to calculated the pooled variance \\(s^2_p\\);\nwe simply use \\(s^2_1\\) and \\(s^2_2\\) as they are.\nFor the unpooled case, the \\(df\\) for the test is smaller: \\[df=\\frac{\\left(\\frac{s_{1}^{2}}{n_{1}} +\\frac{s_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{s_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{s_{2}^{2}}{n_{2} } \\right)^{2}}\\] The \\(df\\) doesn’t have to be an integer in this case."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nWe can test the null hypothesis that the variances are equal using either a Bartlett’s test or Levene’s test. Levene’s is generally favourable over Bartlett’s.\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\ncar::leveneTest(TELETIME~factor(SEX), data=tv)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.1789 0.08149 .\n      44                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHighish p-value means there’s no strong evidence against the null hypothesis that the variances are equal. Therefore, we might feel happy to assume equal variances and use a pooled variance estimate."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nt-test assuming equal variances:\n\nt.test(TELETIME ~ factor(SEX), var.equal = TRUE, data=tv)\n\n\n    Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 44, p-value = 0.4723\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -461.3476  217.2606\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304 \n\n\nt-test not assuming equal variances (Welch test):\n\nt.test(TELETIME ~ factor(SEX), data=tv)\n\n\n    Welch Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 40.653, p-value = 0.4727\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -462.1384  218.0514\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "href": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps (some not currently working)",
    "text": "Shiny apps (some not currently working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/\nFor more on non-parametric tests, see Study Guide."
  },
  {
    "objectID": "slides/Chapter04.html#test-of-proportions",
    "href": "slides/Chapter04.html#test-of-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Test of proportions",
    "text": "Test of proportions\nTesting the null hypothesis that a proportion \\(p=0.5\\)\nAlternative hypothesis: \\(p \\ne 0.5\\)\nSay a survey of 1000 beached whales had 450 females.\n\n\nIs the population 50:50?\n\nprop.test(450, 1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  450 out of 1000, null probability 0.5\nX-squared = 9.801, df = 1, p-value = 0.001744\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4189204 0.4814685\nsample estimates:\n   p \n0.45 \n\n\n\nAn exact version of the test\n\nbinom.test(c(450, 550))\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value = 0.001731\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45 \n\n\n\n\nTo test for a proportion other than 0.5\n\nbinom.test(c(450, 550), p = 3/4)\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45"
  },
  {
    "objectID": "slides/Chapter04.html#comparing-several-proportions",
    "href": "slides/Chapter04.html#comparing-several-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Comparing several proportions",
    "text": "Comparing several proportions\nData on smokers in four group of patients\n(from Fleiss, 1981, Statistical methods for rates and proportions).\n\nsmokers  &lt;- c( 83, 90, 129, 70 )\npatients &lt;- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  smokers out of patients\nX-squared = 12.6, df = 3, p-value = 0.005585\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9651163 0.9677419 0.9485294 0.8536585 \n\n\nMore on chi-squared tests next week."
  },
  {
    "objectID": "slides/Chapter04.html#tests-for-normality",
    "href": "slides/Chapter04.html#tests-for-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Tests for normality",
    "text": "Tests for normality\nTesting the hypothesis that sample data came from a normally distributed population. \\(H_0: X \\sim N(\\mu, \\sigma)\\).\n\nKolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)\nShapiro-Wilk test (based on variance of the difference)\n\nExample: a sample of \\(n\\) = 50 from N(100,1).\n\n\n\nset.seed(123)\nshapiro.test(rnorm(50, mean=100))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(50, mean = 100)\nW = 0.98928, p-value = 0.9279\n\n\n\n\nset.seed(123)\nks.test(rnorm(50), \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  rnorm(50)\nD = 0.073034, p-value = 0.9347\nalternative hypothesis: two-sided\n\n\n\n\nThe large p-values mean that there’s no evidence of non-normality.\nLike all tests, they can give the wrong answer. Try with set.seed(1234)."
  },
  {
    "objectID": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "href": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Can also use plots to assess normality",
    "text": "Can also use plots to assess normality\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\n\nggplot(tv, aes(sample = TELETIME)) + \n  stat_qq() + stat_qq_line() +\n  labs(title = \"Normal quantile plot for TV viewing times\")"
  },
  {
    "objectID": "slides/Chapter04.html#transformations",
    "href": "slides/Chapter04.html#transformations",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Transformations",
    "text": "Transformations\n\nIt can be useful to transform skewed data to make the distribution more like a ‘normal’ if that is required for a particular analysis.\nA linear transformation \\(Y^*= a+bY\\) only changes the scale or centre, not the shape of the distribution.\nTransformations can help to improve symmetry, normality, and stabilise the variance."
  },
  {
    "objectID": "slides/Chapter04.html#example",
    "href": "slides/Chapter04.html#example",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nRight skewed distribution is made roughly symmetric using a log transformation for no. of vehicles variable (rangitikei.* dataset)"
  },
  {
    "objectID": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "href": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "A Ladder of Powers for Transforming Data",
    "text": "A Ladder of Powers for Transforming Data\n\nRight skewed data needs a shrinking transformation\nLeft skewed data needs a stretching transformation\nThe strength or power of the transformation depends on the degree of skew.\n\n\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values"
  },
  {
    "objectID": "slides/Chapter04.html#box-cox-transformation",
    "href": "slides/Chapter04.html#box-cox-transformation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\n\n\nThis is a normalising transformation based on the Box-Cox power parameter, \\(\\lambda\\).\nR gives a point estimate of the Box-Cox power & a confidence interval.\nUsually, we are concerned about whether the residuals from a model are normally distributed – put the model in the lm statement.\nSometimes, no “good” value of \\(\\lambda\\) can be found.\n\n\nlibrary(lindia)\ngg_boxcox(lm(rangitikei$vehicle ~ 1))"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "href": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical inference based on transformed data",
    "text": "Statistical inference based on transformed data\n\nObtain the confidence interval using transformed data and then back transform the limits (to keep the original scale)\n\nThe confidence limits calculated on log-transformed data can be exponentiated back to the raw scale (i.e., \\(e^{\\text{confidence limit}}\\))\nThe confidence limits of square-root-transformed data can be squared back to the raw scale (ie. \\({\\text{confidence limit}^2}\\))\n\nFor hypothesis test, apply the same transformation on the value hypothesised under the null\nExplore https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/ app (not currently working)"
  },
  {
    "objectID": "slides/Chapter04.html#example-1",
    "href": "slides/Chapter04.html#example-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nBrain weights of Animals data are right-skewed.\n\n\ndata(Animals, package = \"MASS\")\n\nbind_cols( \n  # confidence intervals on raw data\n  `raw x` = Animals |&gt; \n    pull(brain) |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data\n  `log x` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data \n  # and then back-transformed\n  `log x, CIs back-transformed` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\") |&gt;\n    exp()\n  ) |&gt; \n  kable()\n\n\n\nLower and upper confidence limits \n\n\nraw x\nlog x\nlog x, CIs back-transformed\n\n\n\n\n56.88993\n3.495101\n32.95362\n\n\n1092.15293\n5.355790\n211.83131"
  },
  {
    "objectID": "slides/Chapter04.html#non-parametric-tests",
    "href": "slides/Chapter04.html#non-parametric-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations).\nMany non-parametric methods rely on replacing the observed data by their ranks."
  },
  {
    "objectID": "slides/Chapter04.html#spearmans-rank-correlation",
    "href": "slides/Chapter04.html#spearmans-rank-correlation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\n\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient.\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\nlibrary(GGally)\n\np &lt;- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n\n\n\n\n\n\n\nFigure 1: Comparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\nt.test(tv$TELETIME, mu=1680)\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter04.html#mann-whitney-test",
    "href": "slides/Chapter04.html#mann-whitney-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\nt.test(rangitikei$people~rangitikei$time)\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter04.html#another-form-of-test",
    "href": "slides/Chapter04.html#another-form-of-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Another form of test",
    "text": "Another form of test\n\n\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter04.html#permutation-tests",
    "href": "slides/Chapter04.html#permutation-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Permutation tests",
    "text": "Permutation tests\nA permutation (or randomisation) test has the following steps:\n\nRandomly permute the observed data many times, thereby destroying any real relationship,\nRecalculate the test statistic \\(T\\) for each random permutation\nCompare the observed value of \\(T\\) (from the actual data) with the values of \\(T\\) under permutation.\n\nOne sample hypothesis test example follows:\n\nlibrary(exactRankTests)\nperm.test(tv$TELETIME, null.value=1500)\n\n\n    1-sample Permutation Test\n\ndata:  tv$TELETIME\nT = 79547, p-value = 2.842e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nFor small samples, this approach is not powerful."
  },
  {
    "objectID": "slides/Chapter04.html#two-group-comparison",
    "href": "slides/Chapter04.html#two-group-comparison",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nperm.test(TELETIME~SEX, distribution ='exact', data=tv)\n\n\n    2-sample Permutation Test\n\ndata:  TELETIME by SEX\nT = 38370, p-value = 0.471\nalternative hypothesis: true mu is not equal to 0\n\n\nAlso using a linear model fit (cover later)\n\nlibrary(lmPerm)\nsummary(lmp(TELETIME~SEX, data=tv))\n\n[1] \"Settings:  unique SS : numeric variables centered\"\n\n\n\nCall:\nlmp(formula = TELETIME ~ SEX, data = tv)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1178.261  -510.793    -7.283   402.989  1130.739 \n\nCoefficients:\n    Estimate Iter Pr(Prob)\nSEX      122   51     0.98\n\nResidual standard error: 570.9 on 44 degrees of freedom\nMultiple R-Squared: 0.0118, Adjusted R-squared: -0.01066 \nF-statistic: 0.5255 on 1 and 44 DF,  p-value: 0.4723 \n\n\nRead the study guide example for bootstrap tests (not examined)"
  },
  {
    "objectID": "slides/Chapter04.html#summary",
    "href": "slides/Chapter04.html#summary",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Summary",
    "text": "Summary\nBasic methods of inference include:\n\nestimating a population parameter (e.g. mean) using a sample of values,\nestimating standard deviation of a sample estimate using the standard error,\nconstructing confidence intervals for an estimate, and\ntesting hypotheses about particular values of the population parameter.\n\nInference is relatively easy for normally distributed populations.\nStudent’s t-tests include:\n\none-sample t-test, including paired-sample t-test for a difference of zero\ntwo-sample t-test assuming equal variances (estimating pooled variance)\ntwo-sample t-test not assuming equal variances (Welch test)\n\nThe t-test is generally robust for non-normal populations (especially for large samples).\nPower transformations, such as square-root, log, or Box-Cox aim to reduce skewness.\nNon-parametric tests can be used for non-normal data, but they are usually less powerful than parametric tests.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter05.html#analysing-frequencies",
    "href": "slides/Chapter05.html#analysing-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Analysing frequencies",
    "text": "Analysing frequencies\n\nThis chapter focuses on data that consist of frequencies or counts of occurrences of some events\nWish to compare observed with what we would have expected\nWe will cover the \\(\\chi ^ 2\\) distribution, “Goodness-of-fit” tests, and test of independence"
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-distribution",
    "href": "slides/Chapter05.html#chi-squared-distribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi squared distribution",
    "text": "Chi squared distribution\n\n\nContinuous 0 to infinity; starts at 0 because it is a square (a square number can’t be negative)\nThe mean of this distribution is its degrees of freedom (k)\nIt is right skewed, mean greater than median and mode; variance is \\(2k\\)\nShape of the \\(\\chi ^ 2\\) distribution is determined by the degrees of freedom (k), at very high k (90 or greater) \\(\\chi ^ 2\\) distribution resembles the normal distribution\nMain purpose is hypothesis testing, not describing real-world distributions"
  },
  {
    "objectID": "slides/Chapter05.html#tables-and-frequencies",
    "href": "slides/Chapter05.html#tables-and-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tables and frequencies",
    "text": "Tables and frequencies\n\nWe often have hypotheses regarding the frequencies of levels of a factor or group of factors.\n\nFor a single two-level factor (e.g., male vs female, survived vs died), we might wish to know how likely the data are to have come from a population with equal proportions (or some other specified proportion).\nFor two factors, we might wish to know whether they are independent.\n\nIn either case, we can specify a null hypothesis and test it using data.\nTo do so, compare observed counts with expected counts, where the expected counts are derived from our null model about the population.\nWe are employing the Chi-squared statistic with data which consists of integers. That is, the data are discrete rather than continuous."
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 1\n\nA dataset contains 40 males and 50 females.\nHow plausible is the null model of these counts coming from a population with 50% males and 50% females?\nThe expected counts in this case would be 45 males and 45 females."
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 2\n\n\n\nDoes the distribution of rejects of metal castings by causes in a particular week vary from the long-term average counts?\nTreat the long-term average counts as the expected counts.\nCompare observed counts with expected counts.\n\n\n\n\n\nCauses of rejection\nRejects during the week\nLong-term average\n\n\n\n\nsand\n90\n82\n\n\nmisrun\n8\n4\n\n\nshift\n16\n10\n\n\ndrop-\n8\n6\n\n\ncorebreak\n23\n21\n\n\nbroken\n21\n20\n\n\nother\n5\n8"
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic",
    "href": "slides/Chapter05.html#chi-squared-test-statistic",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\n\\[\n\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\rm Observed-Expected}\\right)^{{\\rm 2}} }{{\\rm Expected}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E}\n\\]\nIf the number of categories is \\(c\\), then the degrees of freedom is \\(c-1\\)."
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "href": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\nIf the null hypothesis were true, then the value of \\(\\chi ^{2}\\) calculated from our data is a random value from a Chi-squared distribuion: \\[\n\\chi_{0} ^{2} = \\chi_{c-1} ^{2}\n\\] Once we calculate the test statistic, we can compare our observed value with its distribution under \\(H_{0}\\) to calculate a p-value."
  },
  {
    "objectID": "slides/Chapter05.html#assumptions",
    "href": "slides/Chapter05.html#assumptions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe classification of observations into groups must be independent\nNo more than 20% of categories should have expected counts less than 5"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-test",
    "href": "slides/Chapter05.html#goodness-of-fit-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit test",
    "text": "Goodness of fit test\n\nCompare observed frequencies with expected frequencies under some specified null hypothesis.\nMake a hypothesis about the population, what would we expect the frequency to be under that hypothesis?\nFor example:\n\nWish to compare the frequency of occurence of different phenotypes in an organism with the frequencies we would expect under Mendel’s laws of inheritance.\nWish to compare the distribution of a observation with expected count we would obtain from a Poisson distribution.\n\n\nHypotheses of this sort can be tested using the Chi-squared test statistic (\\(\\chi ^ 2\\) = Ki Sq.)"
  },
  {
    "objectID": "slides/Chapter05.html#example",
    "href": "slides/Chapter05.html#example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nA survey of voters included 550 males and 450 females. Will you call this survey as a biased one?\nHere the null hypothesis is that the ratio of males to females is 1:1.\nEquivalent: the proportion of males is equal to the proportion of female in the population\n\n\n\nGender\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\nmale\n550\n500\n\\((550-500)^2/ 500=5\\)\n\n\nfemale\n450\n500\n\\((450-500)^2/ 500=5\\)\n\n\nsum\n1000\n1000\n10\n\n\n\n\\[\n\\chi ^{2} = \\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E} =\n\\frac{(550-500)^2}{500} + \\frac{(450-500)^2}{500} = 10\n\\]"
  },
  {
    "objectID": "slides/Chapter05.html#example-3",
    "href": "slides/Chapter05.html#example-3",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\ndf= (2-1)=1\nAt 5% level (\\(\\alpha =0.05\\)), the critical value is only 3.84. So the sample is a biased one."
  },
  {
    "objectID": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "href": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Mendel’s experiment (see Study Guide)",
    "text": "Mendel’s experiment (see Study Guide)\n\n\nMendel discovered the principles of heredity by breeding garden peas. In one Mendel’s trials ratios of various types of peas (dihybrid-crosses) were 9:3:3:1\nThe observed results are very close to expected results. This results in a small chi squared value. Were experimental results fudged or was there a confirmation bias?"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions",
    "text": "Goodness of fit for distributions\n\nTreat class intervals as categories and obtain the actual counts (O)\nThe assumed distribution gives the expected counts (E)\nPerform a goodness of fit and validate the assumed theoretical distribution\nAdjust the degrees of freedom (df) for the number of estimated parameters of the theoretical distribution\n\nFor example, assume that you have 10 class intervals and test for normal distribution, which has 2 parameters.\n\nSo the df for this test will be 10-1-2=7."
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\nA safety inspector monitors car accidents at a bustling intersection. The inspector enters the counts of monthly accidents.\nNull: The sample data follow the Poisson distribution. Alternative: The sample data do not follow the Poisson distribution.\nNote The Poisson distribution is a discrete probability distribution (integers) that can model counts of events or attributes in a fixed observation space. Many but not all count processes follow this distribution."
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\n\n\n\nAccidents\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\n0\n7\n\n\n\n\n1\n8\n\n\n\n\n2\n13\n\n\n\n\n3\n10\n\n\n\n\n&gt;=4\n12\n\n\n\n\nSum\n40\n\n\n\n\n\nConclusion:"
  },
  {
    "objectID": "slides/Chapter05.html#tests-of-independence",
    "href": "slides/Chapter05.html#tests-of-independence",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tests of Independence",
    "text": "Tests of Independence\n\nIn some cases, we have counts of observations cross-classified in terms of two factors.\nWe are generally interested in determining whether or not the two factors are independent.\nIf we consider the observations falling into each category for factor 1, is this distribution consistent across all levels of factor 2? (or vice versa)"
  },
  {
    "objectID": "slides/Chapter05.html#contingency-table",
    "href": "slides/Chapter05.html#contingency-table",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Contingency table",
    "text": "Contingency table\n\nGiven a two way table of frequency counts, we test whether the row and column variables are independent\nHypotheses:\n\nNull: the two factors are independent\nwritten another way: the row (or column) distributions are the same\n\nThe expected count for cell \\((i,j)\\) is given by \\(E_{ij}\\) = \\((T_i \\times T_j)/n\\) where\n\\(~~~~~T_i\\), the total for row \\(i\\);\n\\(~~~~~T_j\\), the total for column \\(j\\)\n\\(~~~~~n\\), the overall total count\nTest statistic : \\(\\chi ^{2} =\\sum _{{i=1}}^{r}\\sum _{{j=1}}^{{\\rm c}}\\frac{\\left({ O}_{{ ij}} { -E}_{{ij}} \\right)^{{ 2}} }{{ E}_{{ij}} }.\\)\ndegrees of freedom: \\((r-1)(c-1)\\)"
  },
  {
    "objectID": "slides/Chapter05.html#example-4",
    "href": "slides/Chapter05.html#example-4",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nContext: Porcine Stress Syndrome (PSS) result in pale, soft meat in pigs and under conditions of stress- death.\n\nPresence of PPS is a positive reaction to breathing halothane.\n\nSelective breeding for reducing incidence of PSS\n\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#example-5",
    "href": "slides/Chapter05.html#example-5",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343\n\n\n\n\n\n\n\n\nchisq.test(dt)\n\n\n    Pearson's Chi-squared test\n\ndata:  dt\nX-squared = 16.433, df = 8, p-value = 0.03659"
  },
  {
    "objectID": "slides/Chapter05.html#computations",
    "href": "slides/Chapter05.html#computations",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Computations",
    "text": "Computations\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#inference",
    "href": "slides/Chapter05.html#inference",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Inference",
    "text": "Inference\n\nThe tabulated Chisq with (4-1)x(2-1) = 3 d.f are\n\n7.81 at 5% level; 16.27 at 1% level.\n\n\nConclusion: There is a statistical evidence that the breed is not independent of the result of the Halothane test.\n\n\nNote that large counts in the second column (Halothane negative) lead to large expected values but low contributions to chi-squared statistic.\nThis is because of the division by the appropriate expected value. On the other hand, the small observations of first column lead to small expected values but large contributions to the \\(\\chi ^{2}\\)."
  },
  {
    "objectID": "slides/Chapter05.html#significant-cell-contribution",
    "href": "slides/Chapter05.html#significant-cell-contribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Significant cell contribution",
    "text": "Significant cell contribution\n\nCounts follow Poisson distribution for which mean = variance\nHence \\(\\chi^{2}=\\sum \\frac{\\left({\\rm residual}\\right)^{{\\rm 2}} }{{\\rm variance}} =\\sum \\left(\\frac{{\\rm residual}}{{\\rm std\\; dev}} \\right)^{2}.\\)\nIndividual cell contribution is similar to standardized residual. Any standardized residual greater than 2 is regarded as significant.\nSo \\(2^2 = 4\\) is treated as a significant contribution to the \\(\\chi ^{2}\\) statistic."
  },
  {
    "objectID": "slides/Chapter05.html#warnings",
    "href": "slides/Chapter05.html#warnings",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Warnings",
    "text": "Warnings\n\nUse only frequency counts. Use of percentages in place of counts may lead to incorrect conclusions.\nCheck for small expected values. An expected value of less than 5 may lead to concern and a very small value of less than 1 is a warning. Sometimes, you can merge/combine categories in case of small expected counts.\nIf the chi-squared statistic is small enough to be not significant, there is no problem.\nIf chi-squared statistic is significant, check the contributions to each cell. If cells with large expected value (&gt;5) contribute a large amount to chi-squared statistic, again there is no problem.\nIf cells with expected values less than 5 lead large contributions to chi-squared statistic, the significance of the chi-squared statistic should be treated with caution."
  },
  {
    "objectID": "slides/Chapter05.html#simpsons-paradox",
    "href": "slides/Chapter05.html#simpsons-paradox",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nGroup 1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nGroup 2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nAfter amalgamation of both groups\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166"
  },
  {
    "objectID": "slides/Chapter05.html#permutation-test",
    "href": "slides/Chapter05.html#permutation-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Permutation test",
    "text": "Permutation test\nThis test is done maintaining the marginal totals.\nData: Smoking Status vs. Staff  Groupings\n\n\n\nSmoking Status vis-a-vis Staff Groupings\n\n\n\nNone\nModerate\nHeavy\nTotals\n\n\n\n\nJunior employees\n18\n57\n13\n88\n\n\nJunior managers\n4\n10\n4\n18\n\n\nSecretaries\n10\n13\n2\n25\n\n\nSenior employees\n25\n22\n4\n51\n\n\nSenior managers\n4\n5\n2\n11\n\n\nTotals\n61\n107\n25\n193\n\n\n\n\n\n\n\nPermutation test\n\nset.seed(12321)\nchisq.test(tabledata, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tabledata\nX-squared = 15.672, df = NA, p-value = 0.04948\n\n\nRegular Chi-square test\n\nchisq.test(tabledata)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#correspondence-analysis",
    "href": "slides/Chapter05.html#correspondence-analysis",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\nCorrespondence Analysis is an exploratory statistical technique for assessing the interdependence of categorical variables whose data are presented primarily in the form of a two-way table of frequencies\nData: Smoking Status vs. Staff Groupings\n\n\n                 None Moderate Heavy\nJunior employees   18       57    13\nJunior managers     4       10     4\nSecretaries        10       13     2\nSenior employees   25       22     4\nSenior managers     4        5     2\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#row-mass-row-profiles",
    "href": "slides/Chapter05.html#row-mass-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Row mass & row profiles",
    "text": "Row mass & row profiles\n\nrow profiles are found dividing the cell counts by the corresponding row total\nrow mass is found dividing the row totals by the grand total\n\n\n\n                  None Moderate  Heavy\nJunior employees 0.205    0.648 0.1477\nJunior managers  0.222    0.556 0.2222\nSecretaries      0.400    0.520 0.0800\nSenior employees 0.490    0.431 0.0784\nSenior managers  0.364    0.455 0.1818\nrowmass          0.316    0.554 0.1295\n\n\n\nSimilarly column profiles & column masses can be found"
  },
  {
    "objectID": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "href": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Graphical display of row profiles",
    "text": "Graphical display of row profiles\n\n\nNo clear patterns seen"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping\n\nlibrary(\"FactoMineR\")\nCA(tabledata) |&gt; summary()\n\n\n\nCall:\nCA(X = tabledata) \n\nThe chi square of independence between the two variables is equal to 15.7 (p-value =  0.0473 ).\n\nEigenvalues\n                       Dim.1   Dim.2\nVariance               0.074   0.008\n% of var.             90.570   9.430\nCumulative % of var.  90.570 100.000\n\nRows\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nJunior employees |    26.268 | -0.235 34.372  0.962 | -0.047 12.934  0.038 |\nJunior managers  |     8.784 | -0.236  7.042  0.590 |  0.197 47.082  0.410 |\nSecretaries      |     5.618 |  0.195  6.670  0.873 | -0.074  9.301  0.127 |\nSenior employees |    37.894 |  0.379 51.521  1.000 |  0.004  0.045  0.000 |\nSenior managers  |     2.636 |  0.071  0.394  0.110 |  0.203 30.638  0.890 |\n\nColumns\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nNone             |    49.186 |  0.394 66.705  0.997 |  0.020  1.688  0.003 |\nModerate         |    15.679 | -0.157 18.619  0.873 | -0.060 25.941  0.127 |\nHeavy            |    16.335 | -0.289 14.676  0.661 |  0.207 72.371  0.339 |"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping"
  },
  {
    "objectID": "slides/Chapter05.html#summary",
    "href": "slides/Chapter05.html#summary",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Summary",
    "text": "Summary\n\nGoodness of fit is for testing whether the observed counts are from the hypothesised population groups.\n\nFor \\(c\\) categories (groups), the test involves \\(c-1\\) df.\n\nContingency Table (\\(r\\) rows and \\(c\\) columns) data are tested for the independence.\n\nFor \\(r\\times c\\) cells the test involves \\((r-1)(c-1)\\) df\n\n\\(\\chi ^{2}\\) test works well if \\(E&gt; 5\\). Some could be as low as 1.\nDo correspondence analysis (symmetric plots) when independence is rejected.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model",
    "href": "slides/Chapter06.html#what-is-a-statistical-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\nFIT- to explain systematic (non-random) variation in the data\nRESIDUAL - to explain random variation in the data"
  },
  {
    "objectID": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "href": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Analysis of two quantitiative variables",
    "text": "Analysis of two quantitiative variables\n\nWith paired (related) data (X,Y)\nTwo variables: one (Y) is random (response) and the other (X) is considered fixed (predictor)\nInterested in the functional relationship between these two variables \\(Y=f(X)\\) to predict Y, given X\nInterested in estimates of coefficients"
  },
  {
    "objectID": "slides/Chapter06.html#regression",
    "href": "slides/Chapter06.html#regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\nStatistical Model\n\nFitted Model: \\(\\hat{Y}=a+bX\\) (True Model: \\(Y=\\alpha+\\beta X+\\epsilon\\))\nresidual error: \\(e= Y-\\hat{Y}\\) (\\(\\epsilon\\) is not the same as \\(e\\))"
  },
  {
    "objectID": "slides/Chapter06.html#regression-1",
    "href": "slides/Chapter06.html#regression-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\n\nA regression equation is a function that indicates how the average value of one response variable for given values of one or more predictor variables varies with these predictor variables; that is, \\(E(Y|X_1, X_2, ..., X_k)\\)"
  },
  {
    "objectID": "slides/Chapter06.html#simple-regression",
    "href": "slides/Chapter06.html#simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Simple regression",
    "text": "Simple regression\n\nThe term simple means there is a single predictor\nThe fitted model remains as: \\(\\hat{Y}=a+bX_{i}\\)\n\n\n\\(\\hat{Y}\\) is the predicted value of \\(y_{i}\\) given \\(x_{i}\\). Also called ‘fitted’ values, they are the linear funtion of X.\n\\(a\\) the intercept on the y-axis; value of \\(\\hat{Y}\\) when \\(x_{i}=0\\).\n\\(b\\) the slope of the line; the change in \\(\\hat{Y}\\) with a 1-unit increase in \\(x_{i}\\)."
  },
  {
    "objectID": "slides/Chapter06.html#fitting-a-regression",
    "href": "slides/Chapter06.html#fitting-a-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitting a regression",
    "text": "Fitting a regression\n\nWant a line that is as ‘close’ as possible to the existing data points we have\nThe method of least squares is employed to obtain the estimates \\(a\\) and \\(b\\)\n\nThe sum of squared residuals is minimized in the least squares method."
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n\n country Alcohol Death\n       1    22.9  30.0\n       2    15.2  23.6\n       3    12.3  18.9\n       4    11.9   5.0\n       5    10.8  12.3\n       6     9.9  14.2\n       7     8.3   7.4\n       8     7.2   3.0\n       9     6.6   7.2\n      10     5.8  10.6\n      11     5.7   3.7\n      12     5.6   3.4\n      13     4.2   4.3\n      14     3.9   3.6\n      15     3.1   5.4"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data\n\n\\(min \\left( \\sum_{i=1}(y_{i}-\\hat{y_{i}})^2 \\right)\\)\n\n\\(i\\) number of observations\n\\(y_{i}\\) observed value of y\n\\(\\hat{y_{i}}\\) predicted value of y"
  },
  {
    "objectID": "slides/Chapter06.html#r-base-output",
    "href": "slides/Chapter06.html#r-base-output",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "R Base Output",
    "text": "R Base Output\n\nmod1 &lt;- lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nLet us tidy it."
  },
  {
    "objectID": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "href": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitted model and testing its coefficients",
    "text": "Fitted model and testing its coefficients\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n\nFocus on the model, its coefficients\n\nWhat is the scale of these variables? Are these coefficient estimates meaningful in the context?\nWhat is the error around these estimates?"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions",
    "href": "slides/Chapter06.html#assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nModel forming assumptions\n\n\\(X\\) is known without error\n\\(Y\\) is linearly related to \\(X\\)\nThere is a random variability of \\(Y\\) about this line"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions-1",
    "href": "slides/Chapter06.html#assumptions-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nMore assumptions to form \\(t\\) and \\(F\\) statistics:\n\nVariability in \\(Y\\) about the line is constant and independent of \\(X\\) variable.\nThe variability of \\(Y\\) about the line follows normal distribution.\nThe distribution of \\(Y\\) given \\(X = X_i\\) is independent of \\(Y\\) given \\(X = X_j\\)."
  },
  {
    "objectID": "slides/Chapter06.html#residuals-and-assumptions",
    "href": "slides/Chapter06.html#residuals-and-assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals and assumptions",
    "text": "Residuals and assumptions\n\nMost assumptions are on the errors (residuals)\n\nindependent\nnormal\nrandom\n\nIs there a pattern in my residuals?\nDo they suggest what to do?"
  },
  {
    "objectID": "slides/Chapter06.html#residuals",
    "href": "slides/Chapter06.html#residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/Chapter06.html#types-of-residuals",
    "href": "slides/Chapter06.html#types-of-residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Types of residuals",
    "text": "Types of residuals\n\nraw or ordinary residual is just (observation-fit)\nStandardized Residual= Residual/Std.Dev of residual\nThe regression model is influenced by outliers or unusual points because the slope estimate of the regression line is sensitive to these outliers.\nSo we define Studentised or deleted t Residual\nSimilar to Standardized Residual without the observation under consideration. That is,\n\nStudentised residual = residual/std. dev of residual (after omitting the particular observation)."
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\nFor small sample sizes, residual diagnostics is difficult"
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\npar(mfrow=(c(2,2)))\nplot(mod1)"
  },
  {
    "objectID": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "href": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals showing need for transformation",
    "text": "Residuals showing need for transformation\n\nNon-constant Residual Variation"
  },
  {
    "objectID": "slides/Chapter06.html#adding-more-predictors",
    "href": "slides/Chapter06.html#adding-more-predictors",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Adding more predictors",
    "text": "Adding more predictors"
  },
  {
    "objectID": "slides/Chapter06.html#subgrouping-patterns",
    "href": "slides/Chapter06.html#subgrouping-patterns",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Subgrouping patterns",
    "text": "Subgrouping patterns"
  },
  {
    "objectID": "slides/Chapter06.html#outliers",
    "href": "slides/Chapter06.html#outliers",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/Chapter06.html#autocorrelation",
    "href": "slides/Chapter06.html#autocorrelation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nNeighbouring residuals depend on each other"
  },
  {
    "objectID": "slides/Chapter06.html#improving-simple-regression",
    "href": "slides/Chapter06.html#improving-simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Improving simple regression",
    "text": "Improving simple regression\n\nUse a different predictor or explanatory variable\nTransform the \\(Y\\) variable\nAdd other explanatory variables to the model (next week)\nDeletion of invalid (as opposed to outlier) observations\nReconsider the linear relationship"
  },
  {
    "objectID": "slides/Chapter06.html#tests-of-significance",
    "href": "slides/Chapter06.html#tests-of-significance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Tests of significance",
    "text": "Tests of significance\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n-   $t$ tests of intercept and slope\n\n-   $H_0=true~slope=0$; $H_0=true~intercept=0$\n\n    -   For `Alcohol~deaths` model, the slope is significant\n        but not the intercept"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures",
    "href": "slides/Chapter06.html#model-quality-measures",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nModel summary (or Quality) measures\n\n\\(R^2\\) is the proportion of variation explained by the fitted model\n\nA meaningful model must have at least 50% \\(R^2\\)\nA large \\(R^2\\) is important to explain the relationship(s)\n\nResidual standard deviation (error) \\(S\\) has to be small\n\nHow small? Difficult to say. Compare \\(S\\) with the overall spread in \\(Y\\) or with the mean of \\(Y\\)\nA small \\(S\\) is important for prediction"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures-1",
    "href": "slides/Chapter06.html#model-quality-measures-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\n\nout1 &lt;- glance(mod1) |&gt; \n  select(r.squared, sigma, statistic, p.value) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nout1 |&gt; t() |&gt; \n  kable(caption = \"Model summary measures\") |&gt; \n  kable_classic(full_width = T) \n\n\n\nModel summary measures\n\n\n\n\n\n\n\n\nr.squared\n0.79\n\n\nsigma\n3.94\n\n\nstatistic\n48.35\n\n\np.value\n0.00"
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained",
    "href": "slides/Chapter06.html#variance-explained",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\nsummary(ols)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05"
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained-1",
    "href": "slides/Chapter06.html#variance-explained-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\nSS &lt;- anova(ols) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term df    sumsq\n   Alcohol  1 751.4408\n Residuals 13 202.0286\n     Total 14 953.4693\n\n\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\nwhere:\n\\(MS~Error\\) = \\(frac{SSE}{n-k-1}\\)\n\\(MS~Total\\) = \\(frac{SST}{n-1}\\)\ntherefore \\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)"
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test",
    "href": "slides/Chapter06.html#anova-f-test",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n\n\nmod1  &lt;-  lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1) \n\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\n\n\nanova(mod1)\n\n\nAnalysis of Variance Table\n\nResponse: Death\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAlcohol    1 751.44  751.44  48.353 1.001e-05 ***\nResiduals 13 202.03   15.54                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmod1.aov  &lt;-  aov(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1.aov) \n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nAlcohol      1  751.4   751.4   48.35  1e-05 ***\nResiduals   13  202.0    15.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFor the straight line model, the F-test is equivalent to testing the hypothesis that the true slope is zero."
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test-1",
    "href": "slides/Chapter06.html#anova-f-test-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n-   Significant F ratio need not always imply that the\n    straight line is the best fit to the data.\n-   For `Alcohol~deaths` model, the $F$ statistic is \n    significant which means that the fitted model explains significant variation\n\n\nout1 &lt;- anova(mod1) |&gt; tidy() |&gt; mutate_if(is.numeric, round, 2)\n\noptions(knitr.kable.NA = \" \")\n\nkable(out1, caption = \"ANOVA table\") |&gt; \n  kable_classic(full_width = F) \n\n\n\nANOVA table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAlcohol\n1\n751.44\n751.44\n48.35\n0\n\n\nResiduals\n13\n202.03\n15.54"
  },
  {
    "objectID": "slides/Chapter06.html#anova-table-construction",
    "href": "slides/Chapter06.html#anova-table-construction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA table construction",
    "text": "ANOVA table construction\n\nEach source has an associated degrees of freedom.\nFor regression, DF = \\(1\\) as there are two parameters \\(a\\) and \\(b\\) fitted in the model\nFor total, DF = \\(n-1\\) since there are n observations\nFor error, DF = by subtraction = \\((n-1)-1 = n-2\\)\nMean Square (MS) values are obtained as MS = SS/DF\nF ratio for regression =MS(Regression)/MS(Error)\nF ratio follows the \\(F\\) distribution with \\((1, n-2)\\) d.f and provides the significance of the model fitted.\n\nthe ratio of two sample variances (or MS) follows the \\(F\\) distribution (normal case)"
  },
  {
    "objectID": "slides/Chapter06.html#prediction",
    "href": "slides/Chapter06.html#prediction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Prediction",
    "text": "Prediction\n\nThe predicted response at value \\(x=x_0\\) is obtained using the fitted regression equation.\nConfidence & prediction intervals can also be constructed.\nNote that prediction intervals are for individual observations whereas the confidence intervals are for the expected (mean) response for a given \\(x_0\\)\n\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"confidence\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 9.476416 13.97915\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"prediction\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 2.918701 20.53686"
  },
  {
    "objectID": "slides/Chapter06.html#outlier-effect-on-regression",
    "href": "slides/Chapter06.html#outlier-effect-on-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outlier effect on regression",
    "text": "Outlier effect on regression\n\nScatter plot of People vs Vehicle"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance\n\n\nA distant \\(x\\) value has a higher leverage.\n\nThis leverage is often measured by the \\(h_{ii}\\) or hi value\nCheck \\(h_{ii}\\) &gt; \\(\\frac{3p}{n}\\) or not\n\nInfluence of a point on the regression is measured using the Cook’s distance \\(D_i\\)\n\nrelated to difference between the regression coefficients with and without the \\(i^{th}\\) data point.\n\\(D_{i} &gt;0.7\\) can be deemed as being influential (for \\(n&gt;15\\))"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance"
  },
  {
    "objectID": "slides/Chapter06.html#robust-regression-models",
    "href": "slides/Chapter06.html#robust-regression-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust regression models",
    "text": "Robust regression models\nRobust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates.\nFor example, least squares estimates for regression models are highly sensitive to outliers: an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss, and therefore has more leverage over the regression estimates."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models",
    "href": "slides/Chapter06.html#robust-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust models",
    "text": "Robust models\nWe can fit a robust linear model using the functions MASS::rlm() robustbase::lmrob()."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-1",
    "href": "slides/Chapter06.html#robust-models-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\nTo determine if a robust regression model offers a better fit to the data compared to the OLS model, we can calculate the residual standard error of each model.\nThe residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model. The lower the value for RSE, the more closely a model is able to fit the data."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-2",
    "href": "slides/Chapter06.html#robust-models-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\n\n#fit least squares regression model\nols &lt;- lm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of ols model\nsummary(ols)$sigma\n\n[1] 3.942164\n\n#fit robust regression model\nrobust &lt;- rlm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of robust model\nsummary(robust)$sigma\n\n[1] 3.417522"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv",
    "href": "slides/Chapter06.html#cross-validation-cv",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nA tool to avoid overfitting (more important with multiple predictors)\nEvaluate performance of model"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv-1",
    "href": "slides/Chapter06.html#cross-validation-cv-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nSplit the sample data randomly into (equal) k subsets (parts) by resampling.\nFit the model for \\((k − 1\\)) subsets of the data\nPredict for the omitted subsets\nCompare prediction errors\nRepeat for each subsets of the data"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-types",
    "href": "slides/Chapter06.html#cross-validation-types",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation Types",
    "text": "Cross Validation Types\n\nK-fold\nStratified K-fold\n\nSame as K-fold but splitting data governed by criteria so that each subset has the same proportion of obervations of a given categorical value\n\nLeave One Out (LOO)\n\nLeaves out a single data point, then uses the same process as K-fold CV\n\nR has many packages for cross validation"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n# CV using Root Mean Sq Error as measure of prediction error\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit &lt;- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses &lt;- lmfit$resample[,1]\nrlmfit &lt;- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses &lt;- rlmfit$resample[,1]\ndfm  &lt;-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter06.html#choosing-the-best-model",
    "href": "slides/Chapter06.html#choosing-the-best-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Choosing the best model",
    "text": "Choosing the best model\n\nThe best model is not decided purely on statistical grounds.\nIf the main aim is to describe relationships, include all the relevant variables.\nIf the main aim is to predict, prefer the simplest feasible (parsimonious) model with smaller number of predictors.\n\nExamine the literature to discover similar examples, see how they are tackled, discuss the matter with the researcher etc."
  },
  {
    "objectID": "slides/Chapter06.html#main-points",
    "href": "slides/Chapter06.html#main-points",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Main points",
    "text": "Main points\nConcepts and practical skills you should have at the end of this chapter:\n\nUnderstand and be able to perform a simple linear regression on bivariate related data sets\nUse scatter plots or other appropriate plots to visualize the data and regression line\nExamine residual diagnostic plots and test assumptions, then perform appropriate transformations as necessary\nSummarize regression results and appropriate tests of significance. Interpret these results in context of your data\nUse a regression line to predict new data and explain confidence and prediction intervals\nUnderstand and explain the concepts of robust regression modeling, and cross-validation.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives",
    "href": "slides/Chapter07.html#learning-objectives",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nUnderstand and describe a multiple linear regression, the difference from a simple linear regression, and when each are appropriate\nFit and display a multiple regression\nUnderstand and interpret different types of Sums of Squares\nUnderstand and test for multicollinearity and other assumptions"
  },
  {
    "objectID": "slides/Chapter07.html#what-is-a-multiple-regression",
    "href": "slides/Chapter07.html#what-is-a-multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What is a multiple Regression?",
    "text": "What is a multiple Regression?\n\nIn a simple regression, there is only one predictor.\nMultiple regression modelling involves many predictors."
  },
  {
    "objectID": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "href": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When to use multiple predictors",
    "text": "When to use multiple predictors\n\nStatistical control of a confound: controlling treatment for some unwanted variability\nMultiple causation: multiple things are thought to cause changes in the outcome variable\nInteractions: we are interested in how two variables may combine to change our outcome (will cover this in the next chapter)"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression",
    "href": "slides/Chapter07.html#multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWhere to start:\n\nQuestion and hypothesis + domain knowledge\n\nWhat are we interested in testing?\nWhat data is collected? do those variables make sense?\nWhat kind of data are my response and predictor variables?\n\nPerform EDA first\n\nA scatter plot matrix can show nonlinear relationships\nA correlation matrix will only show the strength of pairwise linear relationships\n\nLook for the predictors having the largest correlation with response\n\nLook for inter-correlations between the predictors and choose the one with high correlation with response variable but uncorrelated with the rest."
  },
  {
    "objectID": "slides/Chapter07.html#data-example",
    "href": "slides/Chapter07.html#data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\nBasketball team summary for 2020 regular season\n\nWe have 2118 data points which equates to 1059 games (2 teams per game), 2020 was a short season\nEach team in a standard season plays 82 games. That means in total, an NBA season is comprised of 1,230 games.\n\n\n  Team     Game Spread Outcome MIN PTS       P2p       P3p       FTp OREB DREB\n1  ATL 21900014     17       W  48 117 0.6000000 0.3548387 0.8571429    8   34\n2  ATL 21900028      4       W  48 103 0.6296296 0.3000000 0.5333333    9   43\n3  ATL 21900043     -2       L  48 103 0.4736842 0.3333333 0.6875000    8   37\n4  ATL 21900052    -15       L  48  97 0.5454545 0.2820513 0.6666667    9   24\n5  ATL 21900066     -9       L  48  97 0.5370370 0.2058824 0.6923077   16   34\n6  ATL 21900099      8       W  48 108 0.5333333 0.3666667 0.6875000    9   39\n  AST TOV STL BLK PF\n1  27  13   9   2 15\n2  22  18   5   9 26\n3  23  21  12   3 25\n4  28  20  14   7 29\n5  20  16   5   5 15\n6  22  18   9   5 20\n\n\nSpread: Point difference between winning and losing team\nPTS: Total points scored by a team in a game\nP2p: Percent of 2-pointers made\nP3p: Percent of 3-pointers made\nFTp: Percent of free-throws made\nOREB: Offensive rebounds\nDREB: Defensive rebounds\nAST: Assists\nSTL: Steals\nBLK: Blocks"
  },
  {
    "objectID": "slides/Chapter07.html#data-example-1",
    "href": "slides/Chapter07.html#data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\n\noptions(digits=3)\ncor(teams[,c(3, 6:12, 14:15)])\n\n        Spread     PTS      P2p      P3p     FTp     OREB     DREB     AST\nSpread  1.0000  0.5730  0.36338  0.42719  0.1063 -0.00240  0.46470  0.3477\nPTS     0.5730  1.0000  0.48713  0.55094  0.1897 -0.01487  0.14131  0.5103\nP2p     0.3634  0.4871  1.00000 -0.00601 -0.0105 -0.28585  0.02484  0.3633\nP3p     0.4272  0.5509 -0.00601  1.00000  0.0297 -0.18736  0.00164  0.4054\nFTp     0.1063  0.1897 -0.01053  0.02967  1.0000 -0.08960 -0.01724 -0.0297\nOREB   -0.0024 -0.0149 -0.28585 -0.18736 -0.0896  1.00000  0.03882 -0.0996\nDREB    0.4647  0.1413  0.02484  0.00164 -0.0172  0.03882  1.00000  0.0468\nAST     0.3477  0.5103  0.36333  0.40540 -0.0297 -0.09960  0.04681  1.0000\nSTL     0.1246  0.0352  0.01897 -0.04479  0.0115 -0.00389 -0.18027  0.0518\nBLK     0.1837  0.0623  0.03757  0.00540 -0.0185  0.00311  0.20700  0.0411\n            STL      BLK\nSpread  0.12460  0.18367\nPTS     0.03525  0.06234\nP2p     0.01897  0.03757\nP3p    -0.04479  0.00540\nFTp     0.01153 -0.01849\nOREB   -0.00389  0.00311\nDREB   -0.18027  0.20700\nAST     0.05178  0.04112\nSTL     1.00000  0.03267\nBLK     0.03267  1.00000"
  },
  {
    "objectID": "slides/Chapter07.html#inter-relationships",
    "href": "slides/Chapter07.html#inter-relationships",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Inter-relationships",
    "text": "Inter-relationships"
  },
  {
    "objectID": "slides/Chapter07.html#full-regression-in-r",
    "href": "slides/Chapter07.html#full-regression-in-r",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Full regression in R",
    "text": "Full regression in R\nPlaces all of the predictors in the model\nEquivalent of throwing everything in and hoping something sticks\n\n\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -142.4223     2.8728  -49.58  &lt; 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  &lt; 2e-16 ***\nP3p           74.0109     3.3449   22.13  &lt; 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  &lt; 2e-16 ***\nDREB           1.2033     0.0367   32.81  &lt; 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  &lt; 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#residuals",
    "href": "slides/Chapter07.html#residuals",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residuals",
    "text": "Residuals\nJust like with a simple regression we examine residuals to look for patterns\n\nThese look great, probably because we have a ton of data in this example."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity",
    "href": "slides/Chapter07.html#multicollinearity",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity is where at least two predictor variables are highly correlated.\nMulticollinearity does not affect the residual SD very much, and doesn’t pose a major problem for prediction.\nThe major effects of multicollinearity are:\n\nIt changes the estimates of the coefficients.\nIt inflates the variance of the estimates of the coefficients. That is, it increases the uncertainty about what the slope parameters are.\nTherefore, it matters when testing hypotheses about the effects of specific predictors."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-1",
    "href": "slides/Chapter07.html#multicollinearity-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nThe impact of multicollinearity on the variance of the estimates can be quantified using the Variance Inflation Factor (VIF &lt; 5 is considered ok).\nThere are several ways to deal with multicollinarity, depending on context. We can discard one of highly correlated variable, perform ridge regression, or think more carefully about how the variables relate to each other."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-2",
    "href": "slides/Chapter07.html#multicollinearity-2",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIn R we can examine Multicollinearity using the function vif()\nRemember our basketball regression:\n\nlibrary(car)\nvif(full_reg)\n\n PTS  P2p  P3p  FTp OREB DREB  AST  STL  BLK \n3.07 2.14 2.24 1.14 1.37 1.14 1.50 1.06 1.05 \n\n\nThese values are all small (VIF &lt; 5) so we can continue on in interpreting the regression."
  },
  {
    "objectID": "slides/Chapter07.html#tidy-summary",
    "href": "slides/Chapter07.html#tidy-summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Tidy summary",
    "text": "Tidy summary\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-142.422\n2.873\n-49.575\n0.000\n\n\nPTS\n0.062\n0.027\n2.320\n0.020\n\n\nP2p\n74.117\n3.839\n19.306\n0.000\n\n\nP3p\n74.011\n3.345\n22.126\n0.000\n\n\nFTp\n15.469\n2.027\n7.633\n0.000\n\n\nOREB\n0.719\n0.062\n11.538\n0.000\n\n\nDREB\n1.203\n0.037\n32.809\n0.000\n\n\nAST\n-0.035\n0.048\n-0.729\n0.466\n\n\nSTL\n1.069\n0.068\n15.785\n0.000\n\n\nBLK\n0.350\n0.078\n4.470\n0.000"
  },
  {
    "objectID": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "href": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Variance Explained (Review from Chapter 6)",
    "text": "Variance Explained (Review from Chapter 6)\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\n\\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)\nNow we have more sums of square to add in to our regression\n\n\nSS &lt;- anova(full_reg) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term   df    sumsq\n       PTS    1 141194.6\n       P2p    1   4001.7\n       P3p    1  14465.4\n       FTp    1    583.8\n      OREB    1   7082.2\n      DREB    1  78359.3\n       AST    1     10.1\n       STL    1  20082.0\n       BLK    1   1542.2\n Residuals 2108 162702.8\n     Total 2117 430024.0"
  },
  {
    "objectID": "slides/Chapter07.html#additional-variation-explained",
    "href": "slides/Chapter07.html#additional-variation-explained",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Additional variation explained",
    "text": "Additional variation explained\n\nVariation in \\(Y\\) is separated into two parts SSR and SSE.\n\nThe shaded overlap of two circles represent the variation in \\(Y\\) explained by the \\(X\\) variables.\n\nThe total overlap of \\(X_1\\) and \\(X_2\\), and \\(Y\\) depends on\n\nrelationship of \\(Y\\) with \\(X_1\\) and \\(X_2\\)\ncorrelation between \\(X_1\\) and \\(X_2\\)"
  },
  {
    "objectID": "slides/Chapter07.html#sequential-addition-of-predictors",
    "href": "slides/Chapter07.html#sequential-addition-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Sequential addition of predictors",
    "text": "Sequential addition of predictors\n\nAddition of variables decreases SSE and increases SSR and \\(R^2\\).\n\\(s^2\\) = MSE = SSE/df decreases to a minimum and then increases since addition of variable decreases SSE but adds to df."
  },
  {
    "objectID": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "href": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Significance of Type I or Seq.SS",
    "text": "Significance of Type I or Seq.SS\n\nThe Type I SS is the SS of a predictor after adjusting for the effects of the preceding predictors in the model.\n\nSometimes order matters, particularly with unequal sample sizes\n\nFor unbalanced data, this approach tests for a difference in the weighted marginal means. In practical terms, this means that the results are dependent on the realized sample sizes. In other words, it is testing the first factor without controlling for the other factor, which may not be the hypothesis of interest.\nF test for the significance of the additional variation explained\nR function anova() calculates sequential or Type-I SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-ii",
    "href": "slides/Chapter07.html#type-ii",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type II",
    "text": "Type II\n\nType II SS is based on the principle of marginality.\n\nEach variable effect is adjusted for all other appropriate effects.\n\nequivalent to the Type I SS when the variable is the last predictor entered the model.\n\nOrder matters for Type I SS but not for Type II SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-iii-ss",
    "href": "slides/Chapter07.html#type-iii-ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type III SS",
    "text": "Type III SS\n\nType III SS is the SS added to the regression SS after ALL other predictors including an interaction term.\nThis type tests for the presence of a main effect after other main effects and interaction. This approach is therefore valid in the presence of significant interactions.\nIf the interaction is significant SS for main effects should not be interpreted"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action",
    "href": "slides/Chapter07.html#ss-types-in-action",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\nConsider a model with terms A and B:\nType 1 SS:\nSS(A) for factor A.\nSS(B | A) for factor B.\nType 2 SS:\nSS(A | B) for factor A.\nSS(B | A) for factor B.\nType 3 SS:\nSS(A | B, AB) for factor A.\nSS(B | A, AB) for factor B.\n\nWhen data are balanced and the design is simple, types I, II, and III will give the same results.\nSS explained is not always a good criterion for selection of variables"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action-1",
    "href": "slides/Chapter07.html#ss-types-in-action-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\n\nanova(full_reg)\n\nAnalysis of Variance Table\n\nResponse: Spread\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nPTS          1 141195  141195 1829.34 &lt; 2e-16 ***\nP2p          1   4002    4002   51.85 8.3e-13 ***\nP3p          1  14465   14465  187.42 &lt; 2e-16 ***\nFTp          1    584     584    7.56   0.006 ** \nOREB         1   7082    7082   91.76 &lt; 2e-16 ***\nDREB         1  78359   78359 1015.23 &lt; 2e-16 ***\nAST          1     10      10    0.13   0.718    \nSTL          1  20082   20082  260.18 &lt; 2e-16 ***\nBLK          1   1542    1542   19.98 8.2e-06 ***\nResiduals 2108 162703      77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlibrary(car)\nAnova(full_reg,  type=2)\n\nAnova Table (Type II tests)\n\nResponse: Spread\n          Sum Sq   Df F value  Pr(&gt;F)    \nPTS          416    1    5.38    0.02 *  \nP2p        28767    1  372.71 &lt; 2e-16 ***\nP3p        37787    1  489.57 &lt; 2e-16 ***\nFTp         4497    1   58.26 3.4e-14 ***\nOREB       10276    1  133.13 &lt; 2e-16 ***\nDREB       83083    1 1076.43 &lt; 2e-16 ***\nAST           41    1    0.53    0.47    \nSTL        19232    1  249.17 &lt; 2e-16 ***\nBLK         1542    1   19.98 8.2e-06 ***\nResiduals 162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAnova(full_reg,  type=3)\n\nAnova Table (Type III tests)\n\nResponse: Spread\n            Sum Sq   Df F value  Pr(&gt;F)    \n(Intercept) 189696    1 2457.72 &lt; 2e-16 ***\nPTS            416    1    5.38    0.02 *  \nP2p          28767    1  372.71 &lt; 2e-16 ***\nP3p          37787    1  489.57 &lt; 2e-16 ***\nFTp           4497    1   58.26 3.4e-14 ***\nOREB         10276    1  133.13 &lt; 2e-16 ***\nDREB         83083    1 1076.43 &lt; 2e-16 ***\nAST             41    1    0.53    0.47    \nSTL          19232    1  249.17 &lt; 2e-16 ***\nBLK           1542    1   19.98 8.2e-06 ***\nResiduals   162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter07.html#summary-so-far",
    "href": "slides/Chapter07.html#summary-so-far",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary so far",
    "text": "Summary so far\n\nDefined multiple regression\nWhy we might run a multiple regression\nHow to perform a multiple regression, examine residuals, multicollinearity\nVariation and R squared\nSums of Squares types"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives-1",
    "href": "slides/Chapter07.html#learning-objectives-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nCautionary tales: when correlation misleads us\nHow to make models\nComparing models\nPrinciples of model selection"
  },
  {
    "objectID": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "href": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Does Waffle Houses cause divorce??",
    "text": "Does Waffle Houses cause divorce??"
  },
  {
    "objectID": "slides/Chapter07.html#or-is-it-butter",
    "href": "slides/Chapter07.html#or-is-it-butter",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Or is it butter?",
    "text": "Or is it butter?\n\nAnd if you want more to impress your friends at a BBQ, the source is: http://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/Chapter07.html#spurious-association",
    "href": "slides/Chapter07.html#spurious-association",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Spurious association",
    "text": "Spurious association\n\nYou’ve heard that before: correlation does not imply causation. BUT it doesn’t discard it either\nHope you are seated: causation does not imply correlation.\nCausation implies conditional correlation (up to linearity issues).\nWe need more than just statistical models to answer causal questions."
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "href": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with spurious associations?",
    "text": "How do we deal with spurious associations?\n\nDomain knowledge: you know that waffles and butter don’t cause divorce so why might they be correlated?\n\nIs there another predictor that would be better?\n\nMultiple regressions can disentangle the association between two predictors and an outcome\n\nStatistical control of a confound"
  },
  {
    "objectID": "slides/Chapter07.html#masked-associations",
    "href": "slides/Chapter07.html#masked-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masked associations",
    "text": "Masked associations\n\nAssociation between a predictor variable and an outcome can be masked by another variable.\nYou need to observe both variables to see the “true” influence of either on the outcome.\nHow do we account for the masking variable (seen as a nuisance)?"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "href": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situations tend to arise when:",
    "text": "Masking situations tend to arise when:\n\nBoth predictors are associated with one another.\nHave opposite relationships with the outcome."
  },
  {
    "objectID": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "href": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Headline: Higher ice cream sales increase shark attacks!",
    "text": "Headline: Higher ice cream sales increase shark attacks!\nWe’ll predict ice cream sales from the temperature and the number of shark attacks"
  },
  {
    "objectID": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "href": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "First we will consider simple regressions",
    "text": "First we will consider simple regressions"
  },
  {
    "objectID": "slides/Chapter07.html#single-regression",
    "href": "slides/Chapter07.html#single-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Single regression",
    "text": "Single regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression-1",
    "href": "slides/Chapter07.html#multiple-regression-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple regression",
    "text": "Multiple regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0\n\n\n\n\n\n\n\n\nTemperature and Sharks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.064\n0.000\n1.000\n\n\ntemp\n0.776\n0.094\n8.217\n0.000\n\n\nshark\n-0.008\n0.094\n-0.084\n0.933"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situation",
    "href": "slides/Chapter07.html#masking-situation",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situation",
    "text": "Masking situation\nTend to arise when\n\nBoth predictors are associated with one another.\nHave different relationships with the outcome"
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-this",
    "href": "slides/Chapter07.html#how-do-we-deal-with-this",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with this?",
    "text": "How do we deal with this?\n\nStatistically there is not really an answer\nThe answer lies in the causes and the causes are not in the data; Shark attacks dont cause ice cream sales or vice versa\nRemember that interpreting the (regression) parameter estimates always depends upon what you believe the causal model"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection",
    "href": "slides/Chapter07.html#model-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model Selection",
    "text": "Model Selection\n\nThe first step before selection of the best subset of predictors is to study the correlation matrix\nWe then perform stepwise additions (forward) or subtractions (backward) from the model and compare them\n\nBUT…\n\nWe saw with the illustration of SS how the significance or otherwise of a variable in a multiple regression model depends on the other variables in the model\nTherefore, we cannot fully rely on the t-test and discard a variable because its coefficient is insignificant"
  },
  {
    "objectID": "slides/Chapter07.html#selection-of-predictors",
    "href": "slides/Chapter07.html#selection-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Selection of predictors",
    "text": "Selection of predictors\n\nHeuristic (short-cut) procedures based on criteria such as \\(F\\), \\(R^2_{adj}\\), \\(AIC\\), \\(C_p\\) etc\n\nForward Selection: Add variables sequentially\n\nconvenient to obtain the simplest feasible model\n\nBackward Elimination: Drop variables sequentially\n\nIf difference between two variables is significant but not the variables themselves, forward regression would obtain the wrong model since both may not enter the model.\n\nKnown as suppressor variables case (like masking variables discussed earlier)\n\n\n\n\nExample: (try)\n\n\n\nCall:\nlm(formula = y ~ x1, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1691 -0.6791 -0.0033  0.6441  1.1299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.98876    1.26689    9.46  3.4e-07 ***\nx1           0.00375    0.41608    0.01     0.99    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.832 on 13 degrees of freedom\nMultiple R-squared:  6.24e-06,  Adjusted R-squared:  -0.0769 \nF-statistic: 8.11e-05 on 1 and 13 DF,  p-value: 0.993\n\n\n\nCall:\nlm(formula = y ~ x2, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0900 -0.6334  0.0002  0.6146  1.0403 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.632      0.811   13.11  7.2e-09 ***\nx2             0.195      0.113    1.74     0.11    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.75 on 13 degrees of freedom\nMultiple R-squared:  0.188, Adjusted R-squared:  0.126 \nF-statistic: 3.02 on 1 and 13 DF,  p-value: 0.106\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = suppressor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.01363 -0.00945 -0.00228  0.00863  0.01632 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.51541    0.06114   -73.8   &lt;2e-16 ***\nx1           3.09701    0.01227   252.3   &lt;2e-16 ***\nx2           1.03186    0.00368   280.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0107 on 12 degrees of freedom\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 3.92e+04 on 2 and 12 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#ockhams-razor",
    "href": "slides/Chapter07.html#ockhams-razor",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Ockham’s razor",
    "text": "Ockham’s razor\nNunca ponenda est pluralitas sine necesitate\n(Plurality should never be posited without necessity)\n\nProblem: fit to sample always (*multi-level models can be counter-examples) improves as we add parameters.\nDangers of “stargazing”: selecting variables with low p-values (aka ‘lots of stars’). P-values are not designed to cope with over-/under-fitting."
  },
  {
    "objectID": "slides/Chapter07.html#overfitting",
    "href": "slides/Chapter07.html#overfitting",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Overfitting",
    "text": "Overfitting\n\nOverfitting learning too much from the data, where you are almost just connecting the points rather than estimating.\nUnderfitting is the opposite, i.e. being insensitive to the data.\naka Models with fewer assumptions are to be preferred.\nIn practice, we have to choose between models that differ both in accuracy and simplicity. The razor is not really a useful guidance for this trade off.\n\nWe need tools"
  },
  {
    "objectID": "slides/Chapter07.html#all-possible-models",
    "href": "slides/Chapter07.html#all-possible-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "All possible models:",
    "text": "All possible models:\nAn exhaustive screening of all possible regression models can also be done using software\n\nBest Subsets: Stop at each step and check whether predictors, in the model or outside, are the best combination for that step.\n\n\ntime consuming to perform when the predictor set is large\n\n\nRemember permutations\nFor example:\nIf we fix the number of predictors as 3, then 20 regression models are possible"
  },
  {
    "objectID": "slides/Chapter07.html#what-criteria-do-we-use",
    "href": "slides/Chapter07.html#what-criteria-do-we-use",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What criteria do we use?",
    "text": "What criteria do we use?\nMultiple options\n- R squared\n- Sums of squared (different types, for testing predictor significance)\n- Information criteria (AIC, BIC etc)\n- For prediction: MSD/MSE, MAD, MAPE\n- \\(C_p\\)\n\nRemember its about balance and what you are looking for (fit vs prediction, complexity vs generality)\n\n\nNote\n\nIf a model stands out, it will perform well in terms of all summary measures.\nIf a model does not stand out, summary measures will contradict."
  },
  {
    "objectID": "slides/Chapter07.html#when-r2-becomes-absurd",
    "href": "slides/Chapter07.html#when-r2-becomes-absurd",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When \\(R^2\\) becomes absurd",
    "text": "When \\(R^2\\) becomes absurd"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-1",
    "href": "slides/Chapter07.html#model-selection-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection",
    "text": "Model selection\n\nResidual SD depends on its degrees of freedom\n\nSo comparison of models based on Residual SD is not fully fair\n\nThe following three measures are popular prediction modelling and similar to residual SD\nMean Squared Deviation (MSD): mean of the squared errors (i.e., deviations) (also called MSE)\n\n\\[\\frac{\\sum \\left({\\rm observation-fit}\\right)^{{\\rm 2}} }{{\\rm number~of~ observations}}\\]\n\nMean Absolute Deviation (MAD)\n\n\\[\\frac{\\sum \\left|{\\rm observation-fit}\\right| }{{\\rm number~of~observations}}\\]\n\nMean Absolute Percentage Error (MAPE)\n\n\\[\\frac{\\sum \\frac{\\left|{\\rm observation-fit}\\right|}{{\\rm observation}} }{{\\rm number~of~observations}} {\\times100}\\]"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-continued",
    "href": "slides/Chapter07.html#model-selection-continued",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection (continued)",
    "text": "Model selection (continued)\n\nAvoid over-fitting.\nSo place a penalty for excessive model parameters\nAkaike Information Criterion (AIC; smaller is better)\n\n\\[AIC  =  n\\log \\left(\\frac{SSE}{n} \\right) + 2p\\] - Bayesian Information Criterion (BIC) places a higher penalty that depends on, the number of observations.\n\nAs a result BIC fares well for selecting a model that explains the relationships well while AIC fares well when selecting a model for prediction purposes.\nOther variations: WAIC, AICc, etc (we will not cover them)"
  },
  {
    "objectID": "slides/Chapter07.html#software",
    "href": "slides/Chapter07.html#software",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Software",
    "text": "Software\n\nIn \\(R\\), lm() and step() function will perform the tasks\n\nleaps() and HH packages contain additional functions\ndredge() in MuMIn will produce all the subset models given a full model\nAlso MASS, car, caret, and SignifReg R packages\n\nR base package step-wise selection is based on \\(AIC\\) only."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "href": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validation (review from Chapter 6)",
    "text": "Cross validation (review from Chapter 6)\nIn sample error vs prediction error\n\nFor simpler models, increasing the number of parameters improves the fit to the sample.\nBut it seems to reduce the accuracy of the out-of-sample predictions.\nMost accurate models trade off flexibility (complexity) and overfitting\n\nGeneral idea: - Leave out some observations. - Train the model on the remaining samples; score on those left out. - Average over many left-out sets to get the out-of-sample (future) accuracy."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection-data-example",
    "href": "slides/Chapter07.html#cross-validated-selection-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection: Data example",
    "text": "Cross validated selection: Data example\nConsider the pinetree data set which contains the circumference measurements of pine trees at four positions (First is bottom)"
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection",
    "href": "slides/Chapter07.html#cross-validated-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection",
    "text": "Cross validated selection\n\nModel selection can be done focusing on prediction\n\nmethod = “leapForward” & method = “leapBackward” options\n\n\n\nlibrary(caret);  library(leaps)\nset.seed(123)\nfitControl  &lt;-  trainControl(method  =  \"repeatedcv\",\n                             number  =  5,  repeats  =  100)\nleapBackwardfit  &lt;-  train(Top  ~  .,  data  =  pinetree[, -1],\ntrControl  =  fitControl,  method  =  \"leapBackward\")\nsummary(leapBackwardfit)\n\nSubset selection object\n3 Variables  (and intercept)\n       Forced in Forced out\nThird      FALSE      FALSE\nSecond     FALSE      FALSE\nFirst      FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         Third Second First\n1  ( 1 ) \" \"   \" \"    \"*\"  \n2  ( 1 ) \"*\"   \" \"    \"*\""
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models",
    "href": "slides/Chapter07.html#polynomial-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nA polynomial model includes the square, cube of predictor variables as additional variables.\nHigh correlation (multicollinearity) between the predictor variables may be a problem in polynomial models, but not always."
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example",
    "href": "slides/Chapter07.html#polynomial-models-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\nWe can fit a simple linear regression using the Pine tree data\n\npine1 &lt;- lm(Top ~ First, data = pinetree) \nsummary(pine1)\n\n\nCall:\nlm(formula = Top ~ First, data = pinetree)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.854 -0.881 -0.195  0.630  3.176 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.334      0.765   -8.28  2.1e-11 ***\nFirst          0.763      0.024   31.78  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.29 on 58 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.945 \nF-statistic: 1.01e+03 on 1 and 58 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#look-closer",
    "href": "slides/Chapter07.html#look-closer",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Look closer",
    "text": "Look closer\n\nLooks non-linear"
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example-1",
    "href": "slides/Chapter07.html#polynomial-models-data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                         44.121      7.039    6.27        0\npoly(First, degree = 3, raw = T)1   -3.972      0.695   -5.71        0\npoly(First, degree = 3, raw = T)2    0.142      0.022    6.39        0\npoly(First, degree = 3, raw = T)3   -0.001      0.000   -5.95        0\n\n\n```         \n- For the pinetree example, all the slope coefficients are highly significant for the cubic regression\n- Not so for the quadratic regression\n```\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                           3.85      2.450   1.569    0.122\npoly(First, degree = 2, raw = T)1     0.10      0.155   0.646    0.521\npoly(First, degree = 2, raw = T)2     0.01      0.002   4.319    0.000\n\n\n\nRaw polynomials do not preserve the coefficient estimates but orthogonal polynomials do.\n\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.146  119.26        0\npoly(First, degree = 2)1    41.01      1.130   36.29        0\npoly(First, degree = 2)2     4.88      1.130    4.32        0\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.115  151.03        0\npoly(First, degree = 3)1    41.01      0.892   45.96        0\npoly(First, degree = 3)2     4.88      0.892    5.47        0\npoly(First, degree = 3)3    -5.31      0.892   -5.95        0"
  },
  {
    "objectID": "slides/Chapter07.html#residual-diagnostics",
    "href": "slides/Chapter07.html#residual-diagnostics",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nFor multiple regression fits, including polynomial fits, examine the residuals as usual to-\n- Validate the model assumptions\n- Look for model improvement clues\nQuadratic regression for pinetree data is not satisfactory based on the residual plots shown below:"
  },
  {
    "objectID": "slides/Chapter07.html#categorical-predictors",
    "href": "slides/Chapter07.html#categorical-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nModels can include categorical predictors such as Area in the pinetree dataset\nMake sure that you use the factor() function when numerical codes are assigned to categorical variables.\nArea effect on Top circumference is clear from the following plot"
  },
  {
    "objectID": "slides/Chapter07.html#indicator-variables",
    "href": "slides/Chapter07.html#indicator-variables",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nFactors are employed in a multiple regression using indicator variables which are simply binary variables taking either zero or one\nFor for males and females, indicator variables are defined as follows:\n\nIndicator variable of males: \\(~~~~~~~~\\begin{array}{cccc} I_{\\text {male}} & = & 1 & \\text{for males}\\\\ & & 0& \\text{for females} \\end{array}\\)\nIndicator variable of females \\(~~~~~~~~\\begin{array}{cccc} I_{\\text{female}} & = & 1 & \\text{for females}\\\\ & & 0& \\text{for males} \\end{array}\\)\n\nThere are three different areas of the forest in the pinetree dataset. So we can define three indicator variables.\nOnly two indicator variables are needed because there is only 2 degrees of freedom for the 3 areas."
  },
  {
    "objectID": "slides/Chapter07.html#regression-output",
    "href": "slides/Chapter07.html#regression-output",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Regression output",
    "text": "Regression output\n\n\n\nRegression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.02\n1.11\n17.98\n0.00\n\n\nI2\n-1.96\n1.57\n-1.24\n0.22\n\n\nI3\n-5.92\n1.57\n-3.76\n0.00\n\n\n\n\n\n\n\n\nThe y-intercept is the mean of the response for the omitted category\n\n20.02 is the mean Top circumference for the first Area\n\nslopes are the difference in the mean response\n\n-1.96 is the drop in the mean top circumference in Area 2 when compared to Area 1 (which is not a significant drop)\n-5.92 is the drop in the mean top circumference in Area 3 when compared to Area 1 (which is a highly significant drop)\n\n\nAnalysis of Covariance model employs both numerical and categorical predictors (covered later on).\n\nWe specifically include the interaction between them"
  },
  {
    "objectID": "slides/Chapter07.html#summary",
    "href": "slides/Chapter07.html#summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary",
    "text": "Summary\n\nRegression methods aim to fit a model by least squares to explain the variation in the dependent variable \\(Y\\) by fitting explanatory \\(X\\) variables.\nMatrix plots (EDA) and correlation coefficients provide important clues to the interrelationships.\nFor building a model, the additional variation explained is important. Summary criterion such as \\(AIC\\) is also useful.\nA model is not judged as the best purely on statistical grounds.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives",
    "href": "slides/Chapter08.html#learning-objectives",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine and understand an ANOVA\nUse a one-way ANOVA on example data\nUnderstand and calculate an ANOVA table\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs\nUnderstand the difference between one-way and two-way ANOVAs"
  },
  {
    "objectID": "slides/Chapter08.html#anova",
    "href": "slides/Chapter08.html#anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA",
    "text": "ANOVA\n\nAnalysis of variance, or ANOVA, is an approach to comparing data with multiple means across different groups, and allows us to see patterns and trends within complex and varied data.\nUsed for categorical or grouped data.\nOften data from experiments (treatments make good factors).\nEDA bar, points, or box plots are options to show differences between groups."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model\n\nfabric burn-time data\n\n\n\n\nfabric 1\nfabric 2\nfabric 3\nfabric 4\n\n\n\n\n17.8\n11.2\n11.8\n14.9\n\n\n16.2\n11.4\n11\n10.8\n\n\n17.5\n15.8\n10\n12.8\n\n\n17.4\n10\n9.2\n10.7\n\n\n15\n10.4\n9.2\n10.7\n\n\n\n\nCan we regard the mean burn times of the four fabrics as equal?\n\n\nfabric 1 seems to take longer time to burn\n\nStart with hypotheses and EDA to visualize groups"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-hypotheses",
    "href": "slides/Chapter08.html#one-way-anova-hypotheses",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA Hypotheses",
    "text": "One-way ANOVA Hypotheses\n\n\\(H_0\\): The mean burn times are equal for the four fabrics\n\\(H_a\\): The mean burn time of at least one fabric is different."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova",
    "href": "slides/Chapter08.html#one-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\nfabric=read.table(\"../data/fabric.txt\", header=TRUE, sep=\"\\t\")\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary(fun.data = \"mean_cl_normal\", colour = \"blue\", linewidth = 2, size = 3)\n\n\n\n\n\n\n\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary_bin(fun = \"mean\", geom = \"bar\", orientation = 'x')"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table",
    "href": "slides/Chapter08.html#anova-table",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\n\nobservation = mean + effect + error\n\n\nSS Total = SS Factor + SS Error\n\n\nsummary(aov(burntime ~ fabric, data = fabric))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfabric effect on burntime is highly significant.\n\nIn other words, the null hypothesis of equal mean burntime is rejected.\n\nOr alternatively the mean burntime is different for at least one fabric"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-1",
    "href": "slides/Chapter08.html#anova-table-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nHow are these values calculated?\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-2",
    "href": "slides/Chapter08.html#anova-table-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nCalculating F values of a factor:\n\\(F = \\frac{MS_{Factor}}{MS_{Error}}\\)\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-practice",
    "href": "slides/Chapter08.html#anova-table-practice",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table practice",
    "text": "ANOVA table practice\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\nMSF/MSE\n\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)\n\n\n\n\n\n\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nfabric\n120.5\n\n\n\n\n\n\nResiduals\n46.3\n\n\n\n\n\n\nTOTAL"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-p-values",
    "href": "slides/Chapter08.html#reminder-on-p-values",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on P values",
    "text": "Reminder on P values\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\nA very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\nWe calculate them based on our theoretical sampling distributions (normal, \\(t\\), \\(F\\), \\(\\chi^2\\))"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions\nA sampling distribution is a probabilistic model of sampling variation–it describes the behaviour of some sample statistic\nFor a normal population, when the population parameters and are known, we can easily derive the sampling distributions of the sample mean or sample variance.\nWhen the population parameters are unknown, we have to estimate them from data."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions"
  },
  {
    "objectID": "slides/Chapter08.html#graphical-comparison-of-means",
    "href": "slides/Chapter08.html#graphical-comparison-of-means",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphical comparison of means",
    "text": "Graphical comparison of means\n\nThe graph below shows individual 95% confidence intervals for the fabric means"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "href": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA model Assumptions",
    "text": "One-way ANOVA model Assumptions\n\nResiduals are randomly and normally distributed\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\nThe ANOVA model assumes equal SD for the treatments.\nIf experimental errors are more in some subgroups, divide the problem into separate ones.\nPositive correlation among residuals leads to under estimation of error variance; negative correlation leads to overestimation.\n\n\nThese assumptions are harder to validate to small experimental design data"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions",
    "href": "slides/Chapter08.html#visualize-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nplot(aov(burntime ~ fabric, data = fabric))"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions-1",
    "href": "slides/Chapter08.html#visualize-assumptions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nFigure 1: SD vs mean for four fabrics\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\n\nWith only four fabrics in the sample, it is difficult to make any definitive claim.\nIf the assumptions were valid, we would expect the four points to fall approximately along a horizontal band indicating constant standard deviations, and hence variances, regardless of the means of the groups.\nThis figure suggests that this is the case, so the assumption of equal variances appears to be valid."
  },
  {
    "objectID": "slides/Chapter08.html#equal-variance",
    "href": "slides/Chapter08.html#equal-variance",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Equal variance",
    "text": "Equal variance\nBartlett’s test: - null hypothesis: equal variances - but it has an assumption of its own (response variable must be normally distributed)\nLevene’s test - null hypothesis: equal variances - is applicable for any continuous distribution\n\nbartlett.test(burntime ~ fabric, data = fabric)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  burntime by fabric\nBartlett's K-squared = 2.6606, df = 3, p-value = 0.447\n\ncar::leveneTest(burntime ~ fabric, data = fabric)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.1788 0.9092\n      16"
  },
  {
    "objectID": "slides/Chapter08.html#what-if-the-equal-variance-assumption-is-violated",
    "href": "slides/Chapter08.html#what-if-the-equal-variance-assumption-is-violated",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What if the equal variance assumption is violated?",
    "text": "What if the equal variance assumption is violated?\nANOVA’s are considered to be fairly robust against violations of the equal variances assumption as long as each group has the same sample size.\nIf this assumption is violated, the most common way to deal with it is to transform the response variable using one of the three transformations:\n\nLog Transformation: Transform the response variable from y to log(y).\nSquare Root Transformation: Transform the response variable from y to √y.\nCube Root Transformation: Transform the response variable from y to y1/3.\n\nBy performing these transformations, the problem of heteroscedasticity typically goes away."
  },
  {
    "objectID": "slides/Chapter08.html#normality",
    "href": "slides/Chapter08.html#normality",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Normality",
    "text": "Normality\n\naov_fabric &lt;- aov(burntime ~ fabric, data = fabric)\nshapiro.test(aov_fabric$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov_fabric$residuals\nW = 0.88926, p-value = 0.02606\n\n\nANOVAs are robust to mild issues of non-normality\nBut if have issues with normality and unequal variance try transformations"
  },
  {
    "objectID": "slides/Chapter08.html#when-transformations-do-not-help",
    "href": "slides/Chapter08.html#when-transformations-do-not-help",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When transformations do not help:",
    "text": "When transformations do not help:\n\nWeighted least squares regression: This type of regression assigns a weight to each data point based on the variance of its fitted value.\n\nEssentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.\n\nNon-parametric test:\n\nKruskal-Wallis Test is the non-parametric version of a one-way ANOVA"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd",
    "href": "slides/Chapter08.html#tukey-hsd",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nTukey HSD (Honest Significant Differences) plot allows pairwise comparison of treatment means.\nWhich fabric types have different burn times? (remember the alternative hypothesis of a one-way ANOVA is at least one of the means are different)\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "href": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD (Interval Plot)",
    "text": "Tukey HSD (Interval Plot)"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova",
    "href": "slides/Chapter08.html#two-way-two-factor-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA",
    "text": "Two way (two factor) ANOVA\n\nTwo factors are present.\nA two-way ANOVA is used to estimate how the mean of a continuous variable changes according to the levels of two categorical variables.\nUse a two-way ANOVA when you want to know how two independent variables, in combination, affect a dependent variable.\nVery similar to multiple regression but with categorical variables."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "href": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA example",
    "text": "Two way (two factor) ANOVA example\n\nExample: We will use the built in data ToothGrowth. It contains data from a study evaluating the effect of vitamin C on tooth growth in Guinea pigs. The experiment has been performed on 60 pigs, where each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, (orange juice or ascorbic acid (a form of vitamin C and coded as VC). Tooth length was measured and a sample of the data is shown below.\n\n\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5"
  },
  {
    "objectID": "slides/Chapter08.html#run-a-two-way-anova",
    "href": "slides/Chapter08.html#run-a-two-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Run a Two-way ANOVA",
    "text": "Run a Two-way ANOVA\n\nsummary(aov(len~dose+supp, data=ToothGrowth))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2 2426.4  1213.2   82.81  &lt; 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA two-way ANOVA tests two null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit",
    "href": "slides/Chapter08.html#two-way-model-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nModel:\nObservation = Overall Mean + Factor 1 Effect + Factor 2 Effect + Error\n\n\n\\(H_0\\): factor 1 means are equal; factor 2 means are equal\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2 2426.4  1213.2   82.81  &lt; 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSupplement and dose effects are significant at 5% level."
  },
  {
    "objectID": "slides/Chapter08.html#main-effect-plots",
    "href": "slides/Chapter08.html#main-effect-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Main effect plots",
    "text": "Main effect plots\n\nSimply plot of response means for factor levels"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-1",
    "href": "slides/Chapter08.html#tukey-hsd-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nteeth &lt;- aov(len~supp+dose, data=ToothGrowth)\nTukeyHSD(teeth)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ supp + dose, data = ToothGrowth)\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.679762 -1.720238 0.0004293\n\n$dose\n          diff       lwr       upr p adj\nD1-D0.5  9.130  6.215909 12.044091 0e+00\nD2-D0.5 15.495 12.580909 18.409091 0e+00\nD2-D1    6.365  3.450909  9.279091 7e-06"
  },
  {
    "objectID": "slides/Chapter08.html#summary-so-far",
    "href": "slides/Chapter08.html#summary-so-far",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary so far",
    "text": "Summary so far\n\nANOVA: 1 and 2 factor\nANOVA table\nAssumptions\nTukey’s HSD"
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives-part-2",
    "href": "slides/Chapter08.html#learning-objectives-part-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives: Part 2",
    "text": "Learning Objectives: Part 2\n\nUnderstand the use of interactions in linear models\nUse a two-way ANOVA with interactions on example data\nUnderstand and calculate an ANOVA table when interactions are present\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs with interactions\nUnderstand the difference and similarities between this chapter and previous linear models covered\nReview non-parametric options"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-effect",
    "href": "slides/Chapter08.html#interaction-effect",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction effect",
    "text": "Interaction effect\n\nWhether Factor A effects are constant over Factor B effects or Factor B effects are constant over Factor A effects?\n\nIf the answer is no, then there is an interaction between A & B.\n\nExample:\n\nTemperature and pressure are factors affecting the yield in chemical experiments.\nThey do interact in a mechanistic sense.\n\n\nInteraction may or may not have physical meaning."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "href": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way ANOVA and Interaction effects",
    "text": "Two-way ANOVA and Interaction effects\nA two-way ANOVA with interaction tests three null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable\nThere is no interaction effect between the two variables"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plots",
    "href": "slides/Chapter08.html#interaction-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plots",
    "text": "Interaction Plots\n\nIn the absence of interaction, the plotted means of factor will be roughly parallel\n\nsee Plot 1. A & B do not interact.\n\nIf the the plotted means of factor crossings are far from parallel, then there is interaction\n\nPlot 2 shows extreme (antagonistic) interaction between A & B."
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "href": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plot for Zooplankton data",
    "text": "Interaction Plot for Zooplankton data\n\nlibrary(ggplot2)\np1= ggplot(data = ToothGrowth, aes(x = supp, y = len, group=dose, colour=dose)) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\n\np2= ggplot(data = ToothGrowth, aes(x = dose, y = len, group=supp, colour=supp)) +stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\nlibrary(patchwork)\np1+p2\n\n\n\nInteraction effect may be present"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-1",
    "href": "slides/Chapter08.html#two-way-model-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nobservation = mean + Factor A + Factor B + interaction effect + error\n\n\nThe above model is known as multiplicative model\nIf interaction effect is ignored, we deal with an additive model"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-2",
    "href": "slides/Chapter08.html#two-way-model-fit-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nToothGrowth |&gt; \n  aov(formula = len ~ supp * dose) |&gt; \n  summary()\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  15.572 0.000231 ***\ndose         2 2426.4  1213.2  92.000  &lt; 2e-16 ***\nsupp:dose    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#residual-diagnostics",
    "href": "slides/Chapter08.html#residual-diagnostics",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics"
  },
  {
    "objectID": "slides/Chapter08.html#interpreting-interactions",
    "href": "slides/Chapter08.html#interpreting-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions\n\nIf an interaction term is significant, the effect of Factor A depends on Factor B (and vice versa)\nUse interaction plots to visualize\nPosthoc tests to examine different level combinations\n\n\nTukeyHSD(modl)$`supp:dose`\n\n                 diff        lwr        upr        p adj\nVC:D0.5-OJ:D0.5 -5.25 -10.048124 -0.4518762 2.425209e-02\nOJ:D1-OJ:D0.5    9.47   4.671876 14.2681238 4.612304e-06\nVC:D1-OJ:D0.5    3.54  -1.258124  8.3381238 2.640208e-01\nOJ:D2-OJ:D0.5   12.83   8.031876 17.6281238 2.125153e-09\nVC:D2-OJ:D0.5   12.91   8.111876 17.7081238 1.769939e-09\nOJ:D1-VC:D0.5   14.72   9.921876 19.5181238 2.985978e-11\nVC:D1-VC:D0.5    8.79   3.991876 13.5881238 2.100948e-05\nOJ:D2-VC:D0.5   18.08  13.281876 22.8781238 4.855005e-13\nVC:D2-VC:D0.5   18.16  13.361876 22.9581238 4.821699e-13\nVC:D1-OJ:D1     -5.93 -10.728124 -1.1318762 7.393032e-03\nOJ:D2-OJ:D1      3.36  -1.438124  8.1581238 3.187361e-01\nVC:D2-OJ:D1      3.44  -1.358124  8.2381238 2.936430e-01\nOJ:D2-VC:D1      9.29   4.491876 14.0881238 6.908163e-06\nVC:D2-VC:D1      9.37   4.571876 14.1681238 5.774013e-06\nVC:D2-OJ:D2      0.08  -4.718124  4.8781238 1.000000e+00"
  },
  {
    "objectID": "slides/Chapter08.html#interpreting-interactions-1",
    "href": "slides/Chapter08.html#interpreting-interactions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions"
  },
  {
    "objectID": "slides/Chapter08.html#importance-of-interactions",
    "href": "slides/Chapter08.html#importance-of-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Importance of interactions",
    "text": "Importance of interactions\n\nWhen you have statistically significant interaction effects, you can’t interpret the main effects without considering the interactions\nInteractions can be used to control for unhelpful variation (blocking effect in experiments)\nInteractions can be mechanistically meaningful"
  },
  {
    "objectID": "slides/Chapter08.html#anova-extensions-and-inference",
    "href": "slides/Chapter08.html#anova-extensions-and-inference",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA extensions and inference",
    "text": "ANOVA extensions and inference\nLinear models can theoretically be performed with an infinite amount of predictors\nThe more predictors the more samples needed, your effect sample number is no longer your total sample size but the lowest treatment\nInteraction effects with more than 3 (sometimes even more than 2) become very difficult to interpret and visualize\nGood experimental design and causal thinking can aid in analysis design, design test then collect data, it is much harder the other way around"
  },
  {
    "objectID": "slides/Chapter08.html#indicator-variables",
    "href": "slides/Chapter08.html#indicator-variables",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nIndicator variables are used if the predictor is qualitative rather than quantitative.\n\nConsider gender, a categorical variable.\n\nLet \\(I_1\\) be an indicator variable that takes a value 1 for males and 0 for females.\n\nLet \\(I_2\\) takes 1 for females and 0 for males.\nNote only one of \\(I_1\\) & \\(I_2\\) is sufficient.\n\n\nThe minimum number of indicator variables needed is related to degrees of freedom."
  },
  {
    "objectID": "slides/Chapter08.html#anova-through-regression",
    "href": "slides/Chapter08.html#anova-through-regression",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA through regression",
    "text": "ANOVA through regression\n\nConsider the burn-time data for the four fabrics.\n\nThe four fabric types are categorical.\nDefine-\n\n\\(I_1\\) = 1 for fabric 1 and 0 otherwise\n\\(I_2\\) = 1 for fabric 2 and 0 otherwise\n\\(I_3\\) = 1 for fabric 3 and 0 otherwise\n\\(I_4\\) = 1 for fabric 4 and 0 otherwise\n\n\nNote that any THREE indicator variables are sufficient for the four fabrics.\n\n3 df for 4 fabrics"
  },
  {
    "objectID": "slides/Chapter08.html#regression-summary",
    "href": "slides/Chapter08.html#regression-summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression summary",
    "text": "Regression summary\n\nRegress the burn-time response on the indicator variables \\(I_1\\),\\(I_2\\), \\(I_3\\) & \\(I_4\\)\n\n\n\n   burntime   fabric I1 I2 I3 I4\n1      17.8 Fabric 1  1  0  0  0\n2      16.2 Fabric 1  1  0  0  0\n3      17.5 Fabric 1  1  0  0  0\n4      17.4 Fabric 1  1  0  0  0\n5      15.0 Fabric 1  1  0  0  0\n6      11.2 Fabric 2  0  1  0  0\n7      11.4 Fabric 2  0  1  0  0\n8      15.8 Fabric 2  0  1  0  0\n9      10.0 Fabric 2  0  1  0  0\n10     10.4 Fabric 2  0  1  0  0\n11     11.8 Fabric 3  0  0  1  0\n12     11.0 Fabric 3  0  0  1  0\n13     10.0 Fabric 3  0  0  1  0\n14      9.2 Fabric 3  0  0  1  0\n15      9.2 Fabric 3  0  0  1  0\n16     14.9 Fabric 4  0  0  0  1\n17     10.8 Fabric 4  0  0  0  1\n18     12.8 Fabric 4  0  0  0  1\n19     10.7 Fabric 4  0  0  0  1\n20     10.7 Fabric 4  0  0  0  1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova",
    "href": "slides/Chapter08.html#regression-and-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\n\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n\n\n\nCompare with the one-way output\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova-1",
    "href": "slides/Chapter08.html#regression-and-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\nfabric |&gt; lm(formula = burntime~I1+I2+I3) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: burntime\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nI1         1 111.521 111.521 38.5718 1.248e-05 ***\nI2         1   1.408   1.408  0.4871    0.4952    \nI3         1   7.569   7.569  2.6179    0.1252    \nResiduals 16  46.260   2.891                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCompare with the one-way output\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "href": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Analysis of Covariance (ANCOVA)",
    "text": "Analysis of Covariance (ANCOVA)\nAnalysis of covariance (ANCOVA) is a statistical method that combines linear regression and analysis of variance (ANOVA) to evaluate the relationship between a response variable and various independent variables while controlling for covariates.\n\nIndicator variables are used as additional regressors along with a quantitative predictor (covariate)."
  },
  {
    "objectID": "slides/Chapter08.html#data-example-test-anxiety",
    "href": "slides/Chapter08.html#data-example-test-anxiety",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data example: test anxiety",
    "text": "Data example: test anxiety\nResearchers investigated the effect of exercises in reducing the level of anxiety, where they measured the anxiety score of three groups of individuals practicing physical exercises at different levels (grp1: low, grp2: moderate and grp3: high).\nThe anxiety score was measured pre- and 6-months post-exercise training programs. It is expected that any reduction in the anxiety by the exercises programs would also depend on the participant’s basal level of anxiety score.\nIn this analysis we use the pretest anxiety score (pretest) as the covariate and are interested in possible differences between group with respect to the post-test anxiety scores.\n\n\n# A tibble: 3 × 4\n  id    group pretest posttest\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 15    grp1     19.8     19.4\n2 30    grp2     19.3     17.7\n3 33    grp3     15.5     11"
  },
  {
    "objectID": "slides/Chapter08.html#interaction",
    "href": "slides/Chapter08.html#interaction",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction",
    "text": "Interaction\nAre the regression slopes the same?\n\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \ngroup          2  72.13   36.07 203.973 &lt;2e-16 ***\npretest        1 101.29  101.29 572.828 &lt;2e-16 ***\ngroup:pretest  2   0.04    0.02   0.127  0.881    \nResiduals     39   6.90    0.18                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInteraction term is not significant (at sig. level of 5%)."
  },
  {
    "objectID": "slides/Chapter08.html#check-assumptions",
    "href": "slides/Chapter08.html#check-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Check assumptions",
    "text": "Check assumptions\nLinearity:\n\nAppears to be a linear relationship in each training group"
  },
  {
    "objectID": "slides/Chapter08.html#assumptions",
    "href": "slides/Chapter08.html#assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Assumptions",
    "text": "Assumptions\nResiduals:"
  },
  {
    "objectID": "slides/Chapter08.html#assumptions-1",
    "href": "slides/Chapter08.html#assumptions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Assumptions",
    "text": "Assumptions\nNormality:\n\n\n# A tibble: 1 × 3\n  variable                 statistic p.value\n  &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n1 summary(model$residuals)     0.964   0.849\n\n\nThe Shapiro Wilk test was not significant (\\(\\alpha = 0.05\\)), so we can assume normality of residuals\nEqual variance:\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  0.2604  0.772\n      42               \n\n\nThe Levene’s test was not significant (\\(\\alpha = 0.05\\)), so we can assume homogeneity of the residual variances for all groups."
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit",
    "href": "slides/Chapter08.html#ancova-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\nEstimates are the slope in each treatment (group). Remember that R assigns the first (alphabetical) level of the treatment to Intercept\n\n\n\nCall:\nlm(formula = posttest ~ pretest + group, data = anxiety)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00467 -0.29485 -0.02866  0.33846  0.68799 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.94088    0.72574  -1.296 0.202074    \npretest      1.02775    0.04202  24.461  &lt; 2e-16 ***\ngroupgrp2   -0.64112    0.15137  -4.235 0.000126 ***\ngroupgrp3   -2.98463    0.15027 -19.862  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4114 on 41 degrees of freedom\nMultiple R-squared:  0.9615,    Adjusted R-squared:  0.9587 \nF-statistic: 341.5 on 3 and 41 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit-1",
    "href": "slides/Chapter08.html#ancova-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\nUse ANOVA table to display\n\n\nAnalysis of Variance Table\n\nResponse: posttest\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npretest    1 99.400  99.400  587.16 &lt; 2.2e-16 ***\ngroup      2 74.023  37.011  218.63 &lt; 2.2e-16 ***\nResiduals 41  6.941   0.169                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPost-hoc tests to see which group is different\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = posttest ~ group + pretest, data = anxiety)\n\n$group\n               diff       lwr        upr p adj\ngrp2-grp1 -1.093333 -1.458662 -0.7280048     0\ngrp3-grp1 -3.060000 -3.425329 -2.6946715     0\ngrp3-grp2 -1.966667 -2.331995 -1.6013381     0"
  },
  {
    "objectID": "slides/Chapter08.html#data-example-fast-food-resturants",
    "href": "slides/Chapter08.html#data-example-fast-food-resturants",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data example: fast-food resturants",
    "text": "Data example: fast-food resturants\n\n\n Restaurant  Sales Households Location I1 I2\n          1 135.27        155  Highway  0  0\n          2  72.74         93  Highway  0  0\n          3 114.95        128  Highway  0  0\n          4 102.93        114  Highway  0  0\n          5 131.77        158  Highway  0  0\n          6 160.91        183  Highway  0  0\n          7 179.86        178     Mall  1  0\n          8 220.14        215     Mall  1  0\n          9 179.64        172     Mall  1  0\n         10 185.92        197     Mall  1  0\n         11 207.82        207     Mall  1  0\n         12 113.51         95     Mall  1  0\n         13 203.98        224   Street  0  1\n         14 174.48        199   Street  0  1\n         15 220.43        240   Street  0  1\n         16  93.19        100   Street  0  1\n\n\n\nDo we need three separate models?"
  },
  {
    "objectID": "slides/Chapter08.html#ancova",
    "href": "slides/Chapter08.html#ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA",
    "text": "ANCOVA\n\n\nIn order to allow for different slopes for each location, we define the product (or interaction) variables"
  },
  {
    "objectID": "slides/Chapter08.html#data-set-up",
    "href": "slides/Chapter08.html#data-set-up",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data set up",
    "text": "Data set up\n\n\n  Restaurant  Sales Households Location I1 I2\n1          1 135.27        155  Highway  0  0\n2          2  72.74         93  Highway  0  0\n3          3 114.95        128  Highway  0  0\n4          4 102.93        114  Highway  0  0\n5          5 131.77        158  Highway  0  0\n6          6 160.91        183  Highway  0  0"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model",
    "href": "slides/Chapter08.html#ancova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\nWe examine the relationship between restaurant sales (response variable, in thousands of dollars) and the number of households (H) in the restaurant’s trading area and the location of the restaurant (Mall, Street, and Highway). We can use the indicator variables (\\(I_1\\) and \\(I_2\\)) to define our three locations uniquely.\n\\(Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\nThis model provides a separate model for each location as well as allows for the interaction between location of the restaurant and the number of households through the slope coefficient"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model-1",
    "href": "slides/Chapter08.html#ancova-model-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\n\\(Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\n\nFor Highway Locations: \\(I_1=0\\) & \\(I_2=0\\) hence our model simplifies to\n\\(Sales = \\beta_{0} + (\\beta_3)Households\\)\nFor Mall Locations: \\(I_1=1\\) & \\(I_2=0\\) so the model becomes\n\\(Sales = \\beta_{0} + \\beta_{1} + (\\beta_3 + \\beta_4)Households\\)\nFor Street Locations: \\(I_1=0\\) & \\(I_2=1\\) so the model becomes \\(Sales = \\beta_{0} + \\beta_2 + (\\beta_3 + \\beta_5) Households\\)\n\nnote R does not require us to code Location as an indicator variable"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit-2",
    "href": "slides/Chapter08.html#ancova-fit-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\n\nmodl = lm(Sales~Households*Location, data = restaurant)\nouts &lt;- summary(modl)$coefficients\nround(outs[, 1:4], digits = 3)\n\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 -6.203     11.818  -0.525    0.611\nHouseholds                   0.909      0.083  10.906    0.000\nLocationMall                39.223     16.451   2.384    0.038\nLocationStreet               8.036     16.273   0.494    0.632\nHouseholds:LocationMall     -0.074      0.104  -0.710    0.494\nHouseholds:LocationStreet   -0.012      0.101  -0.120    0.907\n\n\nWe can plug these numbers into our equations \\(Sales = \\beta_{0} + \\beta_{1} I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\n\\(Sales = -6.2 + 39.22 + 8.04 + ( 0.909 -0.074 -0.012) Households\\)"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model-2",
    "href": "slides/Chapter08.html#ancova-model-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\n\\(Sales = -6.2 + 39.22 + 8.04 + ( 0.909 -0.074 -0.012) Households\\)\n\nFor Highway Locations: \\(I_1=0\\) & \\(I_2=0\\) hence our model simplifies to\n\\(Sales = -6.2 + (0.909)Households\\)\nFor Mall Locations: \\(I_1=1\\) & \\(I_2=0\\) so the model becomes\n\\(Sales = -6.2 + 39.22 + (0.909 -0.074)Households\\)\nFor Street Locations: \\(I_1=0\\) & \\(I_2=1\\) so the model becomes \\(Sales = -6.2 + 8.04 + (0.909 -0.012) Households\\)"
  },
  {
    "objectID": "slides/Chapter08.html#graphing-the-model",
    "href": "slides/Chapter08.html#graphing-the-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphing the model",
    "text": "Graphing the model"
  },
  {
    "objectID": "slides/Chapter08.html#summary",
    "href": "slides/Chapter08.html#summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary",
    "text": "Summary\nANOVA models study categorical predictors (factors).\n-   Interaction between factors is important. \n-   ANOVA models and regression models are related and fall under a general family of linear models.\nANCOVA models employs both numerical variables (covariates) and qualitative factors for modelling.\n-   Interaction between factors and covariates is important."
  },
  {
    "objectID": "slides/Chapter08.html#review-of-non-parametric-tests",
    "href": "slides/Chapter08.html#review-of-non-parametric-tests",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Review of non-parametric tests",
    "text": "Review of non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations).\nMany non-parametric methods rely on replacing the observed data by their ranks."
  },
  {
    "objectID": "slides/Chapter08.html#spearmans-rank-correlation",
    "href": "slides/Chapter08.html#spearmans-rank-correlation",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\n\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient.\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\nlibrary(GGally)\n\np &lt;- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n\n\n\n\n\n\n\nFigure 2: Comparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter08.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter08.html#wilcoxon-signed-rank-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\nt.test(tv$TELETIME, mu=1680)\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter08.html#non-parametric-anova",
    "href": "slides/Chapter08.html#non-parametric-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Non-parametric ANOVA",
    "text": "Non-parametric ANOVA\nKruskal-Wallis test: allows to compare three or more groups\nMann-Whitney test: allows to compare 2 groups under the non-normality assumption."
  },
  {
    "objectID": "slides/Chapter08.html#kruskal-wallis-test",
    "href": "slides/Chapter08.html#kruskal-wallis-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Kruskal-Wallis test",
    "text": "Kruskal-Wallis test\nThe null and alternative hypotheses of the Kruskal-Wallis test are:\nH0: The 3 groups are equal in terms of the variable H1: At least one group is different from the other 2 groups in terms of variable\nSimilar to an ANOVA the alternative hypothesis is not that all groups are different. The opposite of all groups being equal (H0) is that at least one group is different from the others (H1).\nIn this sense, if the null hypothesis is rejected, it means that at least one group is different from the other 2, but not necessarily that all 3 groups are different from each other. Post-hoc tests must be performed to test whether all 3 groups differ."
  },
  {
    "objectID": "slides/Chapter08.html#mann-whitney-test",
    "href": "slides/Chapter08.html#mann-whitney-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\nload(\"../data/rangitikei.Rdata\")\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\nt.test(rangitikei$people~rangitikei$time)\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter08.html#another-form-of-test",
    "href": "slides/Chapter08.html#another-form-of-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Another form of test",
    "text": "Another form of test\n\n\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter08.html#linear-models-review",
    "href": "slides/Chapter08.html#linear-models-review",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Linear models Review",
    "text": "Linear models Review\nFamily tree\nT-tests, regressions, ANOVAs, and ANCOVAs are related\nSimilarities:\n-   1 continuous response variable\n-   Assumptions: normality, equal variance, and independence of residuals\nDifferences:\n- EDA\n- test statistics\n- number and type of predictors"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nA t-test is used to determine whether or not there is a statistically significant difference between the means of two groups\n\nsample group vs hypothetical population One-sample t-test\ntwo independent samples Two-sample t-test\n\nalso called independent t-test because the two samples are considered independent\n\nsamples linked in some why, not independent Paired t-test\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nThe main difference between a t-test and an ANOVA is in how the two tests calculate their test statistic to determine if there is a statistically significant difference between groups.\nT-test:\n\nT statistic: ratio of the mean difference between two groups relative to overal standard deviation of the differences\nOne-sample t-test: \\(t = \\frac{x-\\mu}{s/\\sqrt n}\\)\nTwo-sample t-test: \\(t = \\frac{(\\bar{x_{1}}-\\bar{x_{2}})-d}{\\frac{s_1}{\\sqrt n} + \\frac{s_2}{\\sqrt n}}\\)\n\nPaired t-test: \\(t=\\frac{\\bar{d}}{\\frac{S_{d}}{\\sqrt n}}\\)\n\n\\(s =\\) sample standard deviation \\(d =\\) difference\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\) Where \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\nMSF/MSE"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a t.test or an ANOVA?",
    "text": "When to use a t.test or an ANOVA?\nIn practice, when we want to compare the means of two groups, we use a t-test. When we want to compare the means of three or more groups, we use an ANOVA.\nSuppose we have three groups we wish to compare the means between: group A, group B, and group C.\nIf you did the following t-tests:\n\nA t-test to compare the difference in means between group A and group B\nA t-test to compare the difference in means between group A and group C\nA t-test to compare the difference in means between group B and group C"
  },
  {
    "objectID": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "href": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What if we just did many t-tests?",
    "text": "What if we just did many t-tests?\nFor each t-test there is a chance that we will commit a type I error, which is the probability that we reject the null hypothesis when it is actually true. Let’s say we set this to 0.05 (\\(\\alpha = 0.05\\)). This means that when we perform multiple t-tests, this error rate increases.\n\nThe probability that we commit a type I error with one t-test is 1 – 0.95 = 0.05.\nThe probability that we commit a type I error with two t-tests is 1 – (0.95^2) = 0.0975.\nThe probability that we commit a type I error with three t-tests is 1 – (0.95^3) = 0.1427.\n\nThe type I error just increases!\nWe use a post-hoc (after) test to see which group is driving differences, these pairwise comparisons have a correction factor to adjust our Type I error"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nA regression is used to understand the relationship between predictor variable(s) and a response variable\n\none predictor Simple Regression\ntwo or more predictors Multiple Regression\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…\n\nNot too much difference there"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nRegression:\n\nT statistic: test if each predictor’s estimate is different from 0\n\\(R^2\\): proportion of variance explained \\(\\frac{SSR}{SST}\\)\nF statistic: overall F statistic for the regression model, \\(\\frac{MS_{reg}}{MS_{error}}\\) where \\(MS = \\frac{SS}{df}\\)\n\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\)\n\nwhere \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\n\nMSF/MSE\n\nThe F statistic is the same, thats why you can produce an ANOVA table from your regression"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a regression or an ANOVA?",
    "text": "When to use a regression or an ANOVA?\nConventionally, a regression refers to continuous predictors and ANOVAs are used for discrete variables.\nBUT …\nYou can code discrete variable as indicator variables and then treat them as continuous and run a regression!\nGenerally, if you have 1 continuous and 1 discrete variable you should generate indicator variables and run a multiple regression\nIf you are using the continuous predictor as a covariate it would be called an ANCOVA.\nYou can code a discrete variable into continuous but if you do the reverse you lose statistical inference."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-hypotheses",
    "href": "slides/Chapter08.html#reminder-on-hypotheses",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on Hypotheses",
    "text": "Reminder on Hypotheses\nT-Test: difference between the means of two groups\n\nNull: \\(\\mu_{1} - \\mu_{2}\\) is equal to \\(0\\) Can be \\(=\\), \\(\\le\\), or \\(\\ge\\)\nAlt: \\(\\mu_{1} - \\mu_{2}\\) is not equal to \\(0\\) Can be \\(\\ne\\), \\(&lt;\\), or \\(&gt;\\)\n\nRegressions: understand the relationship between predictor variable(s) and a response variable\n\nNull: true slope coefficient is \\(= 0\\) (i.e. no relationship)\nAlt: true slope coefficient \\(\\ne 0\\) (i.e. relationship)\n\nANOVA: difference between the means of three or more groups\n\nNull: group means are equal\nAlt: At least one mean is different"
  },
  {
    "objectID": "slides/Chapter08.html#where-to-go-from-here",
    "href": "slides/Chapter08.html#where-to-go-from-here",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nLinear models form the basis of a lot more statistics tests.\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "workshops/ws01.html",
    "href": "workshops/ws01.html",
    "title": "Getting Started With R",
    "section": "",
    "text": "If you have not installed R and RStudio, refer to the study guide for some instructions.\nTo start this workshop download the source from the class website. You can now save this to your computer and make notes in this document."
  },
  {
    "objectID": "workshops/ws01.html#grammar-of-graphics",
    "href": "workshops/ws01.html#grammar-of-graphics",
    "title": "Getting Started With R",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe main idea behind the grammar of graphics of (Wilkinson 2005) is to mimic the manual graphing approach and define building blocks and combine them to create a graphical display. The building blocks of a graph are:\n\ndata\naesthetic mapping\ngeometric object\ntransformation or re-expression of data\nscales\ncoordinate system\nposition adjustments\nfaceting\n\nIf have not installed ggplot2 or tidyverse, install it with the following commands.\n\n\nCode\ninstall.packages(\"ggplot2\")\n\n\nWe can now load the ggplot2 library with the commands:\n\n\nCode\nlibrary(ggplot2)\n\n\nIn order to work with ggplot2, we must have a data frame or a tibble containing our data. We need to specify the aesthetics or how the columns of our data frame can be translated into positions, colours, sizes, and shapes of graphical elements.\nThe geometric objects and aesthetics of the ggplot2 system are explained below:"
  },
  {
    "objectID": "workshops/ws01.html#aesthetic-mapping-aes",
    "href": "workshops/ws01.html#aesthetic-mapping-aes",
    "title": "Getting Started With R",
    "section": "Aesthetic Mapping (aes)",
    "text": "Aesthetic Mapping (aes)\nIn ggplot land aesthetic means visualisation features or aesthetics. These are\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nAesthetic mappings are set with the aes() function."
  },
  {
    "objectID": "workshops/ws01.html#geometric-objects-geom",
    "href": "workshops/ws01.html#geometric-objects-geom",
    "title": "Getting Started With R",
    "section": "Geometric Objects (geom)",
    "text": "Geometric Objects (geom)\nGeometric objects or geoms are the actual marking or inking on a plot such as:\n\npoints (geom_point, for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for boxplots)\n\nA plot must have at least one geom but there is no upper limit. In order to add a geom to a plot, the + operator is employed. A list of available geometric objects can be obtained by typing geom_&lt;tab&gt; in Rstudio. The following command can also be used which will open a Help window.\nhelp.search(\"geom_\", package = \"ggplot2\")\nConsider the study guide dataset rangitikei.txt (Recreational Use of the Rangitikei river). The first 10 rows of this dataset are shown below:\n\n\n   id loc time w.e cl wind temp river people vehicle\n1   1   1    2   1  1    2    2     1     37      15\n2   2   1    1   1  1    2    1     2     23       6\n3   3   1    2   1  1    2    2     3     87      31\n4   4   2    2   1  1    2    1     1     86      27\n5   5   2    1   1  1    2    2     2     19       2\n6   6   2    2   1  2    1    3     3    136      23\n7   7   1    2   2  2    2    2     3     14       8\n8   8   1    2   1  2    2    2     3     67      26\n9   9   1    1   2  1    3    1     2      4       3\n10 10   2    2   1  2    2    2     3    127      45\n\n\nThe description of the variables is given below:\nloc - two locations were surveyed, coded 1, 2\ntime - time of day, 1 for morning, 2 for afternoon\nw.e - coded 1 for weekend, 2 for weekday\ncl- cloud cover, 1 for &gt;50%, 2 for &lt;50%\nwind- coded 1 through 4 for increasing wind speed\ntemp - temperature, 1, 2 or 3 increasing temp\nriver- murkiness of river in 3 increasing categories\npeople - number of people at that location and time\nvehicle- number of vehicles at that location at that time\n\nThis dataset is downloaded from the web using the following commands.\n\n\nCode\nmy.data &lt;- read.csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", \n  header=TRUE\n  )\n\n\n\n\nCode\nggplot(data = my.data,\n       mapping = aes(x = vehicle, y = people)\n       ) +\n  geom_point()\n\n\n\n\n\nThe aes part defines the “aesthetics”, which is how columns of the dataframe map to graphical attributes such as x and y position, colour, size, etc. An aesthetic can be either numeric or categorical and an appropriate scale will be used. After this, we add layers of graphics. geom_point layer is employed to map x and y and we need not specify all the options for geom_point.\nThe aes() can be specified within the ggplot function or as its own separate function. I prefer this format.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point()\n\n\n\n\n\nWe can add a title using labs() or ggtitle() functions. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  ggtitle(\"No. of people vs No. of vehicles\")\n\n\nor\n\n\nCode\nggplot(my.data)+\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  labs(title = \"No. of people vs No. of vehicles\")\n\n\nNote that labs() allows captions and subtitles.\ngeom_smooth is additionally used to show trends.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSimilar to geom_smooth, a variety of geoms are available.\n\n\nCode\nggplot(my.data) + \n  aes(x = factor(wind), y = people) +\n  geom_boxplot()\n\n\n\n\n\nEach geom accepts a particular set of mappings;for example geom_text() accepts a labels mapping. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_text(aes(label = w.e), \n            size = 5)\n\n\nThe faceting option allows a collection of small plots with the same scales. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x=vehicle, y=people) + \n  geom_point() +\n  facet_wrap(~ river) \n\n\n\n\n\nFaceting is the ggplot2 option to create separate graphs for subsets of data. ggplot2 offers two functions for creating small multiples:\n\nfacet_wrap(): define subsets as the levels of a single grouping variable\nfacet_grid(): define subsets as the crossing of two grouping variables\n\nThe following arguments are common to most scales in ggplot2:\n\nname: the first argument gives the axis or legend title\nlimits: the minimum and maximum of the scale\nbreaks: the points along the scale where labels should appear\nlabels: the labels that appear at each break\n\nSpecific scale functions may have additional arguments. Some of the available Scales are:\n\n\n\nScale\nExamples\n\n\n\n\nscale_color_\nscale_color_discrete\n\n\nscale_fill_\nscale_fill_continuous\n\n\nscale_size_\nscale_size_manual\n\n\n\nscale_size_discrete\n\n\n\n\n\n\nscale_shape_\nscale_shape_discrete\n\n\n\nscale_shape_manual\n\n\nscale_linetype_\nscale_linetype_discrete\n\n\n\n\n\n\nscale_x_\nscale_x_continuous\n\n\n\nscale_x_log\n\n\n\nscale_x_date\n\n\nscale_y_\nscale_y_reverse\n\n\n\nscale_y_discrete\n\n\n\nscale_y_datetime\n\n\n\nIn RStudio, we can type scale_ followed by TAB to get the whole list of available scales.\nTry-\n\n\nCode\nggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  scale_x_continuous(name = \"No. of Vehicles\") + \n  scale_y_continuous(name = \"No. of people\") + \n  scale_color_discrete(name = \"Temperature\")\n\n\n\n\n\nThe other coding option is shown below:\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  xlab(\"No. of Vehicles\") + \n  ylab(\"No. of people\") + \n  labs(colour=\"Temperature\") \n\n\nNote that a desired graph can be obtained in more than one way.\nThe ggplot2 theme system handles plot elements (not data based) such as\n\nAxis labels\nPlot background\nFacet label background\nLegend appearance\n\nBuilt-in themes include:\n\ntheme_gray() (default)\ntheme_bw()\ntheme_minimal()\ntheme_classic()\n\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\n\nNote that the graph is assigned an object name p1 and nothing will be printed unless we then print the object p1.\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\np1\n\n\n\n\n\nTry-\n\n\nCode\np1 + theme_light()\n\n\n\n\n\n\n\nCode\np1 + theme_bw()\n\n\n\n\n\nSpecific theme elements can be overridden using theme(). For example:\n\n\nCode\np1 + theme_minimal() +\n  theme(text = element_text(color = \"red\"))\n\n\n\n\n\nAll theme options can be seen with ?theme.\nTo specify a theme for a whole document, use\n\n\nCode\ntheme_set(theme_minimal())\n\n\nMinimal graphing can be done using the qplot option that will produce a few standard formatted graphs quickly.\n\n\nCode\nqplot(people, vehicle, data = my.data, colour = river)\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nTry-\nqplot(people, data = my.data)\nqplot(people, fill=factor(river), data=my.data)\nqplot(people, data = my.data, geom = \"dotplot\")\nqplot(factor(river), people, data = my.data, geom = \"boxplot\")\nA cheat sheet for ggplot2 is available at https://www.rstudio.com/resources/cheatsheets/ (optional to download). There are many other packages which incorporate ggplot2 based graphs or dependent on it.\nThe library patchwork allows complex composition arbitrary plots, which are not produced using the faceting option. Try\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- qplot(people, data = my.data, geom = \"dotplot\")\np2 &lt;- qplot(people, data = my.data, geom = \"boxplot\")\np3 &lt;- ggplot(my.data, aes(x = vehicle, y = people)) + geom_point()\n\n(p1 + p2) / p3 + \n  plot_annotation(\"My title\", caption = \"My caption\")\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "workshops/ws01.html#ggplot-builder",
    "href": "workshops/ws01.html#ggplot-builder",
    "title": "Getting Started With R",
    "section": "ggplot builder",
    "text": "ggplot builder\nA nice R package, known as esquisse is available to build few simple ggplot graphics interactively. This may help in the early stages of learning to use ggplot graphing.\nIf this package is not installed, install it first & then try.\nlibrary(esquisse)\noptions(\"esquisse.display.mode\" = \"browser\")\nesquisse::esquisser(data = iris)\nYou can also load the desired dataset within R studio and select the dataset.\nThe other option is to load a dataset from the course data web folder and then launch esquisse. Try-\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\"\ndownload.file(url = url1, destfile = \"rangitikei.RData\")\nload(\"rangitikei.RData\")\nesquisse::esquisser(data = rangitikei, viewer = \"browser\")\nYou can also download the associated R codes or save the graph within the esquisse web app."
  },
  {
    "objectID": "workshops/ws01.html#dplyr",
    "href": "workshops/ws01.html#dplyr",
    "title": "Getting Started With R",
    "section": "dplyr",
    "text": "dplyr\nThe following six functions of dplyr are very useful for data wrangling :\n\nFor selecting columns, use select()\nFor subsetting data, use filter()\nFor re-ordering (e.g. ascending/descending), use arrange()\nFor augmenting new calculated columns, use mutate()\nFor computing summary measures, use summarise()\nFor group-wise computations (e.g. summary measures), use group_by()\n\nThere are many other functions such as transmute() which will add newly calculated columns to the existing data frame but drop all unused columns. The across() function extends group_by() and summarise() functions for multiple column and function summaries. For example, you like to report rounded data in a table, which calls for an operation across both rows and columns."
  },
  {
    "objectID": "workshops/ws01.html#piping",
    "href": "workshops/ws01.html#piping",
    "title": "Getting Started With R",
    "section": "Piping",
    "text": "Piping\n\n\n\n\n\n\nTip\n\n\n\nThe piping operation is a fundamental aspect of computer programming. The semantics of pipes is taking the output from the left-hand side and passing it as input to the right-hand side.\n\n\nThe R package magrittr introduced the pipe operator %&gt;% and can be pronounced as “then”. In RStudio windows/Linux versions, press Ctrl+Shift+M to insert the pipe operator. On a Mac, use Cmd+Shift+M.\nR also has its own pipe, |&gt;, which is an alternative to %&gt;%. I tend to use |&gt;. If you want to change the pipe inserted automatically with Ctrl+Shift+M, find on the menu Tools &gt; Global Options, then click on Code and check the box that says “Use Native Pipe Operator”.\nWe often pipe the dplyr functions, and the advantage is that we show the flow of data manipulation and subsequent graphing. This approach also helps to save memory, and dataframes are not unnecessarily created, a necessity for a big data framework.\nTry the following examples after loading the rangitikei dataset.\nselect()\n\n\nCode\nmy.data &lt;- read.csv(\"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", header=TRUE)\n\nnames(my.data)\n\n\n [1] \"id\"      \"loc\"     \"time\"    \"w.e\"     \"cl\"      \"wind\"    \"temp\"   \n [8] \"river\"   \"people\"  \"vehicle\"\n\n\n\n\nCode\nlibrary(tidyverse)\n\nnew.data &lt;- my.data |&gt; \n  select(people, vehicle)\n\nnames(new.data)\n\n\n[1] \"people\"  \"vehicle\"\n\n\n\n\nCode\nmy.data |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() + \n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nWe select two columns and create a scatter plot with the above commands.\nfilter()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() +\n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nThe above commands filter the data for the low wind days and plots vehicle against people.\narrange()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  arrange(w.e) |&gt; \n  select(w.e, people, vehicle)\n\n\n  w.e people vehicle\n1   1    136      23\n2   1     50      22\n3   1    100      31\n4   1    470     122\n5   2     22      11\n\n\nmutate()\nAssume that a $10 levy is collected for each vehicle. We can create this new levy column as follows.\n\n\nCode\nmy.data |&gt; \n  mutate(levy = vehicle*10) |&gt; \n  select(people, levy) |&gt; \n  ggplot() +\n  aes(x = people, y=levy) +\n  geom_point()\n\n\n\n\n\nNote that the pipe operation was used to create a scatter plot using the newly created column.\nsummarise()\n\n\nCode\nmy.data |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total      avg\n1    33 71.72727\n\n\nWe obtain the selected summary measures namely the total and the mean number of people. Try-\n\n\nCode\nmy.data |&gt; \n  filter(wind == 1) |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total   avg\n1     5 155.6\n\n\ngroup_by()\nWe obtain the wind group-wise summaries below:\n\n\nCode\nmy.data |&gt; \n  group_by(wind) |&gt; \n  summarise(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 3 × 3\n   wind total   avg\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5 156. \n2     2    26  59.7\n3     3     2  19  \n\n\nThere are many more commands such as the transmute function which conserves the only the needed columns. Try\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  transmute(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 33 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e total   avg\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     2     1    18  72.1\n 2     2     1    18  72.1\n 3     2     1    18  72.1\n 4     2     1    18  72.1\n 5     2     1    18  72.1\n 6     1     1     4 189  \n 7     2     2     8  31.8\n 8     2     1    18  72.1\n 9     3     2     1   4  \n10     2     1    18  72.1\n# ℹ 23 more rows\n\n\nA simple frequency table is found using count(). Try-\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp)\n\n\n# A tibble: 10 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e  temp     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1\n 2     1     1     3     3\n 3     1     2     3     1\n 4     2     1     1     4\n 5     2     1     2    12\n 6     2     1     3     2\n 7     2     2     2     6\n 8     2     2     3     2\n 9     3     1     2     1\n10     3     2     1     1\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp, river)\n\n\n# A tibble: 16 × 5\n# Groups:   wind, w.e [6]\n    wind   w.e  temp river     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1     1\n 2     1     1     3     3     3\n 3     1     2     3     3     1\n 4     2     1     1     1     1\n 5     2     1     1     2     1\n 6     2     1     1     3     2\n 7     2     1     2     1     3\n 8     2     1     2     2     2\n 9     2     1     2     3     7\n10     2     1     3     3     2\n11     2     2     2     1     2\n12     2     2     2     3     4\n13     2     2     3     2     1\n14     2     2     3     3     1\n15     3     1     2     2     1\n16     3     2     1     2     1\n\n\nThe count() is useful to check the balanced nature of the data when many subgroups are involved."
  },
  {
    "objectID": "workshops/ws01.html#tidyr",
    "href": "workshops/ws01.html#tidyr",
    "title": "Getting Started With R",
    "section": "tidyr",
    "text": "tidyr\nBy the phrase tidy data, it is meant the preferred way of arranging data that is easy to analyse. The principles of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nSimple Manipulations\nThere is always more than one-way of manipulating the data, producing summaries and tables from raw data.\nOne of the simplest manipulations on a batch of data we may do is to change the data type say numeric to character. For example, the television viewing time data in the text file tv.csv is read into a dataframe by the command line\n\n\nCode\nmy.data &lt;- read.csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/tv.csv\", \n  header =TRUE\n  )\n\n\nWe can improve the read.csv command to recognise the data type while reading the table as follows, using the read_csv command from the readr package:\n\n\nCode\nmy.data &lt;- read_csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/tv.csv\", \n  col_types = \"nfcc\"\n  )\n\n\nThe argument col_types = \"nfcc\" stands for {numeric, factor, character, character}, to match the order of the columns.\n\n\nCode\nmy.data\n\n\n# A tibble: 46 × 4\n   TELETIME SEX   SCHOOL STANDARD\n      &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1     1482 1     1      4       \n 2     2018 1     1      4       \n 3     1849 1     1      4       \n 4      857 1     1      4       \n 5     2027 2     1      4       \n 6     2368 2     1      4       \n 7     1783 2     1      4       \n 8     1769 2     1      4       \n 9     2534 1     1      3       \n10     2366 1     1      3       \n# ℹ 36 more rows\n\n\nWe often do a summary of a numerical variable for a given categorical variable. For example, we like to see obtain the summary statistics of TV viewing times for various schools. The commands\n\n\nCode\nattach(my.data)\nby(TELETIME, SCHOOL, summary)\n\n\nWe employed the by() command above and instead, we may also use tapply() aggregate() functions:\ntapply(TELETIME, SCHOOL, summary)\naggregate(TELETIME, list(SCHOOL), summary)\nA tabulated summary of categorical data is obtained using the table() command.\n\n\nCode\nmy.data &lt;- read.csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\",\n  header=TRUE\n  )\n\nwind &lt;- my.data |&gt; pull(wind)\nriver &lt;- my.data |&gt; pull(river)\n\ntable(wind, river)\n\n\nIt is sometimes convenient to work with matrices for some R functions such as apply(). For example, the number of admissions data in hospital.txt data can be formed as a matrix. Note that this is possible because we have the same number of observations for each hospital location.\n\n\nCode\ndata &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/hospital.txt\",\n  header=TRUE, \n  sep=\",\")\n\nM &lt;- data |&gt; \n  select(NORTH1, NORTH2, NORTH3, \n         SOUTH1, SOUTH2, SOUTH3) |&gt; \n  sqrt()\n\nmeans &lt;- apply(M, 1, mean)\nsds &lt;- apply(M, 1, sd)\n\nplot(means, sds)"
  },
  {
    "objectID": "workshops/ws01.html#data-quality-checks",
    "href": "workshops/ws01.html#data-quality-checks",
    "title": "Getting Started With R",
    "section": "Data Quality Checks",
    "text": "Data Quality Checks\nIt is a good idea to check the quality of secondary data sourced from elsewhere. For example, there could be missing values in the dataset. Consider the Telomeres data downloaded from http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\n\n\nCode\n# #| eval: false\n# url &lt;- \"http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\"\n# destfile &lt;- \"rsos192136_si_001.xlsx\"\n# # \n# download.file(url, destfile)\n\n\n\n\nCode\nurl &lt;- \"http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\"\ndestfile &lt;- \"rsos192136_si_001.xlsx\"\n\ncurl::curl_download(url, destfile)\n\n\n\n\nCode\nlibrary(readxl)\nrsos192136_si_001 &lt;- read_excel(\"rsos192136_si_001.xlsx\")\n\n\nThe missingness of data can be quickly explored using many R packages. The downloaded Telomeres dataset contain many missing values.\n\n\nCode\nlibrary(VIM)\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\nres &lt;- rsos192136_si_001 |&gt; \n  aggr(sortVar=TRUE) |&gt; \n  summary() |&gt; \n  pluck(\"combinations\")\n\n\n\n\n\nor\n\n\nCode\nlibrary(naniar)\ngg_miss_var(rsos192136_si_001) \n\n\nThe term Missing completely at random (MCAR) is often used to mean there is there is no pattern to the missing data themselves or alternatively the missingness is not related to any other variable or data in the dataset. In other words, the probability of missingness is the same for all units. So no bias is caused by the missing data, and we can discard cases with missing data when we fit models.\nIn practice, we often find missing data do have a relationship with other variables in the dataset but the actual missing values are random. This situation of data conditionally missing at random is called Missing at random (MAR) data. For a particular survey question, the response rate may differ depending on the respondent’s gender. In this situation, the actual missingness may be random but still related to the gender variable.\nMissing not at random (MNAR) is the pattern when missingness is related to other variables in the dataset, as well as the values of the missing data are not random. In other words, there is a predictable pattern in the missingness. So we cannot avoid the bias when missing cases are omitted.\nThere are also situations such as censoring where we just record a single value without actually measuring the variable of interest.\nImputation of data can be made except for the case of MCAR type. A number of R packages are available for data imputation; see https://cran.r-project.org/web/views/MissingData.html or https://stefvanbuuren.name/fimd/. We may occasionally cover data imputation issue in an assignment question.\nThere are also R packages to perform automatic investigation for data cleaning. Try-\n\n\nCode\nlibrary(dataMaid)\n\nmakeDataReport(rsos192136_si_001, output=\"html\", replace=TRUE)\n\n# or\nlibrary(DataExplorer)\n\ncreate_report(rsos192136_si_001)\n\n\nRule based validation is enabled in the R package validate. The R package janitor has a function get_dupes() to find duplicate entries in the dataset. Cleaner package will allow to clean the variables so that the columns are consistent in terms of the factor, date, numerical variable types. You will be largely using data that are already cleaned for your assignments but be aware that you have to perform data cleaning and perform necessary quality checks before analysis."
  },
  {
    "objectID": "workshops/ws02.html",
    "href": "workshops/ws02.html",
    "title": "Chapter 2 Workshop",
    "section": "",
    "text": "As you work through this workshop, you can copy the code and paste it into a code chunk. Write notes and observations to your self as you go.\nWe will be using a well-known dataset called Prestige from the car R package. This dataset deals with prestige ratings of Canadian occupations. The Prestige dataset has 102 rows and 6 columns. Each row (or ‘observation’) is an occupation.\nThis data frame contains the following columns:\n\neducation - Average education of occupational incumbents, years, in 1971.\nincome - Average income of incumbents, dollars, in 1971.\nwomen - Percentage of incumbents who are women.\nprestige - Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.\ncensus - Canadian Census occupational code.\ntype - Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar. (includes four missing values).\n\nFirst we’ll load the data. The dataset sits in the car package, so you need to load the car package first.\n\n\nCode\nlibrary(car)\ndata(Prestige)"
  },
  {
    "objectID": "studyguide/7-multiple.html",
    "href": "studyguide/7-multiple.html",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "",
    "text": "In this chapter, we consider multiple regression and other models in which there are more than one predictor (or \\(X\\)) variable. This extends what we covered in the last chapter where we examined one predictor to multiple predictors. Once again our focus is on finding the estimates of coefficients or parameters in multiple linear regression models by the method of least squares. For this we assume that\nWe will continue to use the data set horsehearts of the weights of horses’ hearts and other related measurements."
  },
  {
    "objectID": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "href": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Significance testing of Type I SS",
    "text": "Significance testing of Type I SS\nThe significance of the additional variation explained by a predictor can be tested using a \\(t\\) or \\(F\\) statistic. Consider the simple regression model of WEIGHT on EXTDIA. Suppose we decided to add the explanatory variable OUTERDIA to the model, i.e. regress WEIGHT on two explanatory variables EXTDIA and OUTERDIA. Is this new model a significant improvement on the existing one? For testing the null hypothesis that the true slope coefficient of OUTERDIA in this model is zero, the \\(t\\)-statistic is 1.531 (see output below).\n\n\nCode\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\ntwovar.model |&gt; tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.97     0.551      -3.57 0.000885\n2 EXTDIA         0.226    0.0614      3.68 0.000637\n3 OUTERDIA       0.522    0.341       1.53 0.133   \n\n\nThe \\(t\\) and \\(F\\) distributions are related by the equation \\(t^{2} =F\\) when the numerator df is just one for the \\(F\\) statistic. Hence 1.532 = 2.34 is the \\(F\\) value for testing the significance of the additional SSR due to OUTERDIA. In other words, the addition of OUTERDIA to the simple regression model does not result in a significant improvement in the sense that the reduction in residual SS (= 1.247) as measured by the \\(F\\) value of 2.34 is not significant (\\(p\\)-value being 0.133).\n\n\nCode\nonevar.model &lt;- lm(WEIGHT~ EXTDIA, data=horsehearts)\n\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\nanova(onevar.model, twovar.model)\n\n\nAnalysis of Variance Table\n\nModel 1: WEIGHT ~ EXTDIA\nModel 2: WEIGHT ~ EXTDIA + OUTERDIA\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     44 24.115                           \n2     43 22.867  1    1.2472 2.3453  0.133\n\n\nAlthough OUTERDIA is correlated with WEIGHT, it also has high correlation with EXTDIA. In other words, the correlation matrix gives us some indication of how many variables might be needed in a multiple regression model, although by itself it cannot tell us what combination of predictor variables is good or best.\n\n\n\nFigure 5: Issues with multiple predictors\n\n\n\n\n\nFigure 6: Effect of multiple predictors on model summaries\n\n\nFigure 5 and Figure 6 summarise the following facts:\n\nWhen there is only one explanatory variable, \\(R^2\\) = SSR/SST equals the square of the correlation coefficient between that variable and the dependent variable. Therefore if only one variable is to be chosen, it should have the highest correlation with the response variable, \\(Y\\).\nWhen variables are added to a model, the regression sum of squares SSR will increase and the residual or error sum of squares SSE will reduce. The opposite is true if variables are dropped from the model. This fact follows from Figure 6.\nThe other side of the coin to the above remark is that as additional variables are added, the Sums of Squares for residuals, SSE, will decrease towards zero as also shown in Figure 6(c).\nThe overlap of circles in suggests that these changes in both SSR and SST will lessen as more variables are added, see Figure 5(b).\nFollowing on from the last two notes, as \\(R^2\\) = SSR/SST, \\(R^2\\) will increase monotonically towards 1 as additional variables are added to the model. (monotonically increasing means that it never decreases although it could remain the same). This is indicated by Figure 6(a). If variables are dropped, then \\(R^2\\) will monotonically decrease.\nAgainst the above trends, the graph of residual mean square in Figure 6(b) reduces to a minimum but may eventually start to increase if enough variables are added. The residual sum of squares SSE decreases as variables are added to the model (see Figure 5(b)). However, the associated df values also decrease so that the residual standard deviation decreases at first and then starts to increase as shown in Figure 6(b). (Note that the residual standard error \\(s_{e}\\) is the square root of the residual mean square\n\\[s_{e}^{2} =\\frac{{\\text {SSE}}}{{\\text {error degrees of freedom}}},\\]\ndenoted as MSE in Figure 5(b)). After a number of variables have been entered, the additional amount of variation explained by them slows down but the degrees of freedom continues to change by 1 for every variable added, resulting in the eventual increase in residual mean square. Note that the graphs in Figure 5 are idealised ones. For some data sets, the behaviour of residual mean square may not be monotone.\nNotice that the above trends will occur even if the variables added are garbage. For example, you could generate a column of random data or a column of birthdays of your friends, and this would improve the \\(R^2\\) but not the adjusted \\(R^2\\). The adjusted \\(R^2\\) makes adjustment for the degrees of freedom for the SSR and SSE, and hence reliable when compared to the unadjusted or multiple \\(R^2\\). The residual mean square error also partly adjusts for the drop in the degrees of freedom for the SSE and hence becomes an important measure. The addition of unimportant variables will not improve the adjusted \\(R^2\\) and the mean square error \\(s_{e}^{2}\\)."
  },
  {
    "objectID": "studyguide/7-multiple.html#other-ss-types",
    "href": "studyguide/7-multiple.html#other-ss-types",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Other SS types",
    "text": "Other SS types\nThe R anova function anova() calculates sequential or Type-I SS values.\nType-II sums of squares is based on the principle of marginality. Type II SS correspond to the R convention in which each variable effect is adjusted for all other appropriate effects.\nType-III sums of squares is the SS added to the regression SS after ALL other predictors including an intercept term. This SS however creates theoretical issues such as violation of marginality principle and we should avoid using this SS type for hypothesis tests.\nThe R package car has the function Anova() to compute the Type II and III sums of squares. Try-\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova(full.model)\n\nlibrary(car)\n\nAnova(full.model, type=2)\nAnova(full.model, type=3)\n\n\nFor the horsehearts data, a comparison of the Type I and II sums squares is given below:\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova1 &lt;- full.model |&gt; \n  anova() |&gt; \n  tidy() |&gt; \n  select(term, \"Type I SS\" = sumsq)  \n\nanova2 &lt;- full.model |&gt; \n  Anova(type=2) |&gt; \n  tidy() |&gt; \n  select(term, \"Type II SS\" = sumsq) \n\ntype1and2 &lt;- full_join(anova1, anova2, by=\"term\")\n\ntype1and2\n\n\n\n\n\n\n\nterm\nType I SS\nType II SS\n\n\n\n\nINNERSYS\n34.40\n0.20\n\n\nINNERDIA\n3.53\n0.62\n\n\nOUTERSYS\n2.76\n1.69\n\n\nOUTERDIA\n0.13\n0.55\n\n\nEXTSYS\n0.06\n1.79\n\n\nEXTDIA\n1.90\n1.90\n\n\nResiduals\n14.07\n14.07\n\n\n\n\n\nWhen predictor variables are correlated, it is difficult to assess their absolute importance and the importance of a variable can be assessed only relatively. This is not an issue with the most highly correlated predictor in general."
  },
  {
    "objectID": "studyguide/7-multiple.html#best-subsets-selection",
    "href": "studyguide/7-multiple.html#best-subsets-selection",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Best Subsets Selection",
    "text": "Best Subsets Selection\nAn exhaustive screening of all possible regression models (and hence the name best subsets regression) can also be done using software. For example, there are 6 predictor variables in the horses’ hearts data. If we fix the number of predictors as 3, then \\(\\small {\\left(\\begin{array}{c} {6} \\\\ {3} \\end{array}\\right)} = 20\\) regression models are possible. One may select the ‘best’ 3-variable model based on criteria such as AIC, \\(C_{p}\\), \\(R_{adj}^{2}\\) etc. Software must be employed to perform the conventional stepwise regression procedures. Software algorithms give one or more best candidate models fixing the number of variables in each step.\nOn the basis of our analysis on the horses’ hear data, we might decide to recommend the model with predictor variables EXTDIA, EXTSYS, INNERDIA and OUTERSYS. In particular if the model is to be used for describing relationships then we would tend to include more variables. For prediction purposes, however, a simpler feasible model is preferred and in this case we may opt for the smaller model with only INNERDIA and OUTERSYS. See Table 4 produced using the following R codes:\n\n\nCode\nlibrary(leaps)\nlibrary(HH)\nlibrary(kableExtra)\n\nb.model &lt;- regsubsets(WEIGHT ~ ., data = horsehearts) |&gt;\n  summaryHH() \n\nb.model |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling(bootstrap_options = \"basic\", full_width = F)\n\n\n\n\nTable 4: Subset selection\n\n\nmodel\np\nrsq\nrss\nadjr2\ncp\nbic\nstderr\n\n\n\n\nINNERD\n2\n0.658\n19.450\n0.650\n11.923\n-41.677\n0.665\n\n\nINNERD-OUTERS\n3\n0.715\n16.173\n0.702\n4.838\n-46.335\n0.613\n\n\nINNERD-OUTERS-OUTERD\n4\n0.718\n16.043\n0.698\n6.477\n-42.878\n0.618\n\n\nINNERD-OUTERS-EXTS-EXTD\n5\n0.741\n14.739\n0.715\n4.862\n-42.949\n0.600\n\n\nINNERD-OUTERS-OUTERD-EXTS-EXTD\n6\n0.749\n14.272\n0.718\n5.566\n-40.602\n0.597\n\n\nINNERS-INNERD-OUTERS-OUTERD-EXTS-EXTD\n7\n0.753\n14.067\n0.714\n7.000\n-37.437\n0.601\n\n\n\n\n\n\n\n\nSometimes theory may indicate that a certain explanatory variable should be included in the model (e.g. due to small sample size). If this variable is found to make an insignificant contribution to the model, then one should exclude the variable when the model is to be used for prediction but if the model is to be used for explanation purposes only then the variable should be included. Other considerations such as cost and time may also be taken into account. For every method or algorithm, one could find peculiar data sets where it fouls up. The moral – be alert and don’t automatically accept models thrown up by a program. Note there is never one right answer as different methods and different criteria lead to different models.\nVariable selection procedures can be a valuable tool in data analysis, particularly in the early stages of building a model. At the same time, they present certain dangers. There are several reasons for this:\n\nThese procedures automatically snoop though many models and may select ones which, by chance, happen to fit well.\nThese forward or backward stepwise procedures are heuristic (i.e., shortcut) algorithms, which often work very well but which may not always select the best model for a given number of predictors (here best may refer to adjusted \\(R^2\\)-values, or AIC or some other criterion).\nAutomatic procedures cannot take into account special knowledge the analyst may have about the data. Therefore, the model selected may not be the best (or make sense) from a practical point of view.\nMethods are available that shrink coefficients towards zero. The least squares approach minimises the residual sums of squares or RSS without placing any constraint on the coefficients. The shrinkage methods, which place a constraint on the coefficients, work well when there are large numbers of predictors. A ridge regression shrinks the coefficients towards zero but in relation each other. On the other hand, (Least Absolute Selection and Shrinkage Operator) lasso regression shrinks some of coefficients to zero which means these predictors can be dropped. Note that the ridge regression does not completely remove predictors. By shrinking large coefficients, we obtain a model with higher bias but lower variance. This process is known as regularisation in the literature (not covered in this course)."
  },
  {
    "objectID": "workshops/ws01_5.html",
    "href": "workshops/ws01_5.html",
    "title": "Tidy data",
    "section": "",
    "text": "We will be largely using the tidyverse suite of packages for data organisation, summarizing, and plotting; see https://www.tidyverse.org/.\nLet’s load that package now: Remember if you have not installed it you will need to use install.packages() first.\n\n\nCode\nlibrary(tidyverse)"
  },
  {
    "objectID": "slides/Chapter00.html",
    "href": "slides/Chapter00.html",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "",
    "text": "A data analysis session in R/RStudio involves loading the data, graphing, summarizing, and modelling. You finally save your outputs or produce a Report.\nWhen you begin your analysis in RStudio, start it as a new project in the File menu. You can save all your work in one go when you quit the RStudio software. You can always load your project later on to continue the analysis."
  },
  {
    "objectID": "slides/Chapter00.html#the-nature-of-data-random-variables",
    "href": "slides/Chapter00.html#the-nature-of-data-random-variables",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "",
    "text": "Data come from recording observations of the world. E.g.,\n\nrecording the number of heart beats per minute\ncounting birds in your yard\nmeasuring the lengths of fish you catch\n\nEach time you collect one of these observations, it is likely to be different.\n\nPulse may vary between 55 and 70 bpm, depending on when you record it\nNumber of birds varies at different times, and across different yards.\n\nWe call these random variables (often denoted with an upper case letter, \\(X\\)), because the particular value that a single observation or measurement will take is uncertain. The value varies across observations."
  },
  {
    "objectID": "slides/Chapter00.html#types-of-data",
    "href": "slides/Chapter00.html#types-of-data",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Types of data",
    "text": "Types of data"
  },
  {
    "objectID": "slides/Chapter00.html#subtypes-of-qualitative-data",
    "href": "slides/Chapter00.html#subtypes-of-qualitative-data",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Subtypes of qualitative data",
    "text": "Subtypes of qualitative data\n\n\n\n\nNominal variables have no particular order (e.g., gender, colour, species, country)\n\n\n\nOrdinal variables can be ordered (e.g., altitude = {low, mid, high}, age group = {child, juvenile, adult} )"
  },
  {
    "objectID": "slides/Chapter00.html#subtypes-of-quantitative-data",
    "href": "slides/Chapter00.html#subtypes-of-quantitative-data",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Subtypes of quantitative data",
    "text": "Subtypes of quantitative data\n\n\n\n\nContinuous variables have no gaps between possible values, as in measurements (e.g., weight, temperature, length)\n\n\n\nDiscrete variables have gaps between possible values, as in counts (e.g., number of siblings, number of flowers)"
  },
  {
    "objectID": "slides/Chapter00.html#subtypes-of-continuous-data",
    "href": "slides/Chapter00.html#subtypes-of-continuous-data",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Subtypes of continuous data",
    "text": "Subtypes of continuous data\n\n\nInterval scale\n\nNo absolute zero\nDivision & subtraction may not be meaningful\nTemperature in degrees Celcius is interval because 20°C is not twice as hot as 10°C.\n\n\n\n\nRatio scale\n\nZero = zero\nAll arithmetic manipulation can be done\nLength is ratio because 20 mm is twice as long as 10 mm."
  },
  {
    "objectID": "slides/Chapter00.html#data-collection-survey-experiment-census",
    "href": "slides/Chapter00.html#data-collection-survey-experiment-census",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Data Collection: Survey, Experiment, Census",
    "text": "Data Collection: Survey, Experiment, Census\n\n\n\nWe collect data from the world to get information about patterns and processes.\nMost datasets contain a subset, a sample, of a much bigger population of interest.\n\nWe may conduct a survey to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.\nWe might conduct an experiment where we take a sample of elements (people, organisms, objects) and apply some treatment in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.\n\nIf we are not dealing with a sample, if every element of the population of interest is represented in the dataset, we call this a census rather than a sample."
  },
  {
    "objectID": "slides/Chapter00.html#measurement-issues",
    "href": "slides/Chapter00.html#measurement-issues",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Measurement issues",
    "text": "Measurement issues\n\nMeasuring Devices or Instruments\n\na physical device - measuring rule to gauge the heights of plants\na counting device - a Geiger- counter for measuring radioactive material\na questionnaire - requires a more subjective response.\n\nMeasurement Error\n\nmeasuring instrument may be faulty (bias)\nvalues recorded from the same object may vary from one measurement to another (variance)\n\nIndirect measures\n\nFor example, we use Body Mass Index (BMI) as a measure of condition, and we measure temperature with the expansion of mercury."
  },
  {
    "objectID": "slides/Chapter00.html#non-response",
    "href": "slides/Chapter00.html#non-response",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Non-response",
    "text": "Non-response\n\na non-sampling error\nSelection stage: an element may be selected but not found\n\ne.g. sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey.\n\nCollection stage: it may not be possible to take a measurement\n\nsome respondents may forget, or refuse, to answer the questionnaire\n\nDocumentation stage\n\nIncorrect record of measurement\n\nCall-backs reduce non-response"
  },
  {
    "objectID": "slides/Chapter00.html#census-related-concepts",
    "href": "slides/Chapter00.html#census-related-concepts",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Census related concepts",
    "text": "Census related concepts\nTARGET POPULATION the population under study\nFRAME operationalises data collection from a target population. e.g. listing of elements in population.\nACTUAL POPULATION is the resulting set of elements on which usable data have been collected."
  },
  {
    "objectID": "slides/Chapter00.html#sample-vs-population",
    "href": "slides/Chapter00.html#sample-vs-population",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\n\nA sample is a subset of the population.\nDatasets usually only contain a sample from the population; rarely do we have the entire population of data!\nWhy sample?\n\nSampling conserves resources (money, time, etc.).\nA well collected sample is more useful than a badly designed census.\nCollecting data may be destructive.\nThe disadvantage: the statistics we calculate from sample data is subject to sampling variation, which introduces uncertainty* about their true values.\n\n\n\n\n\n“You don’t have to eat the whole ox to know that the meat is tough” – Samuel Johnson (1709-1784)"
  },
  {
    "objectID": "slides/Chapter00.html#population-frame-and-sample",
    "href": "slides/Chapter00.html#population-frame-and-sample",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Population, frame, and sample",
    "text": "Population, frame, and sample"
  },
  {
    "objectID": "slides/Chapter00.html#statistical-inference",
    "href": "slides/Chapter00.html#statistical-inference",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the process of using information from sample data to make conclusions about the population.\n\nFor example, we want to know \\(\\mu\\), mean length of fish in a population. So, we collect a sample of fish, measure their lengths, calculate the mean \\(\\bar{x}\\), and use \\(\\bar{x}\\) as an estimate of \\(\\mu\\). This is statistical inference.\n\n\n\n\nThe sample mean \\(\\bar{x}\\) depends on which particular fish we happened to get in our sample.\nTherefore, the sample mean \\(\\bar{x}\\) itself is a random variable.\nIf we were to take 1000 different samples, we’d get 1000 different means."
  },
  {
    "objectID": "slides/Chapter00.html#bias-vs-sampling-variance",
    "href": "slides/Chapter00.html#bias-vs-sampling-variance",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Bias vs sampling variance",
    "text": "Bias vs sampling variance\n\n\nA method used to estimate \\(\\hat{\\theta}\\) a population parameter \\(\\theta\\) is called an estimator. An estimator includes the study design, methods of data collection, and mathematical operations.\nSampling variance is the sample-to-sample variation in an estimator.\nBias is when our estimator doesn’t get it right on average. That is, the average of estimates over \\(\\infty\\) samples is not centred on the population parameter; \\(\\text{Mean}(\\hat{\\theta}) \\neq \\theta\\).\n\n\n\n\nAn estimator can have high/low sampling variance and high/low bias."
  },
  {
    "objectID": "slides/Chapter00.html#principle-of-randomisation",
    "href": "slides/Chapter00.html#principle-of-randomisation",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Principle of randomisation",
    "text": "Principle of randomisation\n\n\n\nWe want our sample to be representative of (and have similar properties to) the population. The most straightforward way to do this is through randomisation.\nWe randomise the selection of objects for our sample to avoid bias. If we (consciously or subconsciously) tended to chose the largest fish for our sample, we’d get an upwardly biased estimate of the lengths.\nSimple random sampling or EPSEM (equal probability of selection) is the gold standard of random sampling."
  },
  {
    "objectID": "slides/Chapter00.html#simple-random-sampling-srs",
    "href": "slides/Chapter00.html#simple-random-sampling-srs",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Simple Random Sampling (SRS)",
    "text": "Simple Random Sampling (SRS)\n\n\n\nRandom selection of elements\n\n“Random” refers to the process not outcome\nEach (sampling) unit has same chance of being selected\nUnits can be selected with & without replacement\n\n\n\n\n\n\n\nSRS is easy to handle; suits even for a poor sampling frame\nSRS can be costly to implement\nSRS estimates are more variable than some alternatives\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter00.html#stratified-random-sampling-strs",
    "href": "slides/Chapter00.html#stratified-random-sampling-strs",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Stratified Random Sampling (STRS)",
    "text": "Stratified Random Sampling (STRS)\n\n\n\nSuitable for heterogeneous populations\nPopulation is divided into relatively homogeneous groups called strata and a random sample is taken from each stratum.\n\n\n\n\n\n\nSampling Approaches\n\nSample the larger strata more heavily (suits when all the strata are equally variable)\nSample the more varied strata are sampled\n\nAdvantages of STRS\n\nleads to efficient estimation That is, the variance (of an estimate) is usually less than that of SRS\nsample is spread throughout population\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter00.html#cluster-sampling",
    "href": "slides/Chapter00.html#cluster-sampling",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\n\n\nA convenient method of sampling\npopulation is composed of clusters (groups)\nSelect certain clusters (randomly) and collect measurements from a random selection of the elements within the chosen clusters\nLarger variance than SRS!\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter00.html#systematic-random-sampling-syrs",
    "href": "slides/Chapter00.html#systematic-random-sampling-syrs",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Systematic Random sampling (SyRS)",
    "text": "Systematic Random sampling (SyRS)\n\n\n\nSelect every \\(k^{th}\\) element!\nRandom start within the first block of elements.\n\nConvenient and also the sample will be representative of population\nVariance of estimates - generally greater than those of SRS\nInefficient/inappropriate, if cycle or trend is present\n\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter00.html#other-sampling-methods",
    "href": "slides/Chapter00.html#other-sampling-methods",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Other Sampling methods",
    "text": "Other Sampling methods\n\nProbability proportional to size (PPS)\n\ne.g., sampling high-value companies more likely than low-value companies\n\nMultistage\n\ne.g., first stage - cluster; second stage - SRS\n\nNon-probability sampling methods\n\nHaphazard / opportunistic / volunteer; take what you can get!\nSnowball; get your participants to find new participants\nPurposive; select items with certain characteristics; e.g., patients with particular symptoms\n\nNon-probability samples are often treated as random, requiring the assumption that the sample is representative. The validity of this assumption should be carefully considered."
  },
  {
    "objectID": "slides/Chapter00.html#some-sampling-methods",
    "href": "slides/Chapter00.html#some-sampling-methods",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Some sampling methods",
    "text": "Some sampling methods"
  },
  {
    "objectID": "slides/Chapter00.html#effective-sample-size-thumb-rule",
    "href": "slides/Chapter00.html#effective-sample-size-thumb-rule",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Effective Sample size (thumb rule)",
    "text": "Effective Sample size (thumb rule)\n\n\n\n\n\n\n\n\nSample Design\nDesign Effect (\\(d\\))\nEffective Sample Size (\\(\\frac{n}{d}\\))\n\n\n\n\nSRS\n1.00\n\\(n\\)\n\n\nSTRS\n0.80 to 0.90\n\\(\\frac{n}{0.9}\\) to \\(\\frac{n}{0.8}\\)\n\n\nCluster\n1.02 to 1.26\n\\(\\frac{n}{1.26}\\) to \\(\\frac{n}{1.02}\\)\n\n\nSyRS\n1.05\n\\(\\frac{n}{1.05}\\)\n\n\nQuota\n2\n\\(\\frac{n}{2}\\)"
  },
  {
    "objectID": "slides/Chapter00.html#summary",
    "href": "slides/Chapter00.html#summary",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Summary",
    "text": "Summary\n\nIssues to address\n\nWHAT are collected?\nWHO does the data collection?\nHOW are the data collected?\n\nBias occurs due to\n\nSELECTION\nCOLLECTION\nNON-RESPONSE (the single largest cause of bias!)\n\nA sample may have the same biases as a census along with sampling errors"
  },
  {
    "objectID": "studyguide/0-R-tidyverse.html",
    "href": "studyguide/0-R-tidyverse.html",
    "title": "Chapter 0 R quarto and Tidy data",
    "section": "",
    "text": "In this course, we will be using R https://www.r-project.org/, an open-source (i.e., free) software package for data analysis. This software is available for essentially all computing platforms (e.g. Windows, Linux, Unix, Mac) is maintained and developed by a huge community of users including many of the world’s foremost statisticians.\nR is a programming language but you may not be required to do a lot of programming for your course work. R includes functions which enables us to perform a full range of statistical analyses.\nFor installing R software, please visit https://cran.stat.auckland.ac.nz/ and follow the instructions.\nNote that the R software will be sitting in the background in RStudio and you will not be using the standalone version of R in this course.\nRStudio https://www.rstudio.com/products/rstudio/ is an integrated development environment (IDE) for R. It includes a console and a sophisticated code editor. It also contains tools for plotting, history, debugging, and management of workspaces and projects. RStudio has many other features such as authoring HTML, PDF, Word Documents, and slide shows. In order to download RStudio (Desktop edition, open source), go to\nhttps://www.rstudio.com/products/rstudio/download/\nDownload the installation file and run it. Note that RStudio must be installed after installing R.\nR/RStudio can also be run on a cloud platform at https://rstudio.cloud/ after creating a free account. Be aware though that some of the packages covered in this course may not work in the cloud platform and there is a maximum amount of computing time. We do not recommend relying on the cloud platform for this course.\nIf you open RStudio, you will see something similar to the screen shot shown in Figure 1:\n\n\n\nFigure 1: An RStudio window\n\n\nRStudio has many options, such as uploading files to a server, creating documents, etc. You will be using only a few of the options. You will not be using the menus such as Build, Debug, Profile at all in this course.\nYou can either type or copy and paste the R codes appearing in this section on to the R Script window and run them."
  },
  {
    "objectID": "studyguide/0-R-tidyverse.html#to-use-with-summarize",
    "href": "studyguide/0-R-tidyverse.html#to-use-with-summarize",
    "title": "Chapter 0 R quarto and Tidy data",
    "section": "To Use with summarize()",
    "text": "To Use with summarize()\nsummarize() applies summary functions to columns to create a new table. Summary functions take vectors as input and return single values as output.\nCount - dplyr::n(): number of values/rows - dplyr::n_distinct(): # of uniques - sum(!is.na()): # of non-NAs\nPosition - mean(): mean, also mean(!is.na()) - median(): median\nLogical - mean(): proportion of TRUEs - sum(): # of TRUEs\nOrder - dplyr::first(): first value - dplyr::last(): last value - dplyr::nth(): value in the nth location of vector\nRank - quantile(): nth quantile - min(): minimum value - max(): maximum value\nSpread - IQR(): Inter-Quartile Range - mad(): median absolute deviation - sd(): standard deviation - var(): variance"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "161250 Slides",
    "section": "",
    "text": "Chapter 0: Introduction: R, quarto, tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1:Data Collection\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2:Exploratory Data Analysis (EDA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3:Probability Concepts & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4:Introduction to Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5:Tabulated Counts\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6:Models with a Single Predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7:Models with Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/Chapter00.html#what-is-data-analysis",
    "href": "slides/Chapter00.html#what-is-data-analysis",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "What is data analysis?",
    "text": "What is data analysis?\nData analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. (wikipedia)\nA data analysis session involves loading the data, graphing, summarizing, and modelling."
  },
  {
    "objectID": "slides/Chapter00.html#example",
    "href": "slides/Chapter00.html#example",
    "title": "Chapter 0: R, quarto, tidyverse",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/Chapter00.html#getting-started-with-r",
    "href": "slides/Chapter00.html#getting-started-with-r",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Getting started with R",
    "text": "Getting started with R\nWe will be using the programming language R for our data analysis\nR is the language\nR studio is the graphical user interface (GUI)\n\nIf you have not installed R and R studio on your computer please see the resources page on Stream about how to do that."
  },
  {
    "objectID": "slides/Chapter00.html#some-r-basics",
    "href": "slides/Chapter00.html#some-r-basics",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Some R basics",
    "text": "Some R basics\n\nR is case sensitive, so data is not the same as DATA\nSimilarly beware of spaces and punctuation, particularly in column header\n&lt;- (read as “gets”) is the assignment operator. That is, you use &lt;- to assign some content to a variable. The operator = has a slightly different meaning but it can be used in the same way as &lt;-. In R Studio, press ALT and minus key when you are in the R script mode (File &gt;&gt; New File &gt;&gt; R Script).\nComments are denoted by the # symbol. Anything after a # symbol is ignored by R . This is a great way to write yourself notes.\nR coding can be hard to write from scratch. So do not hesitate to adopt R codes written by others. Search the internet for R code to do what you want to do. The usual copy and paste trick works!\nWarnings vs error messages: R gives warnings typically in &lt;span style='color: orange;'&gt;orange&lt;/span&gt; and errors in &lt;span style='color: red;'&gt;red&lt;/span&gt;. A warning means your code likely ran but there is something you should be aware of while an error means your code did not run. We recommend first trying to understand what the error is saying, check for common mistakes (missing failing to close a bracket or quotation, misspelling, capitalization, etc. ), then try to google your error message.\nYou can take notes on your code using the # symbol, called a comment R interprets everything after this symbol as notes not to be run."
  },
  {
    "objectID": "slides/Chapter00.html#r-as-a-calulator",
    "href": "slides/Chapter00.html#r-as-a-calulator",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "R as a calulator",
    "text": "R as a calulator\nAt the most basic level R is a calculator. We can type mathematical equations directly into R.\n\n\n[1] 4\n\n\n[1] 4"
  },
  {
    "objectID": "slides/Chapter00.html#directories",
    "href": "slides/Chapter00.html#directories",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Directories",
    "text": "Directories\nDirectories is a location for storing files on your computer. Information (documents) is stored in folders the location of that folder is called a directory. You can think of this as the address of the file.\n\n For example on my computer my slides for this class are located in a folder called ‘slides’ inside ‘Data_Analysis_course’. The last part of the directory is the most specific folder which is nested inside parent folders."
  },
  {
    "objectID": "slides/Chapter00.html#set-directory-in-r",
    "href": "slides/Chapter00.html#set-directory-in-r",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Set directory in R",
    "text": "Set directory in R\nIn RStudio, set the working directory under the Session menu. It is a good idea to start your analysis as a new project in the File menu so that the entire work and data files can be saved and re-opened easily later on.\n\n\n[1] \"/Users/mmarraff/Documents/Classes/161.250/Data_Analysis_Course/slides\""
  },
  {
    "objectID": "slides/Chapter00.html#quarto",
    "href": "slides/Chapter00.html#quarto",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Quarto",
    "text": "Quarto\nThis course will be using Quarto *.qmd files rather than raw *.R files. Heard of Rmarkdown? Well, Quarto is the successor to Rmarkdown. So, if you’re just starting to use R, then you should begin with Quarto rather than Rmarkdown, because most/all new development will be going into Quarto.\nQuarto files contain text and code, and can be ‘Rendered’ to produce a nicely formatted document, usually in HTML or PDF format, containing sections, text, code, plots, and output. Quarto can also be used to make websites; in fact, the website for this course was made using Quarto.\nHere’s some information to get you started: https://quarto.org/docs/get-started/hello/rstudio.html.\nAnd some other useful tips: https://r4ds.hadley.nz/quarto."
  },
  {
    "objectID": "slides/Chapter00.html#loading-data",
    "href": "slides/Chapter00.html#loading-data",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Loading data",
    "text": "Loading data\nMost data sets we shall consider in this course are in a tabular form. This means that each variable is a column, each row is an observation, columns are separated by white space (or comma), and each column or row may have a name.\nIf the data file is stored locally, you should put the data into the same directory as your Quarto or R markdown script. That way, you can (usually) load it easily without having to type the full pathway (e.g., mydata.csv rather than C:/Users/anhsmith/Work/Project1/data/mydata.csv). Better yet, Projects make this much easier.\nYou can also load data from the web using a URL. For example,\nWe usually store the data in an R object, here named rangitikei."
  },
  {
    "objectID": "slides/Chapter00.html#loading-packages",
    "href": "slides/Chapter00.html#loading-packages",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Loading packages",
    "text": "Loading packages\nMany (but not all) R packages are hosted at a repository called CRAN (Comprehensive R Archive Network). The package install option within RStudio can download and install these optional packages under the menu Packages &gt;&gt; Install. You can also do this using the command install.packages.\nIn this course we will use multiple packages. You need to load them with the library() function every time you run a script/R session but you only need to install them once."
  },
  {
    "objectID": "slides/Chapter00.html#welcome-to-the-tidyverse",
    "href": "slides/Chapter00.html#welcome-to-the-tidyverse",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Welcome to the tidyverse",
    "text": "Welcome to the tidyverse\nWe will be largely using the tidyverse suite of packages for data organisation, summarising, and plotting; see https://www.tidyverse.org/.\nLet’s load that package now:\nRecommended reading to accompany this workshop is pages 1-11 of R for Data Science https://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides/Chapter00.html#piping",
    "href": "slides/Chapter00.html#piping",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Piping",
    "text": "Piping\n\n\n\n\n\n\nTip\n\n\nThe piping operation is a fundamental aspect of computer programming. The semantics of pipes is taking the output from the left-hand side and passing it as input to the right-hand side.\n\n\n\nThe R package magrittr introduced the pipe operator %&gt;% and can be pronounced as “then”. In RStudio windows/Linux versions, press Ctrl+Shift+M to insert the pipe operator. On a Mac, use Cmd+Shift+M.\nR also has its own pipe, |&gt;, which is an alternative to %&gt;%. I tend to use |&gt;. If you want to change the pipe inserted automatically with Ctrl+Shift+M, find on the menu Tools &gt; Global Options, then click on Code and check the box that says “Use Native Pipe Operator”.\nWe often pipe the dplyr functions, and the advantage is that we show the flow of data manipulation and subsequent graphing. This approach also helps to save memory, and dataframes are not unnecessarily created, a necessity for a big data framework."
  },
  {
    "objectID": "workshops/ws01_5.html#piping",
    "href": "workshops/ws01_5.html#piping",
    "title": "Tidy data",
    "section": "Piping",
    "text": "Piping\n\n\n\n\n\n\nTip\n\n\n\nThe piping operation is a fundamental aspect of computer programming. The semantics of pipes is taking the output from the left-hand side and passing it as input to the right-hand side.\n\n\nThe R package magrittr introduced the pipe operator %&gt;% and can be pronounced as “then”. In RStudio windows/Linux versions, press Ctrl+Shift+M to insert the pipe operator. On a Mac, use Cmd+Shift+M.\nR also has its own pipe, |&gt;, which is an alternative to %&gt;%. You will see both used in this course. If you want to change the pipe inserted automatically with Ctrl+Shift+M, find on the menu Tools &gt; Global Options, then click on Code and check the box that says “Use Native Pipe Operator”.\nConsider the study guide dataset rangitikei.txt (Recreational Use of the Rangitikei river). The first 10 rows of this dataset are shown below:\n\n\n   id loc time w.e cl wind temp river people vehicle\n1   1   1    2   1  1    2    2     1     37      15\n2   2   1    1   1  1    2    1     2     23       6\n3   3   1    2   1  1    2    2     3     87      31\n4   4   2    2   1  1    2    1     1     86      27\n5   5   2    1   1  1    2    2     2     19       2\n6   6   2    2   1  2    1    3     3    136      23\n7   7   1    2   2  2    2    2     3     14       8\n8   8   1    2   1  2    2    2     3     67      26\n9   9   1    1   2  1    3    1     2      4       3\n10 10   2    2   1  2    2    2     3    127      45\n\n\nTry the following examples after loading the rangitikei dataset.\nselect()\n\n\nCode\nlibrary(tidyverse)\n\nnew.data &lt;- my.data |&gt; \n  select(people, vehicle)\n\nnames(new.data)\n\n\n[1] \"people\"  \"vehicle\"\n\n\nWhat does select() do?\n\n\nCode\nmy.data |&gt; \n  select(people, vehicle) |&gt; # select columns\n  ggplot() + # make a plot using those columns\n  aes(x=people, y=vehicle) + \n  geom_point()\n\n\n\n\n\nWe select two columns and create a scatter plot with the above commands.\nNow try another function:\nfilter()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() +\n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nWhat does filter() do?\nThe above commands filter the data for the low wind days and plots vehicle against people. filter() subsets the data for all observations matching a specified criteria.\narrange()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  arrange(w.e) |&gt; \n  select(w.e, people, vehicle)\n\n\n  w.e people vehicle\n1   1    136      23\n2   1     50      22\n3   1    100      31\n4   1    470     122\n5   2     22      11\n\n\nmutate()\nAssume that a $10 levy is collected for each vehicle. We can create this new levy column as follows.\n\n\nCode\nmy.data |&gt; \n  mutate(levy = vehicle*10) |&gt; \n  select(people, levy) |&gt; \n  ggplot() +\n  aes(x = people, y=levy) +\n  geom_point()\n\n\n\n\n\nNote that the pipe operation was used to create a scatter plot using the newly created column.\nsummarise()\n\n\nCode\nmy.data |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total      avg\n1    33 71.72727\n\n\nWe obtain the selected summary measures namely the total and the mean number of people. Try-\n\n\nCode\nmy.data |&gt; \n  filter(wind == 1) |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total   avg\n1     5 155.6\n\n\ngroup_by()\nWe obtain the wind group-wise summaries below:\n\n\nCode\nmy.data |&gt; \n  group_by(wind) |&gt; \n  summarise(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 3 × 3\n   wind total   avg\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5 156. \n2     2    26  59.7\n3     3     2  19  \n\n\nThere are many more commands such as the transmute function which conserves the only the needed columns. Try\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  transmute(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 33 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e total   avg\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     2     1    18  72.1\n 2     2     1    18  72.1\n 3     2     1    18  72.1\n 4     2     1    18  72.1\n 5     2     1    18  72.1\n 6     1     1     4 189  \n 7     2     2     8  31.8\n 8     2     1    18  72.1\n 9     3     2     1   4  \n10     2     1    18  72.1\n# ℹ 23 more rows\n\n\nA simple frequency table is found using count(). Try-\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp)\n\n\n# A tibble: 10 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e  temp     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1\n 2     1     1     3     3\n 3     1     2     3     1\n 4     2     1     1     4\n 5     2     1     2    12\n 6     2     1     3     2\n 7     2     2     2     6\n 8     2     2     3     2\n 9     3     1     2     1\n10     3     2     1     1\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp, river)\n\n\n# A tibble: 16 × 5\n# Groups:   wind, w.e [6]\n    wind   w.e  temp river     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1     1\n 2     1     1     3     3     3\n 3     1     2     3     3     1\n 4     2     1     1     1     1\n 5     2     1     1     2     1\n 6     2     1     1     3     2\n 7     2     1     2     1     3\n 8     2     1     2     2     2\n 9     2     1     2     3     7\n10     2     1     3     3     2\n11     2     2     2     1     2\n12     2     2     2     3     4\n13     2     2     3     2     1\n14     2     2     3     3     1\n15     3     1     2     2     1\n16     3     2     1     2     1\n\n\nThe count() is useful to check the balanced nature of the data when many subgroups are involved.\nNow let’s practice using these functions."
  },
  {
    "objectID": "workshops/ws01_5.html#combining-datasets",
    "href": "workshops/ws01_5.html#combining-datasets",
    "title": "Tidy data",
    "section": "Combining datasets",
    "text": "Combining datasets\nTwo tables can be connected through a pair of keys, within each table.\nEvery join involves a pair of keys: a primary key and a foreign key. A primary key is a variable or set of variables that uniquely identifies each observation. When more than one variable is needed, the key is called a compound key.\nThere are four types of joins, we will illustrate them using a simple example:\nFirst lets make some data:\n\n\nCode\ndf1 &lt;- tibble(x = c(1, 2), y = 2:1)\ndf2 &lt;- tibble(x = c(3, 1), a = 10, b = \"a\")\n\n\nNow join the two datasets using the different join functions:\n\n\nCode\ndf1 %&gt;% inner_join(df2) \n\n\nJoining with `by = join_by(x)`\n\n\n# A tibble: 1 × 4\n      x     y     a b    \n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     2    10 a    \n\n\n\n\nCode\ndf1 %&gt;% left_join(df2)\n\n\nJoining with `by = join_by(x)`\n\n\n# A tibble: 2 × 4\n      x     y     a b    \n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     2    10 a    \n2     2     1    NA &lt;NA&gt; \n\n\n\n\nCode\ndf1 %&gt;% right_join(df2)\n\n\nJoining with `by = join_by(x)`\n\n\n# A tibble: 2 × 4\n      x     y     a b    \n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     2    10 a    \n2     3    NA    10 a    \n\n\nCode\ndf2 %&gt;% left_join(df1)\n\n\nJoining with `by = join_by(x)`\n\n\n# A tibble: 2 × 4\n      x     a b         y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;\n1     3    10 a        NA\n2     1    10 a         2\n\n\n\n\nCode\ndf1 %&gt;% full_join(df2)\n\n\nJoining with `by = join_by(x)`\n\n\n# A tibble: 3 × 4\n      x     y     a b    \n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     2    10 a    \n2     2     1    NA &lt;NA&gt; \n3     3    NA    10 a    \n\n\nWhat are the differences between the join functions?"
  },
  {
    "objectID": "slides/Chapter00.html#types-of-data-files",
    "href": "slides/Chapter00.html#types-of-data-files",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Types of data files",
    "text": "Types of data files\nThe letters after the . at the end of a files name are called the extension and tell you the type of file.\nIf the data are stored as a *.csv or “comma separated values” file, then you can use the read.csv() or read_csv() function to load the file. If it’s a text file (.txt) with columns separated by spaces or tabs, you can use read.table() or read_table() function. The ones with underscores ( read_csv() and read_table() ) are in the readr package, so you’ll need to load it first (though readr is part of tidyverse, so if you load tidyverse you’re all set). We will introduce tidyverse in a moment.\nYou can also load Microsoft Excel files using functions read_excel(), available in the readxl package.\nNote that Excel files usually contain blanks for missing or unreported data or allocate many rows for variable description, which can cause issues while importing them."
  },
  {
    "objectID": "slides/Chapter00.html#tidy-data",
    "href": "slides/Chapter00.html#tidy-data",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\nThere are three interrelated rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\nFigure 1: Tidy data"
  },
  {
    "objectID": "slides/Chapter00.html#why-ensure-that-your-data-is-tidy",
    "href": "slides/Chapter00.html#why-ensure-that-your-data-is-tidy",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Why ensure that your data is tidy?",
    "text": "Why ensure that your data is tidy?\nThere are two main advantages:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. Most built-in R functions work with vectors of values.\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data."
  },
  {
    "objectID": "slides/Chapter00.html#summarizing-data",
    "href": "slides/Chapter00.html#summarizing-data",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nSummarizing data is a large part of data analysis."
  },
  {
    "objectID": "slides/Chapter00.html#dplyr",
    "href": "slides/Chapter00.html#dplyr",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "dplyr",
    "text": "dplyr\nThe following six functions of dplyr are very useful for data wrangling :\n\nFor selecting columns, use select()\nFor subsetting data, use filter()\nFor re-ordering (e.g. ascending/descending), use arrange()\nFor augmenting new calculated columns, use mutate()\nFor computing summary measures, use summarise()\nFor group-wise computations (e.g. summary measures), use group_by()\n\nThere are many other functions such as transmute() which will add newly calculated columns to the existing data frame but drop all unused columns. The across() function extends group_by() and summarise() functions for multiple column and function summaries. For example, you like to report rounded data in a table, which calls for an operation across both rows and columns."
  },
  {
    "objectID": "slides/Chapter00.html#grammar-of-graphics",
    "href": "slides/Chapter00.html#grammar-of-graphics",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe main idea behind the grammar of graphics of [@Wilkinson] is to mimic the manual graphing approach and define building blocks and combine them to create a graphical display. The building blocks of a graph are:\n\ndata\naesthetic mapping\ngeometric object\ntransformation or re-expression of data\nscales\ncoordinate system\nposition adjustments\nfaceting"
  },
  {
    "objectID": "slides/Chapter00.html#aesthetic-mapping-aes",
    "href": "slides/Chapter00.html#aesthetic-mapping-aes",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Aesthetic Mapping (aes)",
    "text": "Aesthetic Mapping (aes)\nIn ggplot land aesthetic means visualisation features or aesthetics. These are\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nAesthetic mappings are set with the aes() function."
  },
  {
    "objectID": "slides/Chapter00.html#geometric-objects-geom",
    "href": "slides/Chapter00.html#geometric-objects-geom",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Geometric Objects (geom)",
    "text": "Geometric Objects (geom)\nGeometric objects or geoms are the actual marking or inking on a plot such as:\n\npoints (geom_point, for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for boxplots)\n\nA plot must have at least one geom but there is no upper limit. In order to add a geom to a plot, the + operator is employed. A list of available geometric objects can be obtained by typing geom_&lt;tab&gt; in Rstudio. The following command can also be used which will open a Help window.\nhelp.search(\"geom_\", package = \"ggplot2\")"
  },
  {
    "objectID": "slides/Chapter00.html#tidying-data",
    "href": "slides/Chapter00.html#tidying-data",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Tidying data",
    "text": "Tidying data\nThe principles of tidy data might seem so obvious that you wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most real data is (very) untidy. There are two main reasons:\n\nData is often organized to facilitate some goal other than analysis. For example, it’s common for data to be structured to make data entry easy.\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\n\nThis means that most real analyses will require at least a little tidying. You’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data."
  },
  {
    "objectID": "slides/Chapter00.html#tidying-data-example",
    "href": "slides/Chapter00.html#tidying-data-example",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Tidying data example",
    "text": "Tidying data example\nThe hospital admissions dataset is untidy because it does allocate many columns for a variable.\n\n\n  YEAR PERI NORTH1 NORTH2 NORTH3 SOUTH1 SOUTH2 SOUTH3\n1 1980    1      0      4     27      4     16     27\n2 1980    2      6     11     31      8     18     21\n3 1980    3      6      4     25     20     16     24\n4 1980    4      1     10     31     22     17     20\n5 1980    5      4     16     22     21     30     31\n6 1980    6      3      8     28     31     20     30\n\n\nThe main response variable namely the number of admissions is allocated different columns depending on the North and South locations. This format is also called wide format which can be made into a tidy long format. We can use the dplyr function pivot_longer() to change this data into long format. Try-\nThe command pivot_wider() does the opposite to pivot_longer()"
  },
  {
    "objectID": "slides/Chapter00.html#common-data-issues",
    "href": "slides/Chapter00.html#common-data-issues",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Common data issues",
    "text": "Common data issues\n\nSpelling, capitalization, typos, and white spaces\nthere are some handy functions in stringr and janitor packages\nNAs: there is a difference between not observed, not recorded, and not applicable. This difference requires understanding of the data and data entry process for that project\ncan remove NAs using na.omit(), !is.na(), na.rm=T, etc. depending on what function you are using. See help menu for more details\nMultiple types of data in a single column or wrong data type\ncan force a data class using as.numeric() for example"
  },
  {
    "objectID": "slides/Chapter00.html#joining-data",
    "href": "slides/Chapter00.html#joining-data",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Joining data",
    "text": "Joining data\nMutating joins allow you to combine variables from multiple tables. For example, consider the flights and airlines data from the nycflights13 package. In one table we have flight information with an abbreviation for carrier, and in another we have a mapping between abbreviations and full names. You can use a join to add the carrier names to the flight data:\n\n\n# A tibble: 336,776 × 9\n    year month   day  hour origin dest  tailnum carrier name                    \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                   \n 1  2013     1     1     5 EWR    IAH   N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 LGA    IAH   N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 JFK    MIA   N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 JFK    BQN   N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 LGA    ATL   N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 EWR    ORD   N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 EWR    FLL   N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 LGA    IAD   N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 JFK    MCO   N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 LGA    ORD   N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nAs well as x and y, each mutating join takes an argument by that controls which variables are used to match observations in the two tables."
  },
  {
    "objectID": "slides/Chapter00.html#data-quality-checks",
    "href": "slides/Chapter00.html#data-quality-checks",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Data Quality Checks",
    "text": "Data Quality Checks\nIt is a good idea to check the quality of secondary data sourced from elsewhere. For example, there could be missing values in the dataset. Consider the Telomeres data downloaded from http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\nThe missingness of data can be quickly explored using many R packages. The downloaded Telomeres dataset contain many missing values.\n\n\n\n\n\n\n\n\n\nor\nThe term Missing completely at random (MCAR) is often used to mean there is there is no pattern to the missing data themselves or alternatively the missingness is not related to any other variable or data in the dataset. In other words, the probability of missingness is the same for all units. So no bias is caused by the missing data, and we can discard cases with missing data when we fit models.\nIn practice, we often find missing data do have a relationship with other variables in the dataset but the actual missing values are random. This situation of data conditionally missing at random is called Missing at random (MAR) data. For a particular survey question, the response rate may differ depending on the respondent’s gender. In this situation, the actual missingness may be random but still related to the gender variable.\nMissing not at random (MNAR) is the pattern when missingness is related to other variables in the dataset, as well as the values of the missing data are not random. In other words, there is a predictable pattern in the missingness. So we cannot avoid the bias when missing cases are omitted.\nThere are also situations such as censoring where we just record a single value without actually measuring the variable of interest.\nImputation of data can be made except for the case of MCAR type. A number of R packages are available for data imputation; see https://cran.r-project.org/web/views/MissingData.html or https://stefvanbuuren.name/fimd/. We may occasionally cover data imputation issue in an assignment question.\nThere are also R packages to perform automatic investigation for data cleaning. Try-\nRule based validation is enabled in the R package validate. The R package janitor has a function get_dupes() to find duplicate entries in the dataset. Cleaner package will allow to clean the variables so that the columns are consistent in terms of the factor, date, numerical variable types. You will be largely using data that are already cleaned for your assignments but be aware that you have to perform data cleaning and perform necessary quality checks before analysis.\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter00.html#data-analysis",
    "href": "slides/Chapter00.html#data-analysis",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nIdentify the question you would like to answer\nCollect data\nClean data to prepare it for analysis\nAnalyse data\nInterpret the results"
  },
  {
    "objectID": "slides/Chapter00.html#types-of-data-anlysis",
    "href": "slides/Chapter00.html#types-of-data-anlysis",
    "title": "Chapter 0: Introduction: R, quarto, tidyverse",
    "section": "Types of data anlysis",
    "text": "Types of data anlysis\n\nDescriptive:\nWhat happened?\nDiagnostic:\nWhy did it happen?\nPredictive:\nWhat might happen in the future?\nPrescriptive:\nWhat should we do about it?"
  },
  {
    "objectID": "workshops/ws02.html#make-it-fancy",
    "href": "workshops/ws02.html#make-it-fancy",
    "title": "Chapter 2 Workshop",
    "section": "Make it fancy",
    "text": "Make it fancy"
  },
  {
    "objectID": "workshops/ws01.html#quarto",
    "href": "workshops/ws01.html#quarto",
    "title": "Getting Started With R",
    "section": "Quarto",
    "text": "Quarto\nDemonstrate how to open a new Quarto document"
  },
  {
    "objectID": "workshops/ws02.html#adjusting-labels",
    "href": "workshops/ws02.html#adjusting-labels",
    "title": "Chapter 2 Workshop",
    "section": "Adjusting labels",
    "text": "Adjusting labels\n\n\nCode\n?labs()"
  },
  {
    "objectID": "workshops/ws02.html#color-pallets",
    "href": "workshops/ws02.html#color-pallets",
    "title": "Chapter 2 Workshop",
    "section": "color pallets",
    "text": "color pallets"
  },
  {
    "objectID": "workshops/ws02.html#themes",
    "href": "workshops/ws02.html#themes",
    "title": "Chapter 2 Workshop",
    "section": "themes",
    "text": "themes\nMore graphing examples are here (R code file)."
  },
  {
    "objectID": "studyguide/1-data-collection.html",
    "href": "studyguide/1-data-collection.html",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "",
    "text": "“Data! data! he cried impatiently, I can’t make bricks without clay.”\n— Sir Arthur Conan Doyle, The Vopper Beeches"
  },
  {
    "objectID": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "href": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Categorical or Qualitative Data",
    "text": "Categorical or Qualitative Data\n\nNominal Data\nAt this level, objects are classified by qualities or characteristics, and are placed in categories. Often these categories are really only names, for example Ford Focus, Mondeo, or Mustang; Boeing 737, 747 or 767, etc. Hence we describe the data as ‘nominal’. Sometimes confusion arises because the names are replaced (coded) by numbers as in the aeroplane example, or to facilitate data entry into a computer. An example of the latter would be a person’s marital status, coded as 1 (never married), 2 (currently married), 3 (widowed) or 4 (separated or divorced). It would make no sense to perform any mathematical manipulation on these numbers for they are only abbreviations for the categories. Notice that categories should cover all possibilities so that a further category may be needed, namely 5 (in a de facto relationship).\n\n\nOrdinal Data\nOrdinal data are categorical but the levels have an intrinsic order to them. Typical examples are a 3-point scale for burns - mild, moderate, severe, or a 5-point agreement scale - strongly disagree; disagree; no opinion; agree; strongly agree.\nThe distances between any two responses may differ. For example, disagree may be perceived by some respondents, although not others, as closer to strongly disagree than to no opinion. For this reason, there are problems in aggregating such responses. There are ‘nonparametric’ statistical methods to cope with these problems. It may be feasible, however, to add or average responses if the sample size is reasonably large and the assumption can be made that the differences in usage of the measures will average out over the sample."
  },
  {
    "objectID": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "href": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Quantitative (i.e., measured or counted) data",
    "text": "Quantitative (i.e., measured or counted) data\n\nDiscrete Data\nDiscrete data usually arises by counting events or objects. Because of this discrete data are usually integers, but not always. For example, one may count the number of accidents last year at a particular intersection. On the one hand you can’t have fractions since you can’t have half an accident. On the other hand a further analysis may require you to state the proportion of the accidents that were nose-to-tail. Then if there were n accidents, the proportion of nose-to-tail ones could be \\(\\frac{0}{n}, \\frac{1}{n}, \\frac{2}{n}, ... ,\\frac{n}{n}\\) - a finite range of values, so the proportion is still discrete.\n\n\nContinuous Data\nWithout going into the mathematical definition of continuity, we tend to visualise continuous data as having a flowing nature, like water flowing under a bridge, or time passing in a continuous fashion. So one could in theory measure amounts (of water or time) to any level of precision, with many decimal places. Of course in practice, the measured responses will rarely be exactly continuous for, even if the underlying variables are assumed continuous, the instrument used to take readings will only be able to do so on a discrete scale. If the variable of interest is time, responses may be integers (say) 1, 2, … seconds or if the instrument can distinguish to the nearest \\(\\frac{1}{100}\\) of a second, the response will still be discrete but is now 1.00, 1.01 or 0.99. What we can say is that if the unit of measurement is sufficiently small then each individual possible reading will have an infinitesimal probability of occurring and we can basically ignore such individual points and only talk about probability in terms of intervals (e.g. one-second intervals rather than individual 10.99 seconds, say). Thus the practical difference between discrete and continuous data is that with real discrete data, outcomes each have their own separate probability of occurring, but with continuous data, probability is only thought of as associated with intervals.\n\n\nInterval and ratio scales\nStatisticians sometimes further distinguish among quantitative variables as being interval or ratio data. In fact some theoreticians write at length about the ‘four levels of measurement’: nominal, ordinal, interval or ratio.\nFor interval scale responses are assumed to be points on a linear scale. For example on a temperature scale the difference between 10 and 20 degrees is the same as between 30 and 40 degrees. On the usual Fahrenheit or Celsius scale there is a problem with respect to the origin, or zero point, as 20 degrees cannot be assumed to be twice as hot as 10 degrees. It would be meaningful to take differences between measurements or add them together but not to divide or multiply them. In many statistical formulae, deviations from means are involved which obviates the need for an absolute zero, and ratios such as slopes are quite acceptable.\nRatio scales are sometimes regarded the highest level of measurement, and are usually referred to by saying there is a fixed (or absolute) zero. An example would be the number of people in a room (can’t have negatives!) or the time taken to complete a given task. In this case, a reading of 4.2 seconds is twice that of 2.1 seconds. That is, it is meaningful to perform all the mathematical operations of addition, subtraction, multiplication and division.\nFrom a practical point of view, the type of data (categorical - nominal or ordinal; discrete; or continuous) may be crucial to the statistical analysis. It may be convenient to use a statistical method appropriate to ratio level data in the case mentioned above of the time to complete a given task, but the results may also be analysed by a non-parametric technique which actually only requires ordinal data. Similarly it may be satisfactory as a first step to analyse ordinal data by assigning scores {1, 2, 3, 4, and 5} to the categories and running the data through a regression package that assumes Normal data (better approaches are available!). Indeed, it may be difficult in some circumstances to clearly decide on the appropriate level, and the distinction particularly between interval and ratio levels is often unimportant."
  },
  {
    "objectID": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "href": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Measuring Devices or Instruments",
    "text": "Measuring Devices or Instruments\nAny device used to collect measurements is called an instrument. It may be a physical device such as a measuring rule to gauge the heights of plants or a counting device such as a Geiger-counter for measuring radioactive material. In the social sciences or marketing, the instrument may be such a physical device or it may be a questionnaire which requires a more subjective response.\n\nMeasurement Error\nMeasurement error may arise if the instrument tends to give a reading which is too high or too low. Instruments should be checked as often as possible and standardised by checking them against known standards. This is not really possible with questionnaires but one should note the need for pilot testing the questionnaire on a group of subjects who are not in the frame for the survey.\n\n\nIndirect Measures\nIt is a common mistake to assume that the values reported for a variable are direct measurements of that variable. It is more usually the case that the recorded value reflects the desired value but is only an indirect measure of it. A doctor may wish to have some measure of the health of a patient, and, for this reason, a blood pressure measurement is taken, but blood pressure is not a direct measure of health for there are many inherited, climatic and individual effects which also play a part in the blood pressure reading. Even then a typical blood pressure reading would be required, but the mere fact that a measurement is being made will have an effect on the patient and possibly affect the reading. The stress of being in a hospital or a doctor’s surgery may increase the measurement.\nIt should be noted that the instrument used will only rarely be able to give a direct reading. Temperature is often gauged by the expansion of mercury in glass tube which is an indirect measure and will depend on a number of factors and assumptions, perhaps the greatest being that the expansion of the mercury is linear with temperature. In most experiments the measurements, even if indirect, are closely correlated with the variable under study but such assumptions should not be accepted too readily.\nAnother question which should be asked of data is who did the recording? Often, questionnaires require that people provide the information themselves, that is by self-reporting. Conscious or sub-conscious influences may have some bearing on the numbers provided. If someone is asked his/her income or workload, the answer may depend on what is perceived by the respondent as the aim of the survey, who is asking the question and who will have access to the results. A seemingly direct question may evoke an indirect answer. If the data is collected by a researcher or technician there may be some possible hiccups on the interface between the respondent and the researcher or the instrument and researcher."
  },
  {
    "objectID": "studyguide/1-data-collection.html#range-of-instruments",
    "href": "studyguide/1-data-collection.html#range-of-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Range of Instruments",
    "text": "Range of Instruments\nThe range of instruments is immense and in the remainder of this section a few examples will be given.\n\nObservation\nIt is common in the study of animals or birds to observe them in their natural settings. This may be done directly such as counting the number of wingbeats per minute or it may be of an indirect approach such as counting the frequency of spore in order to determine the number of animals in a given area. This method of data gathering is often costly and time-consuming. It may help to answer what is occurring rather than why.\nAs there is a high labour content with this instrument, errors may occur due to tiredness and boredom and coding errors may be a problem depending on how well defined are the categories to be observed. If many observers are involved, there is the added difficulty that the same behaviour may be perceived differently and hence coded differently by different observers.\nObservation does have an important part to play in a preliminary study to decide on the areas which require further study. Some researchers tend to bypass or shorten this phase by using a multivariate package on the computer hoping that it will sort out important relationships from a morass of complex data. It is surprising how many novel and correct scientific processes were discovered by such painstaking workers as Darwin and Mendel using only visual observation.\n\n\nInterviews\nThis method can suffer from some of the problems of observation in that it can be time consuming, involve coding errors and be influenced by different interviewers making different judgements.\nInterviewers need to be carefully trained so that they allow the person being interviewed to express his or her own views rather than interposing their own feelings or prejudices. On the positive side, a trained interviewer can sense nuances behind supplied answers in a way that observations or questionnaires cannot. A much richer range of responses can then be recorded which makes the interview ideal for pilot studies in particular as it can open up new avenues of study.\n\n\nQuestionnaires\nQuestionnaires are a common method used to collect data and they are very versatile as they can be administered by mail, telephone or face to face. Mailed questionnaires have the advantage of being efficient time-wise for a researcher and may be less threatening to the respondent as they can be answered when and where the respondent chooses. They are appropriate if the respondent needs to give some thought to the questions or where the answers involve sensitive information. Unfortunately, this flexibility can lead to respondents procrastinating or not replying at all. The problem of non-response will be considered later.\nHow long should the questionnaire be? Almost invariably, it is too long as the researcher strives to obtain as much information as possible. A long and tiresome questionnaire may be counter-productive as the quality of the data may suffer. There has been a lot of research on questionnaire design. Some points to note are that straightforward questions should be asked near the beginning so that the respondent feels at ease while questions requiring judgements or moral decisions should come later."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-human-interface",
    "href": "studyguide/1-data-collection.html#the-human-interface",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Human Interface",
    "text": "The Human Interface\nMention has been made of some problems associated with respondents giving replies and researchers seeking to understand those replies and to code them. It is worthwhile taking more time to reconsider these processes for the quality of data will invariably be greatly affected by the human interface with the research instrument.\nThe yields of different varieties of field grasses would seem to be a clear cut and well defined and measured quantity but the yield would be influenced by many factors such as the height of the cut, perhaps the speed at which the mower is operated and the ability of the assistant to mow within the specified boundaries.\nWith more subjective instruments and human respondents other problems may arise and affect the quality of the data.\n\nHuman Respondents\nA number of possible difficulties may arise:\n\nA question, which is clear-cut and unambiguous to the researcher, may not be so to the respondent who may not understand the question or is not too sure about it because it is framed in unfamiliar language or relates to something outside his or her experience.\nThe response may depend on a number of extraneous factors such as how cheerful and at ease the respondent feels.\nQuestions may require remembering certain facts or feelings and memory can be very selective.\nThe situation may involve many variables while the respondent is asked to respond on only one dimension, which may be perceived as a composite of these other variables. The respondent may not be able to make the judgements necessary to give an accurate response. In one questionnaire, some questions may be answered very accurately while others may be of doubtful value. If responses are added in some way to give a total, there may be a problem in that some of the components have been measured accurately but others are less precise.\nQuestionnaires can be self-fulfilling prophecies in that the structure and language of the questions suggest the correct or desired responses. Socialising factors may prejudice the results in that most people are hesitant to be rude or to reveal to strangers their deep desires and true feelings, particularly if these are thought to be socially undesirable.\n\n\n\nResearchers\nThe human interface also arises when someone administers the instrument of the research and some points to note are:\n\nThe competence of the researchers and assistants is important. Judgement may be required to understand the response and to make a note of it. In an interview situation, the researcher must listen and try to understand the responses in terms of his or her framework of judgements and in relation to the aim of the interview. With more objective instruments, they must be read accurately but even with such measuring scales one assistant may tend to read high or to record even calibrations whereas another would not do so, which introduces an unwanted variation in the data collection process.\nThe feelings and beliefs of the assistant may intrude into the collection process. The interaction with the subject in this way may introduce other unwanted variation.\nIt is a salutary point to note that usually the individual who collects the raw data is an underpaid and possibly under-trained junior who is not highly motivated to produce high quality data.\n\n\n\nSelf-Administered Tests\nIn some situations, the subject records the response as in self-administered questionnaires, for example, tick a box or circle a number. There may be advantages in this for the respondent is presumably the best person to know what response should be given. On the other hand, the respondent is usually not trained so that there may be a higher incidence of coding errors and questions which are left unanswered. On the other hand, if the researcher does the coding, unwarranted assumptions may be made.\nIt is conceivable that many human interventions and possible errors may occur in an experiment. For example, a field worker may make certain observations which are studied by another assistant who decides on categories for them while a further assistant codes these categories and finally another person enters them into a computer. Errors are possible at each step along the way.\nAt the other extreme, it is possible for information to be relayed directly to the computer without human interference. Microprocessors can be used to collect and transmit a whole variety of information. In experiments on animals, sensors can be placed on different parts of their bodies to relay information to the computer. These facilities open up new vistas and challenges for statisticians to monitor large amounts of data, for it may not be obvious how to cope with them."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-response-bias",
    "href": "studyguide/1-data-collection.html#non-response-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-response Bias",
    "text": "Non-response Bias\nNon response is very common in all types of data collection and is probably the largest source of non-sampling error. It can occur at three stages of the process. An element may be selected but not found. For example, sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey. It may be hiding or perhaps has died. If the elements are people, some may not be at home at the time, or, if it is a telephone survey, the telephone may be cut off from certain selected individuals.\nIt may not be possible to take a measurement. This happens quite often with people as some may forget, or refuse, to answer the questionnaire. The measuring instrument may not work or give a ludicrous response which may be deleted by the technician. An element may be contacted, a measurement taken but the result not written down in the correct place, or could be so illegible that it cannot be used.\nWherever possible, nonresponse bias should be reduced by making every possible effort to obtain a response. When interviewing people about a particular product, market researchers will often make two call backs (that is three attempts in all) to reach a subject if that person is not at home on the first call. Cost factors will often prohibit further attempts. Surveys by mail often include a stamped addressed envelope or even money or a token for merchandise to arouse moral scruples and force a reply.\nThe problems arising from nonresponse can be illustrated with a simple example: Suppose that 200 people are asked their opinion on a certain issue, only 100 reply of whom 70 are in favour, 30 opposed.\nWhat percentage of the original 200 are in favour?\nThe answer depends on the assumptions made.\n\nSuppose the nonrespondents feel the same about the issue as the respondents. (This is the usual assumption made). This gives:\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n70\n30\n100\n\n\nTotal\n140\n60\n200\n\n\n\nThus the estimate of the number in favour is then 140/200 or 70%.\n\nSuppose that those who did not respond did so because they were opposed to the issue. Thus\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n0\n100\n100\n\n\nTotal\n70\n130\n200\n\n\n\nThe estimate of the number in favour is 70/200 or 35%.\n\nIt may be reasonable in other circumstances to assume that the nonrespondents were quite happy with the status quo and were in favour. Then the estimate of the number in favour is 170/200 or 85%.\nIt may be possible to make other reasonable assumptions if additional information is at hand on the whole batch of 200. We can massage the results in different ways but none of them is very convincing. The moral is: Try to reduce nonresponse by all legitimate means."
  },
  {
    "objectID": "studyguide/1-data-collection.html#some-terminology",
    "href": "studyguide/1-data-collection.html#some-terminology",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Some Terminology",
    "text": "Some Terminology\nIn a census, every element of the population is contacted, counted and other quantities collected or evaluated. Conceptually, the procedure is straightforward and perhaps trivial. In practice and particularly if the population is large, difficulties often arise. There are three critical operations in census taking, namely identifying an element as belonging to the population under study, making contact with that element and obtaining the appropriate information in a suitable form.\nIt may be helpful, at this point, to introduce some terminology to formalise the process of taking a census. The population at which the study is aimed is termed the target population . To obtain information from this population it is necessary to have a means of operationalising the data collection. This involves a frame, which may be a listing of the elements of the population or it may be an operating method such as a geographical map, suitably divided into smaller, manageable areas. A further step is necessary to contact each element of the frame and collect the appropriate information. The resulting set of elements on which completed, usable data has been collected could be called the actual population. It is possible to consider these populations as consisting of the measurements under study rather than the elements, be they objects, animals or people. More will be said on this later.\nCensus taking can then be represented by the sequence\ntarget population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) actual population.\nIdeally, these three populations would consist of the same elements but in practice they could be represented in Figure 1).\n\n\n\nFigure 1: Relationship between the target population, frame and actual population\n\n\nA simple example of a census would be the exercise of collecting information from the students in a particular school. Suppose the school authorities want to discover, from each student, the number of younger siblings who may wish to attend the school in future years. At first glance, the target population is well defined, as the current students in the school. However queries could arise such as whether adult or part-time pupils should be included. The most suitable frame would, no doubt, be the school roll. Hopefully, the roll would accurately reflect the students currently at the school but it may not exactly match the target population. Some students may be temporarily visiting the school, either staying with relatives or a parent who has a local, temporary job. On the other hand, some students on the roll may be visiting another part of the country, or may be in a hospital or other institution, and there may be some doubt whether their absence is temporary or permanent.\nFrame inefficiencies may well occur to the extent that the frame does not match the target population. The collection of information on siblings may encounter snags as some students may not respond due to absence, or parents or student may refuse to divulge the required information. The actual population may then fall short of the frame, which contains the maximum possible elements, which in this case, are students."
  },
  {
    "objectID": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "href": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Elements, Measures and Associated Variables",
    "text": "Elements, Measures and Associated Variables\nWith a census, there is usually some ambivalence as to whether the objects under study are the elements of the population (such as plants, animals or people) or the measurements on these elements. The measurements may simply be the count, or may be such variables as the length, weight, salary or any other measure. The selection process will focus mainly on the elements, but from then on attention will be focussed on measurement and then analysis. These two ideas can be incorporated into the notation \\[U=\\left(L,Y,X\\right)\\] Here \\(U\\) denotes the population, \\(L\\) is the set of labels \\((i = 1,2,...,N)\\) attached to the \\(N\\) elements of the population, \\(Y\\) is the measurement, or response, under study from each element in the population, and \\(X\\) is an associated variable measured on each element in the population.\nFor example, for the census carried out on the students of a high school, the labels \\(i\\) could be the identification numbers of the students on the roll \\(L\\). The measurement on the \\(i^{th}\\) student, represented by \\(y_i\\), may be a variable such as the daily expenditure at the school tuck shop. The associated variable, \\(x_i\\), for the \\(i^{th}\\) student may be the student’s weight. On the other hand, the mark a student gets on a standardised test when entering high school may be \\(x_i\\), with \\(y_i\\) the school certificate mark of that student in mathematics.\nIn this example the survey may, in practice, be carried out as a sample rather than a census but conceptually it would be possible, to carry out such a census. Another point which may need discussion and clarification is whether the population represented by \\(U\\) should refer to the target population, the frame or the actual population; usually \\(U\\) would refer to the target population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-sampling-errors",
    "href": "studyguide/1-data-collection.html#non-sampling-errors",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-sampling Errors",
    "text": "Non-sampling Errors\nA number of errors may occur in collecting information even when a census is being taken. The elements of the census must be contacted, a measurement or observation taken and this information recorded. As no sampling procedure is involved, it seems reasonable to label errors as non-sampling errors.\nThe goal of the survey will be to produce summary statistics such as the population mean and standard deviation. If errors are committed in the collection process, the summary statistics may be incorrect. It is possible that these errors will cancel out but we cannot rely on this. The errors may lead to a tendency for the calculated mean to be too low and the mean is said to be biased. Alternatively, the bias may be on the high side and, at times, it may be difficult even to decide the direction of the bias although we know there is a strong possibility of it occurring."
  },
  {
    "objectID": "studyguide/1-data-collection.html#selection-bias",
    "href": "studyguide/1-data-collection.html#selection-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Selection Bias",
    "text": "Selection Bias\nIt may seem a contradiction in terms to speak of errors in selecting elements of a census for the aim is to contact every element in the population. From the discussion in the above section, it is clear that some difficulties may arise in the sequence from target population to frame and these difficulties are often called frame inefficiencies.\nA survey may be carried out on all exporting firms in New Zealand. The frame may be a published list of exporting firms but the list may be out of date as some firms have gone broke or been taken into receivership. There may be firms not on the list but which, nevertheless, do engage in some form of exporting besides their other activities.\nOnce the elements of the frame have been located, there remains the step of extracting information from them. For some reasons particular firms in the frame may not wish to take part in the survey so that errors may occur in this second step also."
  },
  {
    "objectID": "studyguide/1-data-collection.html#inference",
    "href": "studyguide/1-data-collection.html#inference",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Inference",
    "text": "Inference\nIt is not appropriate to carry out any statistical test or form a confidence interval on census data because they are measurements on the whole population. Statistical inference is using data from a sample to make statements about the broader population from which the sample was taken. When we do a census, we have the whole population, so we are no longer making inferences.\nNext, we’ll consider sampling from the population instead of taking a census."
  },
  {
    "objectID": "studyguide/1-data-collection.html#introduction-1",
    "href": "studyguide/1-data-collection.html#introduction-1",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Introduction",
    "text": "Introduction\nSampling as a scientific exercise is a peculiarly twentieth century phenomenon despite the fact that sampling has been used in an ad hoc manner from earliest times. Sampling, as in sample surveys, was accepted only after many years of argument and counter-argument. The most intensive and extensive discussion took place at the International Statistical Institute (ISI) meeting in Berne in 1895. Kiaer explained the ‘representative method’ he had used in Norway. Further meetings of the ISI enabled Kiaer to expand and clarify his approach. Four principles have been identified in Kiaer’s reasoning:\n\nRepresentativeness in terms of adequate representation of identifiable groups in the population.\nSelection of objects to be as objective as possible.\nThe reliability of the results should be assessed. Each survey should be divided into a number of distinct parts using a different representative method for each. Comparison of results of these parts would provide evidence as to how much faith could be placed in the results of the survey.\nFinally, Kiaer insisted on a complete specification of the method of selection.\n\nBowley (1906) provided a theory of inference for survey samples which helped to answer the question of how accurate an estimate from a large sample was. Using a Bayesian argument, his approach was only valid for the case where the chances were the same for all items of the groups to be sampled.\nIt was not until Neyman (1934) that random, rather than purposeful, sampling was given a strong mathematical justification. Neyman showed that it did not matter if equal or unequal probabilities were used provided that probabilities were known in advance. His arguments were not based on specific distributions as in Bayesian approaches so that the touchstone of randomness seemed to have finally won the day. For an interesting discussion on the controversies in the history of survey sampling, see Brewer (2013)."
  },
  {
    "objectID": "studyguide/1-data-collection.html#why-sample",
    "href": "studyguide/1-data-collection.html#why-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Why Sample?",
    "text": "Why Sample?\nThere is a widespread belief that a census, rather than a sample, should be taken if at all possible. Most people feel secure with a census but are somewhat uneasy about only collecting a sample and using mathematical means to estimate properties of the population. Lack of sufficient resources of money, trained manpower and so on is often the motivation to reject a complete enumeration. A compromise, of course, is necessary in that a cheaper, quicker collection method using a sample may result in less accurate estimates. Often, however, the reduction in accuracy is slight compared with the considerable reduction in cost. Time is also a factor in that a census may require officials to collect data over a long period which may raise difficulties in that the responses would be meant to relate to a particular point in time.\nAn important point which is often overlooked is that a complete enumeration is only superior to a sample if the data is collected and processed at the same high standard. Often, researchers try to collect too much data for the resources and trained man-power available which results in a large amount of data whose accuracy and validity is suspect. A well collected sample will always be of more use than a shoddily taken census. Another common fault is the attempt to collect too much information from every element, be it an object, animal or person. A questionnaire has a tendency to expand to the point that it frustrates or annoys the recipient or the personnel administering it. In countries where monthly household surveys are held, their results are invariably more accurate than the less frequent population censuses.\nOne classical case in which a sample is preferable to a census is that of destructive sampling in which each object is tested until it breaks or expires. For example, testing the number of hours a light bulb shines before it burns out. The selected items are useless after the test so that there are compelling reasons for limiting the size of the sample. It is salutary to realise that many people selected to give information or to take part in tests or experiments will be affected in some ways. The process of collecting data will itself affect the elements in the sample so that they may no longer be representative of the population about which inferences are to be made. For example, in some quarters, there is a fear that political opinion polls do not merely reflect public opinion but to some extent form it.\nNonsampling errors, nonresponse in particular may introduce biases. Unfortunately, the size and/or direction of these biases are generally unknown. Due to the smaller size of a sample, it may be much easier to reduce nonresponse than in a much more cumbersome enumeration. Call-backs and other procedures which have been devised to reduce nonresponse are very time consuming so that they are rarely feasible when the number of elements is large.\nTo reduce unwanted effects such as interviewer biases, statistical tests can be employed. These require certain assumptions about the population and selection method which have been used. We can never be sure that the assumptions associated with these tests are valid. If the sample suggests that they may not be true, some action must be taken such as a data transformation to make the results more ‘Normal’."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "href": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Case for Randomisation",
    "text": "The Case for Randomisation\n\n“Randomization is too important to be left to chance.”\n– J. D. Petruccelli\n\nThe sample should be similar to the population. It is easier to state this than it is to define what is meant by the statement and how to achieve this similarity. Randomisation can be used to obtain a sample which, on average, has similar properties to the population. Not only is this so but it is possible to estimate how closely the sample reflects the population and attach a probability to the possible discrepancy. It is not possible to state with certainty that a particular measurement from a sample is peculiar although it would be possible to attach a probability to such an unusual value. Similarly, it would be possible to make a probability statement about the sample itself if the sampling process was replicated.\nIf the distribution of the variables is known, randomisation enables the distributions of some combinations of the sample values to be calculated. In common with other areas of statistics, it is more likely that the distributions of these variables are not known although these distributions may be hypothesised. The validity of any conclusions will be subject to the proviso that the assumptions are correct. If the distributions are not known the Central Limit Theorem (CLT) may be invoked so that the approximate Normal distributions may be assumed for some estimators.\nOn the other hand, there are enticing reasons for placing restrictions on the sample to make it similar to the population in many respects such as proportions in certain subgroups. Surveys are rarely carried out as a theoretical exercise and results may only be believed if those who commissioned and financed the survey believe in its accuracy. If the sample appears peculiar in relation to some known variables, the validity of the survey may be called in question. To obtain a representative sample to fit the known, or assumed, distribution in the population, the elements may be selected purposely. The resulting estimators of variables may be very accurate but there is no way of determining how accurate they are.\nAs a compromise, it is possible to use a randomisation scheme with certain restrictions. A simple example is that of stratified sampling with different probabilities of selection for each stratum. This procedure can be extended to allow each element in the population to have its own probability of inclusion into the sample. Alternatively, a randomisation approach can be used to draw a sample which may be rejected, however, if certain restrictions are not fulfilled. An important point is that the sample should be chosen with a known probability. Another point to note is that the known information about a population will involve a variable, for example \\(X\\), whereas the measurements to be collected are from a another variable, say \\(Y\\). The efficacy of the survey will depend heavily on the accuracy of \\(X\\) and the relationship between \\(X\\) and \\(Y\\).\nIt should be noted that restrictions may limit the number of elements selected from particular subgroups so that it may not be reasonable to invoke the Central Limit Theorem and assume that estimators in these subgroups follow a Normal distribution."
  },
  {
    "objectID": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "href": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Terminology and Notation for Samples",
    "text": "Terminology and Notation for Samples\nA sample is a subset of a population. There are innumerable ways of drawing a sample so that there is a richness and variety in sampling methods which do not exist in census methods. The first step in obtaining a sample is the same as for taking a census in that a frame must be set up. The sample is then drawn from the frame so that the sequence is target population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) sample.\nThe relationship between population and sample is shown in Figure 2 (which is almost the same as Figure 1).\n\n\n\nFigure 2: Relationship between the target population, frame and sample\n\n\nIt was pointed out above that a population, in practice the frame, could be written as \\(U=\\left(L,Y,X\\right)\\). By contrast probability samples could be denoted by \\(S = \\{P_r, l, y, x\\}\\). The curly brackets, {.} are meant to indicate that there are many samples which can be drawn from the one population. Note that \\(S\\) is the set of all possible samples, \\(s\\), selected under the probability scheme \\(P_r\\). The labels in the sample as well as the sample values for \\(Y\\) and \\(X\\) are denoted by lower case letters. In general, the convention is to use capitals when referring to the population but lower case for the sample. The term \\(P_r\\) indicates the probability, \\(P_r(s)\\), of the sample \\(s\\) being selected.\nAnother way to look at this is to consider the probability that a particular element, or label, be included in the sample. Let \\(\\pi_{i}\\) be the inclusion probability for the \\(i^{th}\\) label. A special but common occurrence is the equality of the \\(\\pi_{i}\\) for \\(i\\) from \\(1\\) through \\(N\\) and, in this case, the \\(P_r(s)\\) will be the same for all \\(s\\) in \\(S\\). In this case, the selection is said to be EPSEM, standing for equal probability of selection, or it is said that the elements are self weighing as the probabilities do not depend on the particular labels which have already been selected. The most obvious example of a self weighting sample is that of a simple random sample but multistage samples can also be arranged with unequal probabilities at each stage but such that the inclusion probabilities are equal."
  },
  {
    "objectID": "studyguide/1-data-collection.html#errors-in-samples",
    "href": "studyguide/1-data-collection.html#errors-in-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Errors in Samples",
    "text": "Errors in Samples\nSamples will be liable to the same kind of biases as a census as well as others inherent in the sampling process. Major sources of biases in samples are classified in Figure 3. On the one hand, the biases can be split into those which occur at the time of selecting the sample, while others occur at the stage of collecting and operation on the data. On the other hand, a division can be made into sampling and nonsampling, depending on whether the bias occurs in a random or fixed manner. The nonsampling errors can be further subdivided according to whether an observation is made or not.\n\n\n\nFigure 3: Classification of Sources of Survey Biases\n\n\nBriefly, the cells in Figure 3 refer to the following problems:\n\nFrame inefficiencies: To select a sample, a frame, which is often a listing but may be such as a map, is used. If certain items are missing or occur more than once, these items and the others in the frame will have a probability of selection which is different from what it should be.\nBiased estimation: In this course, only unbiased estimates such as the sample mean are considered.\nNon-observation: Non-observation could be due to frame inefficiencies or not being able to locate the item.\nNon-response: The item, say a particular person, may be located but he or she may be not at home or refuse to answer all or some of the questions.\nPurposive Selection: When selection occurs deliberately rather than at random then biases will often intrude. In quota sampling, the interviewer is told to contact so many people over 45, or male, or in some other category. The personal choice involved in this selection could introduce bias.\nInterviewer bias, coding errors: Certain characteristics of the interviewer may incline the subject to respond in a certain way. With another interviewer, the subject may respond differently. Once the data is collected, errors can occur in an amazing number of ways in coding the answer, transcribing to summary sheets, entering the data into a computer, as well as error in computer programming.\n\nOther errors are termed random or stochastic in that probabilities are involved so that samples, but not censuses, may suffer from these errors. The assumption is made that the measurement is correct but the estimator varies depending on the sample which has been drawn. Rather than giving the name error to this outcome it would be preferable to speak of sampling variation.\nOur aim in sampling and in all statistical estimation, is to find an estimate which is close to the population parameter. Of course, we have a problem in defining “close” as we do not know if our sample is representative of the whole population or whether it is an unusual one. If the sample yields an estimate of the population parameter we can at least talk about the expected or average distance of the estimate from the parameter.\nAn illustration of the ideas of bias and variance is that of shots fired at a target. If the shots are widely scattered, they exhibit a large variance. If they are spread around the target so that on average they are equally scattered around the bulls-eye (neither too high nor too low etc.), we could say they are unbiased shots of the bulls-eye. The four possible cases are shown in Figure 4).\n\n\n\nFigure 4: Bias and variance\n\n\nNotice that bias can occur in census as well as sample data, but we can only talk about variance in a probability sample. From a single sample yielding one value of a statistic, it is not possible to estimate the bias in the statistic but an estimate of the variance of the statistic can be obtained from the variation in the sample.\nEach sample is not an exact match of the population so that the resulting estimator will invariably differ from the population parameter it is estimating. As the parameter is unknown, the actual difference between the estimator and the parameter is also unknown so that it is important to have certain assumptions about the distributor of the variable under study and distribution of the resulting estimator. Mathematical theory is often available to indicate the amount of bias and variance in the estimator.\nUsually, a sampling method is chosen to ensure that the resulting estimator is unbiased. A peculiar sample does not necessarily mean that the estimator is biased but it may be difficult to convince others that this is so. For example, it may turn out that the sample consists only of females even though the population is evenly divided between the sexes. If females and males give similar responses to the variable under question, the strange composition of the sample in this case would not affect the value obtained for the estimator. On the other hand, a sample which seems to be representative of the population on certain variables may yield a strange value for the estimator. One must assume, for there is no proof as the parameters under study are unknown, that if the sample is chosen carefully, which usually means that an appropriate randomisation scheme is employed, the measurements and hence the estimate obtained will be representative of the population.\nOften, the sample and the estimator are chosen in such a way that the estimator is unbiased. The sampling scheme and the estimator can be considered as two sides of the same coin so that the estimator is chosen to fit the sampling scheme. Furthermore, the sampling scheme can be chosen in such a way that the variance of the estimator is a minimum subject to certain constraints such as cost and time. At times an estimator is chosen deliberately to be biased if in so doing the variance is made smaller. To compensate for this for it may be that a biased estimator will ensure that it is closer to the parameter of interest than an unbiased estimator. These considerations are of more interest to the analysis of data than to their collection; so this topic will not be considered at length here.\nIn sample surveys, an estimator may turn out to be biased because the probability of selection of certain elements is different to what the researcher thought. If the frame used to select people is a list of homeowners, wealthy owners would have a higher probability of selection for they are more likely than less well off people to own more than one house. Consequently, an owner of multiple houses would have a high probability of selection which would bias the estimate towards such people unless the multiple ownership is taken into account. This source of error is termed frame inefficiency and it is likely to arise whenever there are discrepancies with the sampling frame."
  },
  {
    "objectID": "studyguide/1-data-collection.html#simple-random-sample",
    "href": "studyguide/1-data-collection.html#simple-random-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\nIn our lottery example, each of the 1,000 tickets has the same probability (1 in 1,000) of winning any one prize and, indeed, of winning any one of the 3 prizes (3 chances in 1,000, that is, 1 chance in 333).\nIf the researcher has no information about the sampling frame, except perhaps the number of elements \\(N\\) present, simple random sampling may be a feasible approach. It is common in experimental design when treatments are allocated to experimental units. In quality control, simple random sampling is often used.\nIf the population is large, at least in relation to the sample size, it would usually be inefficient to use simple random sampling for it would require considerable time and cost to select and contact the elements of such a sample. If the sampling fraction is large, (say) more than ten percent, it may be reasonable to use simple random sampling as it is likely that the sample will be spread reasonably evenly throughout the population.\nThe more additional information available, the more structured the sampling design can be. The additional information may be of an exact dichotomous or polychotomous nature such as the natural division of the frame into three disparate geographical areas. The additional variable may be continuous such as the age structure of the frame. In other cases, the information may be assumed rather than known for certain. For example, the educational level achieved by every person in a frame may be listed but this information may be a few years out of date. In these cases, a sample survey may be taken based on the age structure of the frame, but the conclusions will only be valid to the extent that there has been no change in this age structure.\nThe additional information can be the basis for the logistics of the survey as different parts of the frame may require more intensive sampling or different methods of organisation. For example, if the frame covers rural and urban areas, the urban sectors may be reached by public transport, but respondents may not be at home during the day as they are likely to be at work. The rural areas may involve other difficulties due to the scattered nature of the population.\nThe obvious danger is that sampling decisions made for administrative ease may hinder accumulation of data and may discredit inferences drawn from the resulting sample. For example, one must query whether the responses collected by one interviewer are likely to be considerably different from those of another, whether data collected by face-to-face methods can be aggregated with data collected by telephone interviews; whether data collected on some elements can be assumed to be similar to other data collected at a later time; whether responses to questions which must be translated into different languages, or explained in simpler English to someone of limited education or facility in English, can give responses which can be aggregated with responses to these same questions when not translated. There are no easy answers to questions such as these but researchers should be aware of such difficulties. Whenever possible, the peculiarities in the collection method should be noted and made available to the reader so that he/she can better judge the conclusions of the survey.\nEstimators with a low variance are likely to be more accurate in that they reflect population values more closely. The use of additional information can lead to a reduction in the variance of estimators such as sample means and sample proportions. This gain in efficiency will depend on the strength of the relationship between the additional information and the variable under study. At times, this relationship may be estimated but, at other times, it must be taken for granted.\nIf the population is small but there is no information available on other aspects of the population, a simple random sample may be appropriate. Other sampling schemes may be used, though, for some of the following reasons:\n\nAdministrative problems\nIf the population size is large, it will be rather clumsy to try to select a simple random sample. If the population is spread out geographically, then considerable cost may be involved unless the sample is grouped in some way.\n\n\nImprove accuracy\nUnder certain conditions, or assumptions, it may be possible to use a scheme which leads to more accurate estimates - accurate in the sense that the variance and/or bias is reduced.\n\n\nPolitical reasons\nA simple random sample of (say) supermarkets in New Zealand may, by chance, consist only of stores in Auckland. The results of this survey may be viewed very sceptically by readers and, more importantly, by those who are funding the survey."
  },
  {
    "objectID": "studyguide/1-data-collection.html#stratified-sampling",
    "href": "studyguide/1-data-collection.html#stratified-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nStratified sampling is a very versatile approach. It can be easy to implement, and can also lead to more efficient estimation of parameters. With stratified sampling, the population is divided into groups called “strata”, and then random samples are taken from each stratum. The advantages of this approach are:\n\nIt is a flexible approach, as strata can be designated in different ways.\nEach stratum can be treated as a separate population so that strata could be surveyed at different times and even different ways. For example, a survey in NZ could be divided into north and south island, urban and rural; the rural sample could be contacted by telephone but the urban by face-to-face interviews. One stratum could involve systematic sampling; other involve simple random sampling.\nIf we have information regarding the variation in strata then the resulting estimator (of the population mean for instance) could be more precise (less variance) than in simple random sampling if the more variable strata are sampled more intensely.\nIt is probably the most common method employed in sample surveys.\nThe sample is spread throughout the population.\nThe variance of an estimate from stratified sampling is usually equal to, or less than, the variance of an estimate from simple random sampling. It would be sensible to sample more heavily the larger strata. This is the approach taken in proportional (stratified) sampling in which the sample size in each stratum is proportional to the size of the stratum. Proportional sampling is appropriate when all the strata are equally variable. When this is not the case the more variable strata are sampled more heavily."
  },
  {
    "objectID": "studyguide/1-data-collection.html#cluster-sampling",
    "href": "studyguide/1-data-collection.html#cluster-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nFor some investigations, it is advisable to consider the population as being composed of clusters or groups and to proceed with the sample survey by selecting certain clusters and collecting measurements from (usually) a random selection of the elements within the chosen clusters. Cluster and stratified sampling differ in that all the strata are sampled whereas only some of the clusters are sampled. It is usual to have a large number of small clusters in cluster sampling but a small number of strata in stratified sampling. Consider a sample survey of the pupils in a school. The classes within the school may be considered as possible strata or possible clusters. There are advantages for cluster sampling in this case.\n\nEase of administration. It will be easier to select certain classes in a school (say) and interview a random sample or all of the students in those classes rather than selecting students from all the classes in the school.\nA listing of all the children is only necessary for the selected classes.\nConfidentiality and/or independence of response may be aided by this approach. It would take longer to collect information from all the classes in the school and the longer it takes the more likely it is that respondents will talk among themselves about the questions before the researcher can collect his/her information. If the whole class is presented with the questionnaire at the same time, confidentiality is more likely to be preserved.\nLocating elements (students) is easier, travel (between classrooms) will be reduced, and it is easy to divide responsibility between interviewers.\n\nIn some cases, it makes more sense to collect measurements on whole clusters rather than just a few individuals within each selected cluster. This is probably true in the case of the above example. Surveys on TV watching collect information on a family is another example where this is probably true. If the TV set is on, it may be difficult to pin down which members are watching as some may claim not to be interested (although they may know quite a lot about that program). Alas, there must be some payment for these benefits and this is reflected in the higher variances of cluster sampling estimators over those of simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#systematic-sampling",
    "href": "studyguide/1-data-collection.html#systematic-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn this section, we shall only consider a particular type of systematic sampling, namely that which includes a component of randomness, as this allows probability statements to be made.\nIn the simplest presentation, the frame is assumed to consist of \\(N\\) elements where \\(N\\) = \\(kn\\) with \\(n\\) being the required sample size. A random digit, \\(i\\), in the range of l through \\(k\\) is chosen as a starting point and then every \\(k^{th}\\) element from then on is placed in the sample. The sample then consists of the elements \\(i, i + k, i + 2k,\\) etc. Clearly this approach is likely to produce a sample which is evenly distributed over the entire sampling frame, and it is therefore likely that the sample will be representative of the population. This is particularly true when the sampling frame is sorted geographically. However, when this is not the case, the variance of the estimators obtained using systematic sampling will generally be greater than those obtained for simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "href": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Probability-proportional-to-size (PPS) Sampling",
    "text": "Probability-proportional-to-size (PPS) Sampling\nProbability-proportional-to-size sampling, or PPS sampling, can be thought of as an extension of stratified sampling. Stratified sampling allows the flexibility for the probabilities of selection to vary with the size and characteristics of the strata. PPS sampling takes this a step further so that each element has its own probability of selection.\nThe procedure requires that an associated variable be known so that each element in the population has a measure of size associated with it. This variable may be a related variable whose values are known from a previous census. For example, auditors usually check only a sample of all the accounts in existence. The size of each account is crucial. So the large value accounts should be checked more intensely than the smaller accounts. This is achieved by giving each account a probability of selection which is proportional to its value. This is called Dollar-Unit Sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "href": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Estimates for Common Sampling Methods",
    "text": "Estimates for Common Sampling Methods\nAssociated with each sampling method is a particular form of estimator for the population parameters of concern. If an element has a high probability of selection, its effect must be reduced when an estimate is formed or, otherwise, the estimate will tend to be too large.\nIf the \\(i^{th}\\) observation is \\(y_{i}\\) and the probability that this element is chosen into the sample is \\(\\pi _{i}\\), it turns out that:\nEstimate of population total = \\(\\sum \\frac{y_{i} }{\\pi _{i} }\\) = sum(each observation in sample/its probability of selection)\nThis is easy to show with simple random sampling when \\(\\pi _ i =\\frac{n}{N}\\) for \\(i=1,2,...n\\). Let \\(Y\\) = population total, \\(\\bar{Y}\\) = population mean, and \\(Y=N\\bar{Y}\\). We estimate \\(\\bar{Y}\\) by \\(\\bar{y}\\) = mean of the sample and we estimate \\(Y\\) by \\(N\\bar{y}\\). If we write the estimate of the population total as \\(\\hat{Y}\\), we have: \\[\\hat{Y}=N\\bar{y}= N\\sum \\frac{y_{i} }{n} =\\sum \\frac{y_{i} }{\\left({\\tfrac{n}{N}} \\right)}\\] Notice that the summation is over all observations in the sample. For simple random samples, the probability that each observation is selected is \\(\\frac{n}{N}\\) so that the estimate of the total is, indeed, the sum of each observation divided by its probability of selection.\nFor stratified sampling with \\(k\\) strata, the estimated total and mean are, respectively \\[\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k}\\] and \\[\\bar{Y}=\\left(N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k} \\right)/N\\] Assume that stratum \\(j\\) contains \\(N_{j}\\) elements and that the sample size for this stratum is \\(n_{j}\\), then the probability that an element in the \\(j^{th}\\) stratum is selected is \\(\\frac{n_{j} }{N_{j} }\\). As an example, suppose that a stratified sample of adult males, in full time employment in NZ, was taken to determine the number of working days lost in the previous year and also their party political preference. In this hypothetical example, the strata are urban versus rural. The results were shown in Figure 5).\n\n\n\nFigure 5: STRS estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate of total days lost \\(\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} = 400,000\\times 5 + 100,000\\times 2 = 2,200,000.\\)\nThe estimate of the total number who prefer Labour is \\[\\hat{Y}=N_{1} p_{1} +N_{2} p_{2} = 400,000 \\times 0.5 + 100,000\\times 0.3 = 230,000.\\] Notice that proportions are means and fit into this structure quite naturally by defining\n\\[x_i = \\left\\{ {\\begin{array}{*{20}c} 1 & {{\\text{if the element has the property understudy}}} \\\\\n0 & {{\\text{otherwise}}} \\\\ \\end{array}} \\right.\\]\nNotice also that we could estimate \\(\\bar{y}\\) by \\(\\hat{Y}/N\\) so that:\nEstimate of total days lost per person = \\(\\hat{Y}/N\\) = 2,200,000/500,000 = 4.4.\nEstimate of proportions who prefer Labour = \\(\\hat{Y}/N\\) = 230,000/500,000 = 0.46.\nThese same formulae for estimating the population total and the population mean are appropriate for probability proportional to size (PPS) sampling. However, sampling without replacement makes the formulae rather complicated so we will not consider the details here."
  },
  {
    "objectID": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "href": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Multistage Designs and EPSEM",
    "text": "Multistage Designs and EPSEM\nLarge, and even moderately sized, sample surveys are often multistage surveys. The probability that an element is selected becomes the product of the probabilities of it being selected at each stage. As a simple example, consider 3 schools having 100, 200 and 300 students, respectively, and suppose that 12 students are to be selected from one of these schools. This could be carried out in a two-stage survey with the first stage being the selection of one school by probability proportional to size (PPS) sampling. In the second stage, 12 students are selected from the chosen school. Let \\(\\pi _{a}\\) be the probability of choosing a particular school and \\(\\pi _{b}\\) the probability of choosing a particular student (given that his/her school has been chosen). The probability of selecting a particular student is \\(\\pi_{a} \\times \\pi_{b}\\); see Figure 6.\n\n\n\nFigure 6: PPS example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, each student has the same probability of being selected which has been brought about by the PPS sampling of the schools. It would be possible, in this small example, to use only a one stage procedure such as a simple random sample of 12 students from a list of the 600. Alternatively each school could be viewed as a separate stratum and a stratified sample of students could be chosen. Of course, if 4 students are chosen from each school, those in the smallest school would have a better chance (4 chances in 100) of being selected than those in the largest school (4 chances in 300). On the other hand, if 2 are chosen from the first school, 4 from the next and 6 from the largest then each student will have the same probability of selection.\nThese EPSEM, equal probability of selection methods, have the advantage that it seems “fair” to allow each person the same probability of selection but, also, it makes for simpler and neater formulae for estimates and their variances."
  },
  {
    "objectID": "studyguide/1-data-collection.html#other-sampling-methods",
    "href": "studyguide/1-data-collection.html#other-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Other Sampling Methods",
    "text": "Other Sampling Methods\n\nVolunteers\nSamples are often selected because they are easy to obtain. Many psychology experiments referred to in the literature make use of long suffering students in introductory psychology courses. Quite often, volunteers are sought for experiments. Indeed, even when a probability sampling scheme is used, some subjects will withdraw so that the actual respondents are, in a sense, volunteers. Sometimes, people are accosted in the street or supermarket by market researchers or media reporters. The sample is by and large an opportunity sample for it is difficult to obtain a random sample under such conditions. Any generalisations from such a sample should be treated with some suspicion.\n\n\nSnowball samples\nA snowball sampling approach is used by medical authorities to track down possible sufferers of sexually transmitted diseases. All former contacts of a known sufferer are contacted, if possible, and former contacts of theirs also contacted. Both volunteer and snowball sampling are purposive methods in that no randomisation is employed.\n\n\nQuota samples\nAnother purposive sampling method is that of quota sampling. The idea is to make the sample representative of the population at least for certain variables. The quotas required in the sample may be decided by partitioning the sample into males and females, and also partitioning it by age groupings. The interviewer would be required to find the stipulated number of respondents in each cell of the sample. Although the sample is representative on the given variables, one is not sure how representative it is in terms of the responses obtained. Furthermore, being a purposive sample, it is difficult to make probability statements about estimators.\n\n\nEstimating wild life populations\nIt would be difficult to estimate the number of deer in a large untamed area such as some parts of the South Island. The target population of deer is very large but also is very hard to locate. One sampling approach would be to use recent deer droppings as a sampling frame, and for researchers to walk in a straight line noting the number of occurrences of droppings in (say) a 2 meter wide path. If the researchers are spaced at intervals of 20 meters then one could assume actual number of droppings of 10 times that obtained in the sample. Further assumptions would be needed to extrapolate from the number of droppings to the number of deer. A more direct approach would be afforded by a capture-recapture method in which a sample of animals is captured, tagged, released and then at a later date a further sample is captured. The proportion of tagged animals in this second sample indicates the size of the population. A number of assumptions must be made such as the uniform spread of the tagged animals throughout the areas of the second sampling. The initial capturing may affect the animals in unusual ways, by making them shy tending to avoid capture in future or making them tamer so that they do not fear capture as much on a subsequent occasion. The method of tagging may also be disadvantageous causing stress. Other assumptions need to be made about the distribution of the untagged and tagged animals. A number of different estimators have been suggested to obtain unbiased and efficient estimators of the number of animals in the population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "href": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Sample Size & Some Practical issues",
    "text": "Sample Size & Some Practical issues\nFigure 7 shows a fictitious example of how samples of four people are drawn from a population of 20 people.\n\n\n\nFigure 7: Methods of Sampling: A toy example\n\n\nIn reality, sampling issues are not as simple as in the above example. Most large studies require quite a complex form of sampling involving layered sampling frames and combinations of design strategies. A typical design for an urban study is described below. The first layer consists of 20 regional areas; the second layer consists of all the municipalities within each regional area; the third layer consists of all the suburbs within each municipality; the fourth layer consists of all the city blocks within each suburb and the fifth layer consists of all the households within each city block. Judgement may be used to select only 5 of the 20 regional areas, one typical of the central regions, one each typical of the northern, southern, eastern and western regions. Next a systematic sample of municipalities is chosen (from a list sorted by postal code) for each of these regional areas, a sample of suburbs is chosen (stratified by socio-economic level) within each municipality, a simple random sample of city blocks is chosen within each suburb and a simple random sample of households is chosen within each city block. Such designs aim to be cost effective in terms of producing a representative sample at reasonable cost. However, complex designs require complex analyses, so try to keep things as simple as possible. The important criteria for the selection of an appropriate sample design are stratification, randomisation and cost. Note that non-response also has cost implications in that a survey with a high non-response rate is more costly than a survey with a good response rate. Of course the sample size also has an influence on cost and this provides opportunities for sample design/sample size trade-off. In the following discussion we assume that there is a 100% response rate. In other words sample size refers to the number of (good) sample elements. When only a 50% response rate is expected for a mail survey it means that the number of questionnaires distributed should be double the required sample size.\n\nSample Size\nUnfortunately, it is the available funds rather than research requirements that often determine what the sample size will be. We will therefore not ask the question How large a sample do we need? Instead we will ask What can a certain sample size \\((n)\\) do for us? The answer to this question depends on the sample design that is to be used. A quantity called a design effect \\((d)\\), related to the variance of the estimate, is used to measure the quality of a sample design. The larger the design effect (variance), the worse the design in terms of producing representative samples. The design effect is used to compute an effective sample size \\((e)\\) from the actual sample size \\({n}\\) using the formula \\({e=}\\frac{n}{d}\\). The effective sample size provides a more meaningful indication of the number of independent (useful) responses than the actual sample size. Figure 8 roughly shows the typical ranges for the design effect \\((d)\\) associated with the various sample designs.\n\n\n\nFigure 8: Crude comparison of effective sample sizes\n\n\nFor a simple random sample the design effect is treated as one. So, for a simple random the effective sample size is the same as the actual sample size. Each observation in the sample is doing a full job. You will see that a quota sample typically has a design effect of 2, producing an effective sample size, which is half the actual sample size. This means that each observation in a quota sample is only doing half a job. However, in the case of a stratified random sample, the design effect is typically less than one indicating a very good design and producing an effective sample size that is greater than the actual sample size \\((n)\\). In this case each observation is working overtime. Clearly a design effect of more than one means that the sample is not as efficient as a random sample would be, while a design effect of less than one means that the sample if more efficient than a random sample would be.\nMost opinion polls give a margin of error (this term will be formally defined later on) when they give the result of their survey. This is the level of inaccuracy associated with the sample result. For example, we hear that a poll indicates that a proportion \\(p\\) of 40% of voters support National but the margin of error could be 6.1% if the effective sample size is only 300. When the purpose of the research is to estimate some proportion (i.e. percentage) then the margin of error can be calculated using an approximate formula (namely \\(2\\sqrt{p(1-p)/e}\\) where \\(p\\) is the expected proportion and e is the effective sample size). As indicated in Figure 9, the margin of error declines as the effective sample size increases. By setting \\(e\\) equal to the actual sample size divided by the design effect we can estimate the margin of error for any design. If the margin of error is too big it means that the sample size needs to be increased. Unfortunately this is usually the case. This example is hypothetical but it illustrates the general relationship between the margin of error and the effective sample size.\n\n\nCode\nlibrary(tidyverse)\n\ndfm &lt;- tibble(\n  \"Sample size\" = 30:1000,\n  \"Margin of error\" = 100/sqrt(`Sample size`)\n)\n\ndfm |&gt; \n  ggplot() +\n  aes(x = `Sample size`,\n      y = `Margin of error`) +\n  geom_path() +\n  theme_minimal()\n\n\n\n\n\nFigure 9: Margin of Error and Sample Size\n\n\n\n\nIn Figure 9, the effective sample size required for a margin of error of 10% is 100. Since the actual sample size equals the effective sample size for a simple random sample this means that the size of the simple random sample must be 100 if the margin of error is 10% when a population proportion is estimated. However the effective sample size depends on the sample design. The actual sample size required for a given margin or error also depends on the sampling design. The actual sample size required in order for the margin of error to be 10 is highest for quota samples at 200 and lowest for a random stratified sample at 80. For the random cluster sample, an actual sample size of 120 will produce this margin of error.\nThe following rule of thumb can be used when sample results are to be generalised to a population. Firstly, if fairly sophisticated (multivariate analysis) is required then the effective sample size should be at least 200. Secondly, there should be at least four sample elements for every variable included in the study. However, this rule and the above formula are irrelevant when the population is relatively small or when we do not wish to generalise our findings to the population. In studies of this nature smaller sample sizes are sufficient. But the relative percentage to the size of the population should be large. For example if the population is of size only 100, an effective sample of size 80 is needed to achieve a certain degree of margin of error when the true population proportion is about 0.5. However, only an effective sample of size 370 is needed for the same margin of error if the population size is 10,000. The effective sample size needs to be increased only to 385 if the population size is 100,000. The moral is that percentage sampling is a fallacy, and that the absolute sample size is more important than sampling a fixed percentage of the population."
  },
  {
    "objectID": "studyguide/2-eda.html",
    "href": "studyguide/2-eda.html",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "“Data has no meaning apart from its context.” — Walter A Shewhart"
  },
  {
    "objectID": "studyguide/2-eda.html#error-checking",
    "href": "studyguide/2-eda.html#error-checking",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Error checking",
    "text": "Error checking\nThe data are assumed to be in some sort of electronic form; e.g. text file, spreadsheet, database, etc. Before analysing the data, it is prudent to check for mistakes in the data. If one has access to the raw data (e.g. returned questionnaires), one can check observations by hand. If the raw data are not available, then the best one can do is to look for observations whose values are out of allowable ranges, or are very suspicious. A common problem occurs with the coding of missing information. Missing values are commonly recorded as a blank, *, -1, -999, or NA. Only one type of coding should be used. The use of NA is preferred since this is easier to spot. The use of blanks is particularly dangerous when one is exporting data from a spreadsheet to a text file. When the data from the text file is read into a computer package, a missing value may be skipped and the value from the next observation read in its place. When the data set is large, looking at a data set one observation at a time may not be feasible. In this case, plotting the data may show any observations with illegal values."
  },
  {
    "objectID": "studyguide/2-eda.html#understanding-data",
    "href": "studyguide/2-eda.html#understanding-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Understanding data",
    "text": "Understanding data\nAfter any data errors have been found and corrected, the data understanding stage commences. This stage involves tabulating, summarising, and plotting the data in many ways to gain insight into the data set. We typically want to:\n\nexamine the distributions, or shapes, of individual variables;\ncompare different groups of data, with special emphasis on location and scale;\nand discover any trends and relationships exhibited between pairs of variables.\n\nAgain, don’t fall into the trap of data snooping, where you develop and test hypotheses with the same data. For instance, suppose that boxplots of the income of individuals in different age groups show that youngest and oldest age groups are the most different. If one elects to perform the hypothesis test for the difference between these two groups based on observations made during EDA, the resulting \\(p\\)-value is not valid.\nIn the sections below, we briefly review many common tools used to visualise data during EDA."
  },
  {
    "objectID": "studyguide/2-eda.html#bar-chart",
    "href": "studyguide/2-eda.html#bar-chart",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Bar chart",
    "text": "Bar chart\nThe rules for constructing a bar chart are very simple and perhaps they can be reduced to a single rule and that is to make the height of each bar proportional to the quantity displayed. Consider the dataset Snodgrass available in the R package archdata. This dataset contains information on the size, location and contents of 91 house pits at the Snodgrass site which was occupied between about CE 1325-1420. The variable names and their description are given below:\n\nEast: East grid location of house in feet (excavation grid system)\nSouth: East grid location of house in feet (excavation grid system)\nLength: House length in feet\nWidth: House width in feet\n\nSegment: Three areas within the site 1, 2, 3\n\nInside: Location within or outside the “white wall” Inside, Outside\n\nArea: Area in square feet\n\nPoints: Number of projectile points\n\nAbraders: Number of abraders\n\nDiscs: Number of discs\n\nEarplugs: Number of earplugs\n\nEffigies: Number of effigies\n\nCeramics: Number of ceramics\n\nTotal: Total Number of artifacts listed above\n\nTypes: Number of kinds of artifacts listed above\n\nThe data from 91 house pits at the Snodgrass site were reported by Price and Giffin in 1979. The layout of the houses follows a grid pattern with the long axis oriented north-east surrounded by a fortification trench. There is also evidence of an interior wall that may have separated the houses inside that wall from those outside the wall. Price and Griffin use differences in house size and artifacts composition to suggest that those distinctions may have reflected rank differences between the occupants of the two areas.\nThe distribution of number of Ceramics found can be displayed in the form of a bar chart; see Figure 1.\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Ceramics) + \n  geom_bar()\n\n\n\n\n\nFigure 1: Bar Chart of Ceramics counts\n\n\n\n\nThe bar graph illustrates clearly that no ceramics were found in many houses but a lot were found in a few houses. This may be due to status and wealth of the occupants.\nNote that we are treating number of Ceramics as ordinal in the above graph. For nominal data software tend to order the categories alphabetically.\nHowever, a graph is not always the best way to display a small set of numbers. Graphs do have some disadvantages: they take up a lot of space, and they do not usually allow the recovery of the exact observations. For these reasons, particularly for a small data sets, a table is sometimes more effective. There were only three segment categories; and their counts can be simply displayed as a table.\n\n\nCode\ntable(Snodgrass$Segment)\n\n\n\n 1  2  3 \n38 28 25 \n\n\nIt would be interesting to see whether the discovery of ceramics is Segment dependent. In other words, we would like to explore two factors in display. This can be done in many ways (Figure 2).\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\np1 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) + \n  geom_bar() + \n  ggtitle(\"Bar plot with colour grouping\")\n\np2 &lt;- ggplot(Snodgrass) + \n  aes(x = Ceramics) + \n  geom_bar() + \n  facet_grid(vars(Segment)) + \n  ggtitle(\"Segment-wise Bar plots\")\n\np3 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) +\n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Barplot\")\n\np4 &lt;- ggplot(Snodgrass) +\n  aes(x=Ceramics, fill = Segment) + \n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Bar plot - flipped\") +\n  scale_fill_grey() + coord_flip()\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 2: Bar Chart of Ceramics counts by Segment\n\n\n\n\nIt is easy to spot that the Ceramics findings were largely from the first segment. This EDA conclusion can be drawn from all of the above plots. However, these bar plots do not rate the same as a “presentation style graphic”.\nTufte (2001) argues in favour of certain principles of graphical integrity for presentation style graphics. These are:\n\nThe representation of numbers in a graph should be directly proportional to the numbers themselves;\nClear and detailed labels should be used to prevent ambiguity;\nProvide explanations and important events pertaining to data on the graphic itself;\nThe graph should display data variation, not design variation—visual changes in the graph should be due to the data, not changes in way the data are drawn;\nThe number of dimensions used in the graph should not exceed the number of dimensions in the data;\nGraphs must not quote data out of context; for example, a monthly drop in the murder rate of a city should be plotted within the context of the murder rate over the last few years, or murder rates in comparable cities over the same period.\n\nA final point concerns the use of colour. In many sorts of graphs, data types are differentiated by the use of colour. This practice is perfectly legitimate, but one should be aware that a surprising proportion of people have impaired colour perception, such as being unable to distinguish between red and green (see here for colourblind-friendly pallettes). A remedy to these problems is to use different plotting symbols, line types, or shading, in conjunction with different colours. One should always make sure that the colours selected are clearly visible in the medium in which they are presented. Colours on a computer monitor can appear quite different when projected onto a screen in a lecture theatre, or when printed onto paper.\nAccording to Tufte (2001), a good graphical display should:\n\nDisplay the data;\nEncourage the viewer to concentrate on the substance of the data, in the context of the problem, not on the artistic merits of the graph;\nAvoid distorting, intentionally or unintentionally, what the data have to say;\nHave a high data to ink ratio; that is, convey the greatest amount of information in the shortest time with the least ink in the smallest space;\nEnable one to easily compare different pieces of data;\nReveal overall trends and patterns in the data that are not obvious from viewing the raw numbers;\nReveal detailed information secondary to the main trend; for example, the location of outliers, variability of the response.\n\nThis Chapter is largely concerned with EDA for discovery of patterns and peculiarities in a dataset. So we will not be too worried if the graphs produced are not aesthetically pleasing or does not meet a particular standard for a presentation style graph.\nThe end goal of most analyses is of course some form of written report or presentation. In either case, the findings of the study, should be supported by tables and graphs created during the EDA; these should clarify and support the final conclusions, not deceive or confuse the audience. One should refrain from showing all tables and plots that were created, but rather a careful selection that reinforces the main findings. The remaining figures can, if necessary, be placed in an appendix of the written report. The figures and tables that are shown should be well labelled and annotated. Abbreviations and cryptic variable names should be replaced by sensible labels and descriptions. The emphasis should be on conveying information rather than producing an eye catching plot. Special care should be taken to ensure that the table or graph cannot be easily misinterpreted.\nIn addition, a poorly made graph can be deceptive. This is a problem both in the understanding the data phase, and the presenting the data phase. One common mistake (but by no means the only one) in creating a graph is to use more dimensions to represent the data than there are data dimensions. For example, it is not uncommon to see a bar chart where the bars are drawn as three dimensional rectangular prisms. The false perspective actually obscures the differences in the heights of the bars.\nAnother pitfall is to use a clever graphic rather than one that clearly presents the data. The classic example of this is a pictogram, where pictures of varying size represent some quantities. Pictograms are often drawn dishonestly: to show a two-fold difference, each side of the picture is increased by a factor of two, so the area is in fact increased by a factor of four. Dishonesty in data presentation is clearly unacceptable. But even an honest pictogram is usually not the best way to present data. It is difficult to accurately judge differences between areas; which is what a pictogram asks the viewer to do."
  },
  {
    "objectID": "studyguide/2-eda.html#pie-charts",
    "href": "studyguide/2-eda.html#pie-charts",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Pie charts",
    "text": "Pie charts\nPie charts are a popular, though usually rather poor, way of presenting data. Like bar charts, they frequently use a large figure to represent just a few numbers. Pie charts also have a poor accuracy of decoding. That means people’s impression of the difference in size between two pie slices is often quite different from the actual difference in size. Pie charts use angles to represent percentages; people tend to underestimate the size of acute angles and overestimate the size of obtuse angles. The exploded pie chart makes it more difficult to accurately compare pie slices: the exploded piece always appears bigger. The three dimensional pie chart is even worse. Bar charts, dot charts, or tables should be preferred to pie charts."
  },
  {
    "objectID": "studyguide/2-eda.html#dotplot",
    "href": "studyguide/2-eda.html#dotplot",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Dotplot",
    "text": "Dotplot\nOne of the simplest kinds of graph for a numerical variable is the single line (or one dimensional) graph. Each observation is represented by a point (or a star * or a dot .) on the line. These graphs can be useful to see the rough distribution and gaps, if any, in the data. Figure 3 shows a dotplot of Length of the house pits for the Inside and Outside groupings. Evidently privileged tribes lived inside in general, and the length of their dwellings is longer.\n\n\nCode\nggplot(Snodgrass) +\n  aes(x=Inside, y=Length) + \n  geom_dotplot(binaxis='y', dotsize=.5) + \n  coord_flip()\n\n\n\n\n\nFigure 3: A typical dot plot for two groups\n\n\n\n\nOn reflection, the one dimensional dotplot gives rise to a second dimension wherever observations are identical or nearly the same. In a dotplot such points are usually stacked. As our eyes tend to pick up trends very easily, they tend to be distracted by the vertical changes. The main problem here is that some of the values which coincide by chance may be taken to represent real increases in density. To overcome this we try ‘jittering’ the data, which is equivalent to plotting a random number on the vertical axis against the data values on the horizontal axis. It is suggested that the vertical jitter be restricted to a small range so that the overall effect is that of a one-dimensional spread with greater, or lesser, density of points along the graph. Figure 4 shows a jittered dotplot. You can decide for yourself which strategy is better.\nClearly, the use of jittering is much more relevant for larger sets of data. Notice that with jittering we are interested in the relative density of the graph in different places rather than in trends.\nAs the vertical reading is determined at random our eyes may be distracted by a seeming trend which is mainly due to this randomness. For this reason, it is helpful to try more than one plot with different levels of jitter This is in the spirit of EDA, in which data is viewed from as many angles as is feasible. The other side of this coin is that conclusions from one display of data should be treated with some scepticism. The availability of computers allows data to be plotted, tabulated and transformed quickly, so we should look at it several ways to better appreciate the peculiarities inherent in our data.\nIn practice, one tends to avoid jittering except in (two-variable) scatter plots where there are many repeated \\((x, y)\\) points. Graphs with jitter are mainly needed when the data are heavily rounded, discrete, or grouped in some way, or there are a large number of points to be plotted.\n\n\n\n\n\nFigure 4: A jittered dot plot"
  },
  {
    "objectID": "studyguide/2-eda.html#histograms",
    "href": "studyguide/2-eda.html#histograms",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nThe histogram is a standard method of showing the distribution of a quantitative variable. A histogram is made by dividing the range of a variable into “bins” and counting the number of data points that fall within each bin. It is like a bar chart after forcing a quantitative variable into bins. It can be used for discrete ungrouped data, but a bar chart is generally more suitable for discrete data because histograms can mislead the reader into thinking that values exist other than at the centre of the intervals.\nFigure 5 shows the histogram of Length. Clearly the distribution is far from symmetric and left skewed. The bimodal pattern also suggests subgrouping.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length) + \n  geom_histogram(bins=12, \n                 color=\"black\", \n                 fill=\"white\")\n\n\n\n\n\nFigure 5: A typical histogram\n\n\n\n\nNotice that a different shape of histogram may result depending on the class intervals (bins) used (that is by choosing different class widths or different midpoints of the classes). This suggests that it may be wise to draw more than one histogram for a given data set particularly if statements are to be made about modes or gaps in the data. The guideline for the maximum number of intervals, \\(L\\), can follow the same formula as for stem-and-leaf displays. Some software use the Sturges (1926) formula namely \\[\\texttt{bin width} = \\frac {\\texttt{range}(x)} {\\log_2(n)+1}.\\] For fixing the width of the bins, the range of the data is divided by the sum of one and the base two logarithm of the sample size.\nThere are a few ways to scale the y-axis of a histogram, as shown in Figure 6:\n\nFrequency histograms show the counts in each bin; the heights of the bars sum to \\(n\\).\nRelative frequency histograms show the proportions or percent of data in each bin—the counts in each bin divided by the total; the heights sum to 1 or 100%.\nDensity histograms show the densities; the areas (heights \\(\\times\\) widths) of the bars sum to 1.\n\n\n\nCode\np1 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(bins = 20) +\n  ylab(\"\") + \n  ggtitle(\"(a) Frequency histogram\", \n          \"Heights of the bars sum to n\")\n\np2 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(count / sum(count)))\n    ) + \n  ylab(\"\") +\n  ggtitle(\"(b) Relative frequency histogram\",\n          \"Heights sum to 100%\") + \n  scale_y_continuous(labels = scales::percent)\n\np3 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(density))) +\n  ylab(\"\") + \n  ggtitle(\"(c) Density histogram\",\n          \"Heights x widths sum to 1\")\n\nlibrary(patchwork)\n\np1 + p2 + p3\n\n\n\n\n\nFigure 6: Three types of histograms\n\n\n\n\nIf, instead of drawing blocks over the class intervals as in a histogram, we join the mid points of the tops of the bars, we obtain a display known as a frequency polygon . If we use relative frequencies, it is a relative frequency polygon; see Figure 7.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length, y = after_stat(density)) +\n  geom_freqpoly( bins = 12, color = \"black\")\n\n\n\n\n\nFigure 7: A typical frequency polygon\n\n\n\n\nFigure 8 shows a kernel density plot, which is a method that produces a smoother density line.\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(Length) + \n  geom_density() + \n  geom_rug(alpha = .3, colour = \"darkorange2\")\n\n\n\n\n\nFigure 8: A typical smoothed density curve\n\n\n\n\nKernel Density Estimation (KDE) is generated by dropping a little mini-distribution, usually a normal curve, on each individual point and summing them up (Figure 9).\n\n\n\nFigure 9: A demonstration of kernel density estimation\n\n\nThese smooth plots are also useful to see the type of skewness involved, multimodality (several peaks) and unusual observations (or outliers). The disadvantage is that we may over-smooth and mask some of the subgrouping patterns; see Figure 10. Even though the two distributions appear to be similar, the length distribution for the Inside group has a larger mean/median etc (called the location parameters).\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Length, \n      colour = Inside) + \n  geom_density()\n\n\n\n\n\nFigure 10: Identification of Subgrouping"
  },
  {
    "objectID": "studyguide/2-eda.html#theoretical-distributions",
    "href": "studyguide/2-eda.html#theoretical-distributions",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Theoretical Distributions",
    "text": "Theoretical Distributions\nA histogram gives a general picture of the frequencies or relative frequencies of the data. If an observation is taken at random from the population from which the sample was drawn, the chance of it falling in any one of the class intervals should be approximately proportional to the area of the block over that interval (not exactly proportional because the histogram was only based on a sample from the population, not the population itself).\nIf we could draw a histogram for the whole population, then the probabilities would be exact. The relative frequencies form a probability distribution and if the class intervals are small enough the tops of the bars of the histogram would often approximate a smooth curve such as the density plot. In many cases this smooth curve is almost symmetrical with a rounded peak at the centre. One particular shape, fitting a certain mathematical formula, is a very good approximation to many sets of data. For instance, heights of people, weights of animals or yields of wheat all follow this distribution fairly well (although theoretically the distribution has an infinite range of possible values). One of the widely used theoretical distribution is called the Normal or Gaussian distribution but we will not bore you with its formula. However, most software can compute the tail areas (probabilities) under theoretical distributions. We shall study this distribution in greater detail in a Chapter 3. For the moment let us note that our EDA techniques should provide us with information about the parameters of the distribution, and we also want graphical techniques that display the continuous nature of the data.\nDiscrete data also have probability distributions and the bar graph gives a general picture of its shape. One commonly occurring discrete theoretical distribution is known as the Poisson distribution and approximates reasonably well the numbers of road accidents over a fixed time period. The main feature of a Poisson distribution is that there is a peak at the nearest integer below or equal to the average, and then a steady falling off. The mean is a parameter of the model, so we want our exploratory techniques to tell us about the mean, and to help us decide whether or not the Poisson model is appropriate for a given set of data. Theoretically, possible values are any whole positive numbers, but large numbers of accidents are very unlikely. Again, any graphical method we use must properly display the discrete nature of Poisson data. The other commonly adopted discrete distribution is the binomial distribution. This distribution is valid when the individual outcomes are classified into two categories such as Success and Failure."
  },
  {
    "objectID": "studyguide/2-eda.html#symmetry-plots",
    "href": "studyguide/2-eda.html#symmetry-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Symmetry plots",
    "text": "Symmetry plots\nA symmetry plot is based on the distance of left and right side points. If the underlying distribution is symmetric, the points on the left and right side would be roughly equidistant from the median. For the length data, we obtain the symmetry plot after writing a little R function as follows:\n\n\nCode\nsymmetryplot &lt;- function(x)\n{\n  require(ggplot2)\n  \n  x &lt;- na.omit(x)\n  m &lt;- length(x) %/% 2\n  sx &lt;- sort(x)\n  dfm &lt;- tibble( \n    X = median(x) - sx[1:m], \n    Y = rev(sx)[1:m] - median(x)\n    )\n  \n  dfm |&gt; \n    ggplot() + \n    geom_point(mapping = aes(x=X, y=Y)) + \n    geom_line(mapping = aes(x=X, y=X)) + \n    theme(aspect.ratio=1) + \n    xlab(\"Distance below median\") + \n    ylab(\"Distance above median\")\n}\n\n\n\n\nCode\nsymmetryplot(Snodgrass$Length)\n\n\n\n\n\nThe points do not follow the \\(45^\\circ\\) line where \\(Y=X\\); divergence from this line indicates a lack of symmetry.\nIf each half of the distribution is further divided in halves, we obtain the fourths or( hinges), as the medians for the lower and upper halves. The end points, the minimum and the maximum are called the lower and upper extreme values, respectively.\nThe R function obtains the set (minimum, lower hinge, median, upper hinge, maximum) in its function fivenum.\n\n\nCode\n# Five number summary\nfivenum(Snodgrass$Length)\n\n\n[1]  4.00 13.00 14.50 17.75 21.00"
  },
  {
    "objectID": "studyguide/2-eda.html#box-plots",
    "href": "studyguide/2-eda.html#box-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Box plots",
    "text": "Box plots\nA boxplot (or sometimes a box-and-whisker plot) is a visual display of the five-number summary, which can be used to assess the skew in the data. The components of a boxplot are explained in Figure 11.\n\n\nWarning in geom_segment(aes(x = fn[2], xend = fn[4], y = -0.1, yend = -0.1), : All aesthetics have length 1, but the data has 91 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = fn[2], xend = fn[2] - 1.5 * (fn[4] - fn[2]), : All aesthetics have length 1, but the data has 91 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\nFigure 11: Annotated boxplot\n\n\n\n\nThe box shows the positions of the first and third quartiles, or “hinges”, and the thick middle line shows the median. As 50% of the observations fall between the first and third quartiles, the position of the box is important and is the first thing that catches our eyes. If the median is in the centre of the box, this suggests that the distribution may be symmetric; if the median is near one end, the distribution is skewed. In particular, if the median is in the right side of the box, it implies that the middle 50% of the data are skewed to the left. For right skewed data, the median will be placed at the left end of the box (as against in the centre of the box).\nAfter having considered the ‘centre’ of a distribution, which we have measured by the median, and looked at where the bulk of the data lies, measured by the quartiles; then we are particularly interested in the strange, unusual values. They could be obvious mistakes (incorrectly coded, or decimal places in the wrong place, transposition of digits such as 91 instead of 19) or incorrect measurements or recording. These last two possibilities often occur before the data gets to the statistician so that it may be difficult to check whether these mistakes have occurred. Apart from obvious errors, we can only point to ‘far out’ values as being strange and we term them ‘outliers’. To help decide this matter, we define a few more terms, of which the most common one is the interquartile range or IQR, which is the distance between the first and third quartiles ( \\(Q3-Q1\\) ). Any observation which is more than \\(1.5 \\times IQR\\) (sometimes called the “step”) less than the first or more than the third quartile can be considered a possible outlier (Figure 11.\nBoxplots are ideal for comparing a few groups of data because they give a visual summary of certain main features of the distributions but they are not as cluttered as other displays. Common sense must prevail, for there is a limit to the amount of information that the eye can take in. Up to 10 or 12 boxplots can be plotted, preferably on the same scale; more than 12 could be confusing.\nShown in Figure 12 are the boxplots of Length for the Inside-Outside groupings. Note that the boxplots have been constructed on the same scale; that is, they have common (Length) axis.\n\n\nCode\nggplot(Snodgrass) + \n  aes(y = Length, \n      x = Inside, \n      col = Inside) + \n  geom_boxplot() + \n  coord_flip()\n\n\n\n\n\nFigure 12: Boxplots\n\n\n\n\nThe boxplots in Figure 12 show that the general shapes of the distributions of Length are broadly similar. In each case, the median is near the centre of the box. The box of the Outside group is entirely to the right of the box of the Inside group, indicating that the distribution of Length is higher for the Outside group. We can also see that the Inside group may have greater spread than the Outside group."
  },
  {
    "objectID": "studyguide/2-eda.html#cumulative-frequencies",
    "href": "studyguide/2-eda.html#cumulative-frequencies",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Cumulative frequencies",
    "text": "Cumulative frequencies\nWhen the data consists of individual points rather than grouped data, we first order the data from smallest to largest and we can plot the row numbers against these.\nFor the Length data from the Snodgrass dataset, we have shown the ordered values earlier. If we assume that these are evenly spread throughout the distribution, these 91 values divide the distribution into 91 even parts each containing a relative frequency of 1/91 = 0.010989. In the absence of information about Length of other house pits, the above assumption seems reasonable. It would have even stronger justification if this sample of 91 were chosen at random from the population of the of all house pits, but this stretches the imagination somewhat as these 91 observations are just a chance archaeological find of what was preserved. Nevertheless, we have no real reason to consider that they are not representative of the whole population Snodgrass house pits.\nTo find the cumulative relative frequency, we divide the column of row numbers by 91. There is a slight problem in that we do not know the smallest, and the largest Length. Hence we make an arbitrary decision that the smallest is likely to be 4 feet with cumulative relative frequency of zero and the largest 21 feet with a cumulative frequency of 1.\nWith the Length data, there is a slight hitch in that there is duplication of the values. This is not really a problem, so long as we ignore the cumulative frequency in the table which corresponds to each first repeated value. The frequency jumps by one at the value of each observation except when there is a repetition in which case it jumps by the number of repetitions. Having obtained the cumulative relative frequencies, we can plot these against the ordered Length values.\n\n\nCode\nggplot(Snodgrass) + \n  aes(Length) + \n  stat_ecdf()\n\n\n\n\n\nFigure 13: Cumulative Relative Frequency Curve\n\n\n\n\nFigure 13 shows the Cumulative Relative Frequency Curve. The procedure we described is a very basic one, and R software will get a better one where we make more assumptions on how to relate the observed data to the cumulative relative frequencies."
  },
  {
    "objectID": "studyguide/2-eda.html#quantiles",
    "href": "studyguide/2-eda.html#quantiles",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Quantiles",
    "text": "Quantiles\nAn important use of cumulative relative frequency graph is to find the quantiles of the distribution which is a general term for percentiles, deciles etc. For example, the \\(50^{\\text th}\\) percentile or the 5\\(^{\\text th}\\) decile (tenth) is the median which has 50% of the distribution up to, and including it. Any desired percentile can be found from the graph by drawing a horizontal line at the desired cumulative relative frequency and noting the value at which it cuts the curve. Figure 14 illustrates how the 30\\(^{\\text th}\\) percentile can be interpolated for the Length variable. It is desirable to use software to draw the cumulative relative frequency curves or estimate the quantiles.\n\n\nCode\nq30 &lt;- quantile(Snodgrass$Length, 0.3)\n\nggplot(Snodgrass) + \n  aes(Length) + \n  stat_ecdf() + \n  geom_segment(\n    aes(x = q30, \n        y = 0, \n        xend = q30, \n        yend = .3),  \n    colour=2, \n    arrow = arrow(length=unit(0.30,\"cm\"), \n                  ends=\"first\")\n    ) + \n  geom_segment(\n    aes(x = 0, \n        y = .3, \n        xend = q30, \n        yend = .3), \n    colour=2, \n    arrow = arrow(length=unit(0.30,\"cm\"))\n    ) + \n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) + \n  ylab(\"ECDF(Length)\")\n\n\nWarning in geom_segment(aes(x = q30, y = 0, xend = q30, yend = 0.3), colour = 2, : All aesthetics have length 1, but the data has 91 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.3, xend = q30, yend = 0.3), colour = 2, : All aesthetics have length 1, but the data has 91 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\nFigure 14: ECDF Curve for Length data\n\n\n\n\nThe 25\\(^{\\text th}\\) percentile is known as the lower quartile and the 75\\(^{\\text th}\\) percentile is known as the upper quartile. The inter-quartile range (IQR) is defined as the difference between the upper and lower quartiles. The lower and upper quartiles will slightly differ from the lower and upper hinges for small data sets due to the differences in estimating the cumulative relative frequency function known as the empirical cumulative distribution function (ECDF).\nThe R function quantile() will get the quantiles easily. The following output shows the percentiles for Length data.\n\n\nCode\nquantile(Snodgrass$Length, seq(0,1,0.1))\n\n\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n 4.0 11.5 12.0 13.5 14.0 14.5 16.0 16.5 18.0 20.0 21.0"
  },
  {
    "objectID": "studyguide/2-eda.html#quantile-quantile-plots",
    "href": "studyguide/2-eda.html#quantile-quantile-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile Plots",
    "text": "Quantile-Quantile Plots\nAt times, two groups may have different numbers of observations but they may be similar in some ways and it may be appropriate to compare them. This comparison can be done in several ways but Q-Q plot is once such graph useful for such a comparison. The quantiles of the first batch is plotted against the quantiles of the second batch. A 45 degree \\((Y=X)\\) line is also drawn. If the points on a quantile-quantile plot fall along, or close to, the 45 degree line, then the two variables follow the same distribution; if they fall on a line parallel to the 45 degree line, the two variables follow similar distributions but one has a larger location parameter (say mean, or median) than the other; if the slope differs from the 45 degree line the variables have a similar distribution but may have a different standard deviation; if the plot is linear then the distributions of \\(Y\\) and \\(X\\) are different.\n\n\nCode\nset.seed(12344)\nX &lt;- rnorm(100)\nY &lt;- rnorm(100)\nnq &lt;- 201\n\np &lt;- seq(1 , nq, length.out = 50) / nq - 0.5 / nq\n\np1 &lt;- ggplot() + \n  aes(x = quantile(X, p), y = quantile(Y, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"X & Y roughly follow\\nthe same distributions\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX2 &lt;- rnorm(100)\nY2 &lt;- rnorm(100, 1)\n\np2 &lt;- ggplot() + \n  aes(x = quantile(X2, p), \n      y = quantile(Y2, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger location (mean/median)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX3 &lt;- rnorm(100, 0, 1)\nY3 &lt;- rnorm(100, 0, 2)\n\np3 &lt;- ggplot() + \n  aes(x = quantile(X3, p), \n      y = quantile(Y3, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger scale (SD, F-spread etc)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX4 &lt;- rbeta(100, 1, 4)\nY4 &lt;- rnorm(100, 0, 2)\n\np4 &lt;- ggplot() + \n  aes(x = quantile(X4, p), y = quantile(Y4, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distributions of  X and Y \\nare different\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 15: Quantile-Quantile plot patterns\n\n\n\n\nIn Figure 15, different kinds of quantile-quantile plots are shown; note the following three cases in particular.\n\n\\(Y\\) and \\(X\\) follow similar distributions but the quantiles of \\(Y\\) are a constant amount greater than the quantiles of \\(X\\).\n\\(Y\\) and \\(X\\) follow similar distributions but the standard deviation of \\(Y\\) is greater than that of \\(X\\).\n\\(Y\\) and \\(X\\) follow different distributions.\n\nFigure 16 gives Q-Q plot comparing the Length of house pits for Inside and Outside groups. This Q-Q plot suggests that the two distributions are similar but the average Length is greater for the Inside dwellings.\n\n\nCode\nnq &lt;- length(Snodgrass$Length)\np &lt;- seq(1 , nq, length.out = 20) / nq - 0.5 / nq\n\nX &lt;- Snodgrass |&gt; \n  filter(Inside == \"Inside\") |&gt; \n  pull(Length)\n\nY &lt;- Snodgrass |&gt; \n  filter(Inside == \"Outside\") |&gt; \n  pull(Length)\n\nggplot() + \n  aes(x = quantile(X, p), \n      y = quantile(Y, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  xlab(\"Length quantiles for Inside Group\") + \n  ylab(\"Length quantiles for Outside Group\")\n\n\n\n\n\nFigure 16: Quantile-Quantile plot of Length for Inside-Outside groups\n\n\n\n\nNote that the distribution of two variables \\(X\\) and \\(Y\\) can be compared in a number of ways using boxplots, overlaid ECDF plots etc."
  },
  {
    "objectID": "studyguide/2-eda.html#scatterplots",
    "href": "studyguide/2-eda.html#scatterplots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Scatterplots",
    "text": "Scatterplots\nConsider data which can be written as pairs (\\(X\\), \\(Y\\)). The two groups of data (\\(X\\) and \\(Y\\)) need not be in the same units but they are assumed to be related in some way. Scatterplots are useful in exploring the type of relationship between \\(X\\) and \\(Y\\).\n\n\nCode\nif( !file.exists(\"testmarks.RData\") ) {\n\n  download.file(\n    url = \"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData\", \n    destfile = \"testmarks.RData\")\n\n}\n\n\n\n\nCode\nload(\"testmarks.RData\")\n\n\n\n\nCode\nggplot(testmarks) +\n  aes(y=English, x=Maths) + \n  geom_point() + \n  coord_equal()\n\n\n\n\n\nFigure 17: Scatter plot of English vs Maths scores\n\n\n\n\nThe data in testmarks.txt consists of the records of 40 students, at a school, on standard tests of comprehension of English and Mathematics. The test marks data can be graphed plotting English test scores against Mathematics test scores. The resulting graph, known as the scatterplot, is shown as Figure 17. This plot enables us to see if there is a relationship between \\(X\\) and \\(Y\\).\nHere, the scatterplot shows a clear upward trend – as the Mathematics mark increases so does the English mark, in general.\nThere are a number of useful features that can be added to a scatterplot. The individual distributions of \\(X\\) and \\(Y\\) can be shown by adding appropriate graphical displays (e.g. boxplots) along the axes. If a number of the students have the same marks then it may be advisable to add a jitter as we discussed earlier.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(testmarks) + \n  aes(y=English, x=Maths) +\n  geom_point() + \n  coord_equal()\n\nggMarginal(p1, type=\"boxplot\")\n\n\n\n\n\nFigure 18: English vs Maths scores\n\n\n\n\nIn Figure 18, it can be seen (with some effort) that each set of scores seems to be uniformly distributed along its axis, although the Maths marks exhibit gaps which may indicate different groups within the class.\nFinally, does the variability in English marks change as the Maths mark changes, or is it fairly constant, i.e. does the vertical height of the scatter change from left to right? In this case the variability appears to be fairly constant (we will see why this is important later).\nThe Rangitikei data gives the number of people making recreational use of the Rangitikei River and the number of vehicles seen at a particular location and time. The data were collected at two locations (location being a categorical variable).\n\n\nCode\nif( !file.exists(\"rangitikei.RData\") ) {\n\n  download.file(\n    url = \"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\",\n    destfile = \"rangitikei.RData\")\n\n}\n\n\n\n\nCode\nload(\"rangitikei.RData\")\n\n\nIf the data are grouped, this information can be added to a scatterplot by using different symbols and/or colours for each group. For example, we can show a scatterplot of people vs vehicles with different points for locations (Figure 19).\n\n\nCode\nggplot(rangitikei) +\n  aes(y = people, x = vehicle, col = loc, shape = loc) + \n  geom_point()\n\n\n\n\n\nFigure 19: Scatterplot with a grouping variable\n\n\n\n\nThere appears to be little difference in the number of people and vehicles seen among the locations.\nFigure 20 shows the number of people against the number of vehicles seen as well as the distribution of each variable along the axes using boxplots (these are often referred to as marginal distributions). The first plot in Figure 20 (A) shows the scatter of all points, whereas the second one (B) shows the scatter with the very large value (Observation #26) removed. In the first plot, the outlier shows up quite clearly in both boxplots. In the second plot, the boxplot associated with the variable ‘people’ still shows one point as an outlier, though this doesn’t appear to be as extreme as the outlier in the first plot.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- rangitikei |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() +\n  ggtitle(\"(A): All of the data\")\n\np2 &lt;- rangitikei |&gt; \n  # remove the rows with highest value of 'people'\n  filter(people &lt; max(people)) |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() + \n  ggtitle(\"(B): Without observation #26\")\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"boxplot\"), \n  ggMarginal(p2, type=\"boxplot\"), \n  ncol=1)\n\n\n\n\n\nFigure 20: People vs vehicle plots\n\n\n\n\nNote that the outlier point in Figure 20 (A) showed clearly in the marginal distributions of both variables. However, it was consistent with the relationship between vehicle and people. While this might not surprise us (the relationship between people and vehicles might be expected to stay the same even for a location with large numbers of each), it follows that an outlier with respect to the underlying relationship between two variables will be different from an outlier in the marginal distribution sense. And such a value may not be an outlier in either marginal distribution.\nLook again at Figure 18 - there is one student with a high Maths mark but a low English mark. While neither mark is unusual in itself, the point stands out from the rest of the scatterplot because it is unusually high or low (i.e. up or down) on the plot. In general an outlier with respect to the relationship between \\(X\\) and \\(Y\\) is one whose \\(Y\\) value is unusually large or small compared to other cases with similar \\(X\\) values.\nNow let’s load the horsehearts dataset, which contains the weights of horses’ hearts along with some other heart measurements.\n\n\nCode\nif( !file.exists(\"horsehearts.RData\") ) {\n\n  download.file(\n    url = \"http://www.massey.ac.nz/~anhsmith/data/horsehearts.RData\",\n    destfile = \"horsehearts.RData\")\n\n}\n\n\n\n\nCode\nload(\"horsehearts.RData\")\n\n\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(horsehearts) + \n  aes(y=WEIGHT, x=EXTDIA) + \n  geom_point()\n\np2 &lt;- p1 + geom_smooth(se=FALSE)\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"density\"), \n  ggMarginal(p2, type=\"density\"), \n  ncol=1)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nFigure 21: Weight vs Exterior Widths\n\n\n\n\nFigure 21 shows scatterplots of weights of horses’ hearts against their exterior widths in the systolic phase (that is the contracted phase). The scatterplot indicates that a straight line would not fit the data well. On the other hand, there is some evidence the weights fall into one of two subgroups (with a value around 2.5kg being the cut off point). Scatterplots are particularly useful in pinpointing possible groupings of points so that the structure of the data is revealed which might otherwise go undetected in descriptive statistics calculated for the full data set.\nOne way of quickly assessing whether a straight line fit is reasonable is to smooth the points in the plot using a LOWESS (LOcally WEighted Scatterplot Smoother) smoothed line. The lowess (also called loess) procedure is based on local sets of points (instead of the whole data set) at a given \\(x\\) range, and fits a smoothed line to the data. The second scatterplot in Figure 21 shows the lowess smoother which confirms that there is curvature in the underlying relationship- possibly a cubic curve would fit this."
  },
  {
    "objectID": "studyguide/2-eda.html#correlation-coefficients",
    "href": "studyguide/2-eda.html#correlation-coefficients",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Correlation coefficients",
    "text": "Correlation coefficients\nAs we have seen before, scatterplots provide a good starting point for exploring the relationship between two variables. In Figure 22, the weight, \\(Y\\), is plotted against the exterior width \\(X\\) (diastolic phase in column 7 of horsehearts.txt). It is helpful to divide the scatterplot into four quarters, called quadrants, by drawing a line horizontally at the average \\(Y\\) value or weight (2.235 kg) and vertically at the average \\(X\\) value (or diastolic exterior width 14.13 mm). The first quadrant is the top right hand side quarter. We move anti-clockwise for numbering the quadrants, and the bottom left hand side quadrant is called the third quadrant.\n\n\n\n\n\nFigure 22: Four quadrants of a scatter plot\n\n\n\n\nIn Figure 22, most of the points fall in the third and first quadrants which indicates that there is an increasing relationship between the two variables (as the length increases, the weight tends to increase which is not too surprising!).\nAlthough this scatterplot indicates a nonlinear relationship would fit better than a linear relationship, we will continue to use it for illustration. It is useful to have an objective measure of how strong a linear relationship is (as we have seen, stretching or compressing one or both axes can give different impressions). This measure is called the Pearsonian correlation coefficient between \\(X\\) and \\(Y\\) and is denoted by \\(r_{xy}\\) or \\(r\\) for short:\n\\[\n\\begin{aligned}\nr_{xy} &= \\frac{{\\text {sum}(x - \\bar x)(y - \\bar y)}}{{\\text{squareroot}[\\text {sum}(x - \\bar x)^2  {\\text{sum}}(y - \\bar y)^2 ]}} \\\\\n\\\\\n&= \\frac{\\sum (x-\\bar{x})(y-\\bar{y}) }{\\sqrt{\\sum (x-\\bar{x})^{2}  \\sum (y-\\bar{y})^{2}  } } \\\\\n\\\\\n&=\\frac{S_{xy} }{\\sqrt{S_{xx} S_{yy} } }\n\\end{aligned}\n\\]\nHere \\(S_{xx}=S_x^2\\) is the variance of \\(X\\), \\(S_{yy}=S_y^2\\) is the variance of \\(Y\\), and \\(S_{xy}\\) is the covariance between \\(X\\) and \\(Y\\).\nThe Pearson correlation (coefficient) between WEIGHT and EXTDIA equals 0.759. Perfect correlation (+1) would occur if all points fell on a straight line in the first and third quadrants. Assuming a bivariate normal distribution, the null hypothesis that the true correlation coefficient \\(\\rho _{xy} =0\\) can be tested using the test statistic \\[t = \\frac{{r_{xy} }}{{\\sqrt {\\frac{{1 - r_{xy}^2 }}{{n - 2}}} }}\\] Statistical software programs directly give the \\(p\\)-value for this test. The estimated correlation coefficient of 0.759 for \\((n-2)=44\\) df is significant (\\(p\\)-value is close to zero). Testing of hypothesis related topics are covered in the next Chapter.\nThe scatterplot indicates that there may be two groups of horses; whose hearts weigh about 2.3 kg or more and whose heart weights are less than 2.3 kg. If possible, one should try to establish whether there are two groups for some reason (perhaps age, sex or breed). If the points tended to concentrate in quadrants 2 and 4, this would indicate that the correlation between \\(Y\\) and \\(X\\) is negative (as \\(X\\) increases, \\(Y\\) tends to decrease). If the points were evenly scattered among the four quadrants, the correlation coefficient would be close to zero.\n\n\n\n\n\nFigure 23: Limitations of the correlation coefficient\n\n\n\n\nFigure 23 shows a number of possible plots of \\(Y\\) against \\(X\\) and their associated correlation coefficients. Notice that the correlation coefficient may be close to zero even though there is a strong pattern present. This is because the correlation coefficient only measures the strength of the linear relationship between two variables. In the presence of subgroups the correlation coefficient can become spurious or even be close to zero when there is indeed the correlation is high within the subgroup. Simpson (1951) demonstrated that relationships noted within a population may not hold and could be the opposite within all subgroups. This amalgamation paradox can happen to most summary statistical measures including the correlation coefficient.\nThe other situation that is not shown in Figure 23 is that the correlation coefficient can be positive and a line would be a reasonable fit although the scatter diagram indicates that both \\(Y\\) and \\(X\\) have skewed distributions suggesting that transformations should be applied.\n\n\n\nFigure 24: Sign of the correlation coefficient\n\n\nTo see why the sign of \\(r\\) indicates the direction of the relationship, consider Figure 24.\n\nRegardless of which quadrant the point is in, the contribution to \\(S_{xx}\\) and \\(S_{yy}\\) is positive, as these are sums of squares.\nIn the first quadrant the contribution of each point to the cross product \\(S_{xy}\\) is \\((+) \\times (+)\\), that is a positive amount. In the third quadrant, each contribution is \\((-)\\times (-)\\), that is a positive amount. This is the reason that the correlation coefficient is positive if most points are in the first and third quadrants.\nPoints in the second and fourth quadrant contribute negative amounts, \\((-)\\times (+)\\) or \\((+)\\times (-)\\), to the cross product \\(S_{xy}\\). If there are a considerable number of points in quadrants 2 and 4, the correlation coefficient will tend to be negative."
  },
  {
    "objectID": "studyguide/2-eda.html#correlation-matrices",
    "href": "studyguide/2-eda.html#correlation-matrices",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Correlation matrices",
    "text": "Correlation matrices\nWhen there are more than two variables of interest, it is possible to draw scatterplots of each pair of variables but this becomes a bewildering exercise as the number of variables increases. One way to keep track of these plots is to set them out in a scatterplot matrix as in Figure 25 which is based on the pinetree data set, which contains the circumference measurements of pine trees at four positions (Top, Third, Second and First) and in three areas of a forest. This plot also shows the simple correlation coefficients on the upper diagonal and smoothed density plots on the main diagonal of the matrix. This plot clearly shows that the variables are strongly related to each other. In particular, the Top circumference can be predicted reasonably well based on the bottom circumference. Even though the correlations are high, a quadratic model may fit better. Figure 25 seems to show a non-random pattern in the way points appear on the plot. This may be due to the Area effect because the circumference data were collected from three different areas. Figure 25 confirms that this is indeed the case.\n\n\nCode\nif( !file.exists(\"pinetree.RData\") ) {\n\n  download.file(\n    url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\", \n    destfile = \"pinetree.RData\")\n\n}\n\n\n\n\nCode\nload(\"pinetree.RData\")\n\n\n\n\nCode\nlibrary(GGally)\n\nggpairs(pinetree, columns = 2:5)\n\n\n\n\n\nFigure 25: Four quadrants of a scatter plot\n\n\n\n\nIt is also possible to use the colour option as well as the subgroup-wise correlation coefficients. Try\nggpairs(\n  pinetree, \n  columns = 2:5,\n  mapping = aes(colour = Area)\n  )"
  },
  {
    "objectID": "studyguide/2-eda.html#bivariate-distributions",
    "href": "studyguide/2-eda.html#bivariate-distributions",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Bivariate distributions",
    "text": "Bivariate distributions\nThe two-dimensional or the bivariate probability distribution of \\(X\\) and \\(Y\\) can be displayed as a 2-d smoothed density plot or as a probability contour plot. Figure 26 shows such plots. This figure rather suggests that there may be two separate joint probability distributions of Maths and English scores. This clue is not obtained in the scatter plot of English score against Maths score but only from the other three plots.\n\n\nCode\ny = testmarks$English\nx = testmarks$Maths\ndef.par = par(mfrow = c(2, 2))\n\nplot(x, y, xlim = c(0,100), ylim = c(0,100),\n     xlab = \"Maths Marks\",\n     ylab = \"English Marks\",\n     main = \"Scatter plot of English Scores vs Maths Scores\")\n\nf1 = kde2d(x, y, n = 50, lims = c(0, 100, 0, 100))\n\nimage(f1,\n      xlab = \"Maths Marks\",\n      ylab = \"English Marks\",\n      col=gray((0:32)/32), main = \"Image Plot\")\n\ncontour(f1, \n        xlab = \"Maths Marks\",\n        ylab = \"English Marks\",\n        main = \"Density contourPlot\")\n\nop = par(mar=c(0,0,2,0)+.1)\n\npersp(f1, phi = 45, theta = 30,\n      xlab = \"Maths Marks\", ylab = \"English Marks\", zlab = \"density\",\n      col = \"gray\", shade = .5, border = NA,\n      main = \"Perspective plot (density estimation)\")\n\npar(op)\n\npar(def.par)\n\n\n\n\n\nFigure 26: Exploring bivariate distributions\n\n\n\n\nIn summary, scatterplots may reveal many properties and peculiarities about the relationship of \\(Y\\) to \\(X\\), namely\n\nTrends: the overall trend may be positive or negative. It may be appropriate to fit a straight line or a curve.\nOutliers: points which appear to be unusual, or outliers, may be evident. In a scatterplot, the outlier need not be in the extreme scale of both \\(X\\) and \\(Y\\), and anything appearing peculiar or as a rogue point must be investigated.\nGaps may be seen in the data which may indicate that different groups are present. On the other hand, if it is known that points belong to different groups, the points can be tagged with different symbols. The basic idea is to see whether different groups follow the same pattern or different patterns.\nVariability of \\(Y\\) change with \\(X\\), or may be fairly constant. Clustering of points with smaller variability when compared to rest of the points may also be revealed in some scatterplots.\n\nBoth joint and marginal distributions of \\(X\\) and \\(Y\\) are important. We also consider the distribution of each variable separately, and explore whether they have symmetric distributions or not."
  },
  {
    "objectID": "studyguide/2-eda.html#higher-dimensional-data",
    "href": "studyguide/2-eda.html#higher-dimensional-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Higher dimensional data",
    "text": "Higher dimensional data\nA short-cut to explore three numerical variables is to obtain a contour plot. In Figure 26, the contours were the joint density of Maths and English marks. Instead, we can just use three numerical variables and show the third variable (Z) as a contour on a scatter plot of Y vs. X. What this means is that several combinations of X and Y can lead to the same Z. So we have to use some smoothing method or a model to obtain the contours and form a grid. The R package plotly is rather simple and powerful to obtain dynamic or interactive plots. The plots produced with this package are colour optimised for viewing and hence they may not be the best for printing but good for EDA purposes. We can also hover over the graph and view the graphed data. After loading the pine tree data, try-\n\n\nCode\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)\n\n\n\n\n\n\nA bubble plot is a way to plot three variables on a two-dimensional scatter plot, where a third variable gives the size of the points (or ‘bubbles’). This type of plot can be restrictive for some datasets depending on the variability involved in the third variable. Figure 27 shows the location of the house pits and total Area. This plot not only gives the spatial view of the tribal area but indirectly reveals the clustering of house sizes.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x=East, y=South, colour=Area, size=Area) + \n  geom_point()\n\n\n\n\n\nFigure 27: A bubble plot for the Snodgrass data\n\n\n\n\nUsing the appropriate software, it is possible to explore three variables using a 3-D scatter plot. We explore the pinetree data in Figure 28 which clearly shows the strong relationships and the Area effect. The other forms of exploring 3 variables include 3-D surface plots.\n\n\nCode\nlibrary(lattice)\ncloud(Top ~ First+Second, group=Area, data=pinetree)\n\n\n\n\n\nFigure 28: A 3-D plot\n\n\n\n\nplotly is great for interactive 3d plots (though they only work with HTML, not PDF).\n\n\nCode\nplot_ly(Snodgrass, x = ~East, y = ~South, z = ~Area) |&gt; \n  add_markers()\n\n\n\n\n\n\nIf we wish to examine the relationships between (three or more) variables we could do so with a conditioning plot, otherwise known as a coplot. A coplot is a scatterplot matrix where each panel in the matrix is a scatterplot of the same two variables, whose observations are restricted by the values of a third (and possibly a fourth) variable.\n\n\nCode\ncoplot(Top~ First | Second*Area, data = pinetree)\n\n\n\n\n\nFigure 29: A co-plot\n\n\n\n\nThe coplot in Figure 29 shows that pine trees are not maturing well in Area 3, given that fewer points are on the right hand side of the graph when compared to Areas 1 and 2."
  }
]