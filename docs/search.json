[
  {
    "objectID": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "href": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "title": "Chapter 1:Data Collection",
    "section": "The nature of data: random variables",
    "text": "The nature of data: random variables\nData come from recording observations of the world. E.g.,\n\nrecording the number of heart beats per minute\ncounting birds in your yard\nmeasuring the lengths of fish you catch\n\nEach time you collect one of these observations, it is likely to be different.\n\nPulse may vary between 55 and 70 bpm, depending on when you record it\nNumber of birds varies at different times, and across different yards.\n\nWe call these random variables (often denoted with an upper case letter, \\(X\\)), because the particular value that a single observation or measurement will take is uncertain. The value varies across observations."
  },
  {
    "objectID": "slides/Chapter01.html#types-of-data",
    "href": "slides/Chapter01.html#types-of-data",
    "title": "Chapter 1:Data Collection",
    "section": "Types of data",
    "text": "Types of data"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "href": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of qualitative data",
    "text": "Subtypes of qualitative data\n\n\n\n\nNominal variables have no particular order (e.g., gender, colour, species, country)\n\n\n\nOrdinal variables can be ordered (e.g., altitude = {low, mid, high}, age group = {child, juvenile, adult} )"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "href": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of quantitative data",
    "text": "Subtypes of quantitative data\n\n\n\n\nContinuous variables have no gaps between possible values, as in measurements (e.g., weight, temperature, length)\n\n\n\nDiscrete variables have gaps between possible values, as in counts (e.g., number of siblings, number of flowers)"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-continuous-data",
    "href": "slides/Chapter01.html#subtypes-of-continuous-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of continuous data",
    "text": "Subtypes of continuous data\n\nInterval scale\n\nNo absolute zero\nDivision & subtraction may not be meaningful\nTemperature in degrees Celcius is interval because 20°C is not twice as hot as 10°C.\n\n\nRatio scale\n\nZero = zero\nAll arithmetic manipulation can be done\nLength is ratio because 20 mm is twice as long as 10 mm."
  },
  {
    "objectID": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "href": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "title": "Chapter 1:Data Collection",
    "section": "Data Collection: Survey, Experiment, Census",
    "text": "Data Collection: Survey, Experiment, Census\n\n\n\nWe collect data from the world to get information about patterns and processes.\nMost datasets contain a subset, a sample, of a much bigger population of interest.\n\nWe may conduct a survey to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.\nWe might conduct an experiment where we take a sample of elements (people, organisms, objects) and apply some treatment in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.\n\nIf we are not dealing with a sample, if every element of the population of interest is represented in the dataset, we call this a census rather than a sample."
  },
  {
    "objectID": "slides/Chapter01.html#measurement-issues",
    "href": "slides/Chapter01.html#measurement-issues",
    "title": "Chapter 1:Data Collection",
    "section": "Measurement issues",
    "text": "Measurement issues\n\nMeasuring Devices or Instruments\n\na physical device - measuring rule to gauge the heights of plants\na counting device - a Geiger- counter for measuring radioactive material\na questionnaire - requires a more subjective response.\n\nMeasurement Error\n\nmeasuring instrument may be faulty (bias)\nvalues recorded from the same object may vary from one measurement to another (variance)\n\nIndirect measures\n\nFor example, we use Body Mass Index (BMI) as a measure of condition, and we measure temperature with the expansion of mercury."
  },
  {
    "objectID": "slides/Chapter01.html#non-response",
    "href": "slides/Chapter01.html#non-response",
    "title": "Chapter 1:Data Collection",
    "section": "Non-response",
    "text": "Non-response\n\na non-sampling error\nSelection stage: an element may be selected but not found\n\ne.g. sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey.\n\nCollection stage: it may not be possible to take a measurement\n\nsome respondents may forget, or refuse, to answer the questionnaire\n\nDocumentation stage\n\nIncorrect record of measurement\n\nCall-backs reduce non-response"
  },
  {
    "objectID": "slides/Chapter01.html#census-related-concepts",
    "href": "slides/Chapter01.html#census-related-concepts",
    "title": "Chapter 1:Data Collection",
    "section": "Census related concepts",
    "text": "Census related concepts\nTARGET POPULATION the population under study\nFRAME operationalises data collection from a target population. e.g. listing of elements in population.\nACTUAL POPULATION is the resulting set of elements on which usable data have been collected."
  },
  {
    "objectID": "slides/Chapter01.html#sample-vs-population",
    "href": "slides/Chapter01.html#sample-vs-population",
    "title": "Chapter 1:Data Collection",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\n\nA sample is a subset of the population.\nDatasets usually only contain a sample from the population; rarely do we have the entire population of data!\nWhy sample?\n\nSampling conserves resources (money, time, etc.).\nA well collected sample is more useful than a badly designed census.\nCollecting data may be destructive.\nThe disadvantage: the statistics we calculate from sample data is subject to sampling variation, which introduces uncertainty* about their true values.\n\n\n\n\n\n“You don’t have to eat the whole ox to know that the meat is tough” – Samuel Johnson (1709-1784)"
  },
  {
    "objectID": "slides/Chapter01.html#population-frame-and-sample",
    "href": "slides/Chapter01.html#population-frame-and-sample",
    "title": "Chapter 1:Data Collection",
    "section": "Population, frame, and sample",
    "text": "Population, frame, and sample"
  },
  {
    "objectID": "slides/Chapter01.html#statistical-inference",
    "href": "slides/Chapter01.html#statistical-inference",
    "title": "Chapter 1:Data Collection",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the process of using information from sample data to make conclusions about the population.\n\nFor example, we want to know \\(\\mu\\), mean length of fish in a population. So, we collect a sample of fish, measure their lengths, calculate the mean \\(\\bar{x}\\), and use \\(\\bar{x}\\) as an estimate of \\(\\mu\\). This is statistical inference.\n\n\n\n\nThe sample mean \\(\\bar{x}\\) depends on which particular fish we happened to get in our sample.\nTherefore, the sample mean \\(\\bar{x}\\) itself is a random variable.\nIf we were to take 1000 different samples, we’d get 1000 different means."
  },
  {
    "objectID": "slides/Chapter01.html#bias-vs-sampling-variance",
    "href": "slides/Chapter01.html#bias-vs-sampling-variance",
    "title": "Chapter 1:Data Collection",
    "section": "Bias vs sampling variance",
    "text": "Bias vs sampling variance\n\n\nA method used to estimate \\(\\hat{\\theta}\\) a population parameter \\(\\theta\\) is called an estimator. An estimator includes the study design, methods of data collection, and mathematical operations.\nSampling variance is the sample-to-sample variation in an estimator.\nBias is when our estimator doesn’t get it right on average. That is, the average of estimates over \\(\\infty\\) samples is not centred on the population parameter; \\(\\text{Mean}(\\hat{\\theta}) \\neq \\theta\\).\n\n\n\n\nAn estimator can have high/low sampling variance and high/low bias."
  },
  {
    "objectID": "slides/Chapter01.html#principle-of-randomisation",
    "href": "slides/Chapter01.html#principle-of-randomisation",
    "title": "Chapter 1:Data Collection",
    "section": "Principle of randomisation",
    "text": "Principle of randomisation\n\n\n\nWe want our sample to be representative of (and have similar properties to) the population. The most straightforward way to do this is through randomisation.\nWe randomise the selection of objects for our sample to avoid bias. If we (consciously or subconsciously) tended to chose the largest fish for our sample, we’d get an upwardly biased estimate of the lengths.\nSimple random sampling or EPSEM (equal probability of selection) is the gold standard of random sampling."
  },
  {
    "objectID": "slides/Chapter01.html#simple-random-sampling-srs",
    "href": "slides/Chapter01.html#simple-random-sampling-srs",
    "title": "Chapter 1:Data Collection",
    "section": "Simple Random Sampling (SRS)",
    "text": "Simple Random Sampling (SRS)\n\n\n\nRandom selection of elements\n\n“Random” refers to the process not outcome\nEach (sampling) unit has same chance of being selected\nUnits can be selected with & without replacement\n\n\n\n\n\n\n\nSRS is easy to handle; suits even for a poor sampling frame\nSRS can be costly to implement\nSRS estimates are more variable than some alternatives\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#stratified-random-sampling-strs",
    "href": "slides/Chapter01.html#stratified-random-sampling-strs",
    "title": "Chapter 1:Data Collection",
    "section": "Stratified Random Sampling (STRS)",
    "text": "Stratified Random Sampling (STRS)\n\n\n\nSuitable for heterogeneous populations\nPopulation is divided into relatively homogeneous groups called strata and a random sample is taken from each stratum.\n\n\n\n\n\n\nSampling Approaches\n\nSample the larger strata more heavily (suits when all the strata are equally variable)\nSample the more varied strata are sampled\n\nAdvantages of STRS\n\nleads to efficient estimation That is, the variance (of an estimate) is usually less than that of SRS\nsample is spread throughout population\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#cluster-sampling",
    "href": "slides/Chapter01.html#cluster-sampling",
    "title": "Chapter 1:Data Collection",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\n\n\nA convenient method of sampling\npopulation is composed of clusters (groups)\nSelect certain clusters (randomly) and collect measurements from a random selection of the elements within the chosen clusters\nLarger variance than SRS!\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "href": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "title": "Chapter 1:Data Collection",
    "section": "Systematic Random sampling (SyRS)",
    "text": "Systematic Random sampling (SyRS)\n\n\n\nSelect every \\(k^{th}\\) element!\nRandom start within the first block of elements.\n\nConvenient and also the sample will be representative of population\nVariance of estimates - generally greater than those of SRS\nInefficient/inappropriate, if cycle or trend is present\n\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#other-sampling-methods",
    "href": "slides/Chapter01.html#other-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Other Sampling methods",
    "text": "Other Sampling methods\n\nProbability proportional to size (PPS)\n\ne.g., sampling high-value companies more likely than low-value companies\n\nMultistage\n\ne.g., first stage - cluster; second stage - SRS\n\nNon-probability sampling methods\n\nHaphazard / opportunistic / volunteer; take what you can get!\nSnowball; get your participants to find new participants\nPurposive; select items with certain characteristics; e.g., patients with particular symptoms\n\nNon-probability samples are often treated as random, requiring the assumption that the sample is representative. The validity of this assumption should be carefully considered."
  },
  {
    "objectID": "slides/Chapter01.html#some-sampling-methods",
    "href": "slides/Chapter01.html#some-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Some sampling methods",
    "text": "Some sampling methods"
  },
  {
    "objectID": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "href": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "title": "Chapter 1:Data Collection",
    "section": "Effective Sample size (thumb rule)",
    "text": "Effective Sample size (thumb rule)\n\n\n\n\n\n\n\n\nSample Design\nDesign Effect (\\(d\\))\nEffective Sample Size (\\(\\frac{n}{d}\\))\n\n\n\n\nSRS\n1.00\n\\(n\\)\n\n\nSTRS\n0.80 to 0.90\n\\(\\frac{n}{0.9}\\) to \\(\\frac{n}{0.8}\\)\n\n\nCluster\n1.02 to 1.26\n\\(\\frac{n}{1.26}\\) to \\(\\frac{n}{1.02}\\)\n\n\nSyRS\n1.05\n\\(\\frac{n}{1.05}\\)\n\n\nQuota\n2\n\\(\\frac{n}{2}\\)"
  },
  {
    "objectID": "slides/Chapter01.html#summary",
    "href": "slides/Chapter01.html#summary",
    "title": "Chapter 1:Data Collection",
    "section": "Summary",
    "text": "Summary\n\nIssues to address\n\nWHAT are collected?\nWHO does the data collection?\nHOW are the data collected?\n\nBias occurs due to\n\nSELECTION\nCOLLECTION\nNON-RESPONSE (the single largest cause of bias!)\n\nA sample may have the same biases as a census along with sampling errors\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter02.html#two-modes-of-data-analysis",
    "href": "slides/Chapter02.html#two-modes-of-data-analysis",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Two modes of data analysis",
    "text": "Two modes of data analysis\n\n\nHypothesis-generating\n\n“Exploratory analysis”\nAim is to explore data to discover new patterns\nResults must not be presented as formal tests of a priori hypotheses\nTesting a hypothesis using the same data that gave rise to the hypothesis is circular reasoning\n\n\nHypothesis-testing\n\n“Confirmatory analysis”\nAim is to evaluate evidence for specific a priori hypotheses\nThe hypotheses and ideas were conceived of before the data were observed\nCan be used for formal scientific inference\n\n\n\nPresenting hypothesis-generating analyses as hypothesis-testing analyses (i.e., pretending the hypotheses were conceived prior to the analysis) is scientifically dishonest, and a major contributor to the replication crisis in science."
  },
  {
    "objectID": "slides/Chapter02.html#plots-for-categorical-data",
    "href": "slides/Chapter02.html#plots-for-categorical-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Plots for categorical data",
    "text": "Plots for categorical data\nBar graphs\n\nShow the frequency of each category (level) in categorical variables\nThe height of each bar is proportional to the frequency\nCan be “stacked” or “clustered”"
  },
  {
    "objectID": "slides/Chapter02.html#tea-data",
    "href": "slides/Chapter02.html#tea-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Tea data",
    "text": "Tea data\nData from 300 individuals’ tea-drinking habits (18 questions), perceptions (12 questions), and personal details (4 questions).\n\ndata(tea, package = \"FactoMineR\")\nglimpse(tea)\n\nRows: 300\nColumns: 36\n$ breakfast        &lt;fct&gt; breakfast, breakfast, Not.breakfast, Not.breakfast, b…\n$ tea.time         &lt;fct&gt; Not.tea time, Not.tea time, tea time, Not.tea time, N…\n$ evening          &lt;fct&gt; Not.evening, Not.evening, evening, Not.evening, eveni…\n$ lunch            &lt;fct&gt; Not.lunch, Not.lunch, Not.lunch, Not.lunch, Not.lunch…\n$ dinner           &lt;fct&gt; Not.dinner, Not.dinner, dinner, dinner, Not.dinner, d…\n$ always           &lt;fct&gt; Not.always, Not.always, Not.always, Not.always, alway…\n$ home             &lt;fct&gt; home, home, home, home, home, home, home, home, home,…\n$ work             &lt;fct&gt; Not.work, Not.work, work, Not.work, Not.work, Not.wor…\n$ tearoom          &lt;fct&gt; Not.tearoom, Not.tearoom, Not.tearoom, Not.tearoom, N…\n$ friends          &lt;fct&gt; Not.friends, Not.friends, friends, Not.friends, Not.f…\n$ resto            &lt;fct&gt; Not.resto, Not.resto, resto, Not.resto, Not.resto, No…\n$ pub              &lt;fct&gt; Not.pub, Not.pub, Not.pub, Not.pub, Not.pub, Not.pub,…\n$ Tea              &lt;fct&gt; black, black, Earl Grey, Earl Grey, Earl Grey, Earl G…\n$ How              &lt;fct&gt; alone, milk, alone, alone, alone, alone, alone, milk,…\n$ sugar            &lt;fct&gt; sugar, No.sugar, No.sugar, sugar, No.sugar, No.sugar,…\n$ how              &lt;fct&gt; tea bag, tea bag, tea bag, tea bag, tea bag, tea bag,…\n$ where            &lt;fct&gt; chain store, chain store, chain store, chain store, c…\n$ price            &lt;fct&gt; p_unknown, p_variable, p_variable, p_variable, p_vari…\n$ age              &lt;int&gt; 39, 45, 47, 23, 48, 21, 37, 36, 40, 37, 32, 31, 56, 6…\n$ sex              &lt;fct&gt; M, F, F, M, M, M, M, F, M, M, M, M, M, M, M, M, M, F,…\n$ SPC              &lt;fct&gt; middle, middle, other worker, student, employee, stud…\n$ Sport            &lt;fct&gt; sportsman, sportsman, sportsman, Not.sportsman, sport…\n$ age_Q            &lt;fct&gt; 35-44, 45-59, 45-59, 15-24, 45-59, 15-24, 35-44, 35-4…\n$ frequency        &lt;fct&gt; 1/day, 1/day, +2/day, 1/day, +2/day, 1/day, 3 to 6/we…\n$ escape.exoticism &lt;fct&gt; Not.escape-exoticism, escape-exoticism, Not.escape-ex…\n$ spirituality     &lt;fct&gt; Not.spirituality, Not.spirituality, Not.spirituality,…\n$ healthy          &lt;fct&gt; healthy, healthy, healthy, healthy, Not.healthy, heal…\n$ diuretic         &lt;fct&gt; Not.diuretic, diuretic, diuretic, Not.diuretic, diure…\n$ friendliness     &lt;fct&gt; Not.friendliness, Not.friendliness, friendliness, Not…\n$ iron.absorption  &lt;fct&gt; Not.iron absorption, Not.iron absorption, Not.iron ab…\n$ feminine         &lt;fct&gt; Not.feminine, Not.feminine, Not.feminine, Not.feminin…\n$ sophisticated    &lt;fct&gt; Not.sophisticated, Not.sophisticated, Not.sophisticat…\n$ slimming         &lt;fct&gt; No.slimming, No.slimming, No.slimming, No.slimming, N…\n$ exciting         &lt;fct&gt; No.exciting, exciting, No.exciting, No.exciting, No.e…\n$ relaxing         &lt;fct&gt; No.relaxing, No.relaxing, relaxing, relaxing, relaxin…\n$ effect.on.health &lt;fct&gt; No.effect on health, No.effect on health, No.effect o…"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-one-variable",
    "href": "slides/Chapter02.html#bar-charts-one-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — one variable",
    "text": "Bar charts — one variable\n\n\nggplot(tea) +\n  geom_bar(aes(x = price)) + \n  ggtitle(\"Bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-two-variables",
    "href": "slides/Chapter02.html#bar-charts-two-variables",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — two variables",
    "text": "Bar charts — two variables\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts---flipped",
    "href": "slides/Chapter02.html#bar-charts---flipped",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts - flipped",
    "text": "Bar charts - flipped\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  aes(x = price) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nPie charts are popular but not usually the best way to show proportional data\nRequires comparison of angles or areas of different shapes\nBar charts are almost always better\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs",
    "href": "slides/Chapter02.html#one-dimensional-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nggplot(Animals) +\n  aes(x = brain) + \n  geom_dotplot() + \n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Dotplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs-1",
    "href": "slides/Chapter02.html#one-dimensional-graphs-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nAnimals |&gt; \n  mutate(\n    Animal = fct_reorder(\n      rownames(Animals), \n      brain )\n    ) |&gt; \n  ggplot() +\n  aes( y = Animal, \n       x = brain\n       ) + \n  geom_point() + \n  ylab(\"Animal\") + \n  ggtitle(\"Strip chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#histograms",
    "href": "slides/Chapter02.html#histograms",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nDivide the data range into “bins”, count the occurrences in each bin, and make a bar chart.\nY-axis can show raw counts, relative frequencies, or densities\n\nset.seed(1234); dfm &lt;- data.frame(X = rnorm(50, 100))\n\np1 &lt;- ggplot(dfm, aes(X)) + geom_histogram(bins = 20) + ylab(\"count\") + ggtitle(\"Frequency histogram\", \"Heights of the bars sum to n\")\np2 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(count/sum(count))) + geom_histogram(bins = 20) + ylab(\"relative frequency\") +\n  ggtitle(\"Relative frequency histogram\", \"Heights sum to 1\")\np3 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(density)) + geom_histogram(bins = 20) + \n  ggtitle(\"Density histogram\",\"Heights x widths sum to 1\")\n\nlibrary(patchwork); p1+p2+p3"
  },
  {
    "objectID": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "href": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Frequency polygon & kernel density plots",
    "text": "Frequency polygon & kernel density plots\n\n\nHistograma coarse visualisation of the distribution\n\nggplot(vital) + aes(Life_female) + \n  geom_histogram(bins = 12) +\n  geom_freqpoly(bins = 12)\n\n\n\n\n\n\n\n\n\nKernel densitya smooth approximation of the density\n\nggplot(vital) + aes(Life_female) +\n  geom_histogram(bins = 12, aes(y = after_stat(density))) + \n  geom_density()"
  },
  {
    "objectID": "slides/Chapter02.html#kernel-density-estimation-kde",
    "href": "slides/Chapter02.html#kernel-density-estimation-kde",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Kernel density estimation (KDE)",
    "text": "Kernel density estimation (KDE)"
  },
  {
    "objectID": "slides/Chapter02.html#summary-statistics-for-eda",
    "href": "slides/Chapter02.html#summary-statistics-for-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary statistics for EDA",
    "text": "Summary statistics for EDA"
  },
  {
    "objectID": "slides/Chapter02.html#five-number-summary",
    "href": "slides/Chapter02.html#five-number-summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Five-number summary",
    "text": "Five-number summary\nMinimum, lower hinge, median, upper hinge and maximum\n\nset.seed(1234)\nmy.data &lt;- rnorm(50, 100)\nfivenum(my.data)\n\n[1]  97.65430  99.00566  99.46477  99.98486 102.41584\n\nsummary(my.data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.65   99.01   99.46   99.55   99.96  102.42"
  },
  {
    "objectID": "slides/Chapter02.html#boxplots",
    "href": "slides/Chapter02.html#boxplots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Boxplots",
    "text": "Boxplots\n\nGraphical display of 5-number summary\nCan show several groups of data on the same graph"
  },
  {
    "objectID": "slides/Chapter02.html#cumulative-frequency-graphs",
    "href": "slides/Chapter02.html#cumulative-frequency-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Cumulative frequency graphs",
    "text": "Cumulative frequency graphs\n\nShow the left tail area\nUseful to obtain the quantiles (deciles, percentiles, quartiles etc)\n\n\n\nset.seed(123)\n\nd &lt;- data.frame(\n  x = rnorm(50, 100)\n  )\n\nggplot(d) + \n  aes(x) + \n  stat_ecdf()"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps",
    "href": "slides/Chapter02.html#shiny-apps",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nLots of examples are available\n\nIn the study guide and workshops for this course (though not all of them are working currently)\nOn the web\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.univariate.graphs/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.univariate.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "href": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) plot",
    "text": "Quantile-Quantile (Q-Q) plot\nQ-Q plots compare the distributions of two data sets by plotting their quantiles against each other.\n\n\nvital &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/vital.txt\", \n  header=TRUE, sep=\",\")\n\nquants &lt;- seq(0, 1, 0.05)\n\nvital |&gt; \n  summarise(\n    Female = quantile(Life_female, quants),\n    Male = quantile(Life_male, quants)\n  ) |&gt; \n  ggplot() +\n  aes(x = Female, y = Male) +\n  geom_point() + \n  geom_abline(slope=1, intercept=0) +\n  coord_fixed() +\n  ggtitle(\n    \"Quantiles of life expectancy\",\n    subtitle = \"are lower for males vs females\"\n    )"
  },
  {
    "objectID": "slides/Chapter02.html#some-q-q-plot-patterns",
    "href": "slides/Chapter02.html#some-q-q-plot-patterns",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Some Q-Q Plot patterns",
    "text": "Some Q-Q Plot patterns\n\nCase a: Quantiles of Y (mean/median etc) are higher than those of X\nCase b: Spread or SD of Y &gt; spread or SD of X\nCase c: X and Y follow different distributions \n\nR function: qqplot()."
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships",
    "href": "slides/Chapter02.html#bivariate-relationships",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-1",
    "href": "slides/Chapter02.html#bivariate-relationships-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_smooth(span = 0.8, se = FALSE) + \n  ggtitle(\"Scatterplot with lowess smoother\")"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-2",
    "href": "slides/Chapter02.html#bivariate-relationships-2",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_density_2d() +\n  ggtitle(\"Scatterplot with 2D density\")"
  },
  {
    "objectID": "slides/Chapter02.html#marginal-plot",
    "href": "slides/Chapter02.html#marginal-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Marginal Plot",
    "text": "Marginal Plot\nShows both bivariate relationships and univariate (marginal) distributions\n\n\np1 &lt;- ggplot(rangitikei) +\n  aes(x = people, y = vehicle) + \n  geom_point() + theme_bw()\n\nlibrary(ggExtra)\nggMarginal(p1, type=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "href": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot / scatterplot matrix",
    "text": "Pairs plot / scatterplot matrix\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1])"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "href": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot with a grouping variable",
    "text": "Pairs plot with a grouping variable\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1], \n        aes(colour = pinetree$Area))"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-coefficients",
    "href": "slides/Chapter02.html#correlation-coefficients",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation coefficients",
    "text": "Correlation coefficients\nThe Pearson correlation coefficient measures the linear association between two variables."
  },
  {
    "objectID": "slides/Chapter02.html#correlation-matrix",
    "href": "slides/Chapter02.html#correlation-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\nTo show all pairwise correlation coefficients\nUseful to explore the inter-relationship between variables \n\n\n\nlibrary(psych)\ncorr.test(pinetree[,-1])\n\n\nCall:corr.test(x = pinetree[, -1])\nCorrelation matrix \n        Top Third Second First\nTop    1.00  0.92   0.96  0.97\nThird  0.92  1.00   0.95  0.91\nSecond 0.96  0.95   1.00  0.97\nFirst  0.97  0.91   0.97  1.00\nSample Size \n[1] 60\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       Top Third Second First\nTop      0     0      0     0\nThird    0     0      0     0\nSecond   0     0      0     0\nFirst    0     0      0     0\n\n To see confidence intervals of the correlations, print with the short=FALSE option"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-plots",
    "href": "slides/Chapter02.html#correlation-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Plots",
    "text": "Correlation Plots\n\n\nlibrary(corrplot)\ncorrplot(\n  cor(pinetree[,-1]),  \n  type = \"upper\", \n  method=\"number\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#network-plots",
    "href": "slides/Chapter02.html#network-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Network plots",
    "text": "Network plots\n\n\nlibrary(corrr)\npinetree[,-1] |&gt; \n  correlate() |&gt; \n  network_plot(min_cor=0.2)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots",
    "href": "slides/Chapter02.html#d-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-1",
    "href": "slides/Chapter02.html#d-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1 + aes(colour = Area)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plotly\n\n\nlibrary(plotly)\n\nplot_ly(\n  pinetree, \n  x = ~First, \n  y = ~Second, \n  z = ~Top\n  ) |&gt; \n  add_markers()"
  },
  {
    "objectID": "slides/Chapter02.html#contour-plots",
    "href": "slides/Chapter02.html#contour-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Contour plots",
    "text": "Contour plots\n\n3D plots are difficult to interpret than 2D plots in general\nContour plots are another way of looking three variables in two dimensions\n\n\n\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots",
    "href": "slides/Chapter02.html#conditioning-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\ncoplot(Top ~ First | Second*Area, \n       data = pinetree)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots-1",
    "href": "slides/Chapter02.html#conditioning-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"mpru/ggcleveland\")\nlibrary(ggcleveland)\ngg_coplot(\n  pinetree, \n  x = First, \n  y = Top, \n  faceting = Second, \n  number_bins = 6, \n  overlap = 3/4\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#more-r-graphs",
    "href": "slides/Chapter02.html#more-r-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "More R graphs",
    "text": "More R graphs\nBuild plots in a single layout (R packages patchwork or gridExtra)\n\n\np1 &lt;- ggplot(testmarks) +\n  aes(y = English, x = Maths) + \n  geom_point()\n\np2 &lt;- p1 + \n  stat_density_2d(\n    geom = \"raster\",\n    aes(fill = after_stat(density)),\n    contour = FALSE) + \n  scale_fill_viridis_c() + \n  guides(fill=FALSE)\n\nlibrary(patchwork)\np1 / p2"
  },
  {
    "objectID": "slides/Chapter02.html#learning-eda",
    "href": "slides/Chapter02.html#learning-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Learning EDA",
    "text": "Learning EDA\n\nThe best way to learn EDA is to try many approaches and find which are informative and which are not.\n\nChatfield (1995) on tackling statistical problems:\n\nDo not attempt to analyse the data until you understand what is being measured and why. Find out whether there is prior information such as are there any likely effects.\nFind out how the data were collected.\nLook at the structure of the data.\nThe data then need to be carefully examined in an exploratory way before attempting a more sophisticated analysis.\nUse common sense, and be honest!"
  },
  {
    "objectID": "slides/Chapter02.html#summary",
    "href": "slides/Chapter02.html#summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary",
    "text": "Summary\n\nSize\n\nFor small datasets, we cannot be too confident in any patterns we see. More likely for patterns to occur ‘by chance’.\nSome displays are more affected by sample size than others\n\nShape\n\nIn can be interesting to display the overall shape of distribution.\nAre there gaps and/or many peaks (modes)?\nIs the distribution symmetrical? Is the distribution normal?\n\nOutliers\n\nBoxplots & scatterplots can reveal outliers\nMore influential than points in the middle\n\nGraphs should be simple and informative; certainly not misleading!\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter03.html#probability-and-randomness",
    "href": "slides/Chapter03.html#probability-and-randomness",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\n\n\n\nProbability and randomness are placeholders for incomplete knowledge.\nAfter I shuffled a deck of cards, you might consider the identity of the top card to be “random”.\nBut is it really?\nIf you knew the starting positions of the cards and a good HD video of my shuffling, you could surely know the positions of the cards, and which is on top.\nLikewise for rolling a die. If we know everything about the starting position, how it was thrown, the texture of the surface, humidity, etc., could we predict what it would roll?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "href": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability as a relative frequency",
    "text": "Probability as a relative frequency\n\n\nThe classical definition of probability is just the relative frequency of an event.\n\nIf a fair die is rolled, there are \\(n = 6\\) possible outcomes. Let the event of interest be getting a number 4 or more. The probability of this event is 3 out of 6 or \\(p=1/2\\).\n\nThe sample space or the set of all possible outcomes need not be finite.\n\nExample: Tossing a coin until the first head appears will result in an infinite sample space. The probability can be viewed as a limiting or long run fraction of \\(m/n\\) (i.e. when \\(n \\to \\infty\\)).\n\nWhen the sample space is finite and outcomes are equally likely, we can assume that classical probability will be the same as empirical probability.\n\nExample: To find the probability of a fair coin landing heads it is not necessary to toss the coin repeatedly and observe the proportion of heads.\n\nProbabilities can only be between \\(0\\) (impossible) and \\(1\\) (certain).\nProbabilities can be subjective (such as expert opinion, or a guess)"
  },
  {
    "objectID": "slides/Chapter03.html#mutually-exclusive-events",
    "href": "slides/Chapter03.html#mutually-exclusive-events",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\nFor mutually exclusive events,\n\nThe probability of any two events co-occurring is zero\nThe probability of one event or another event occurring is the sum of the two respective probabilities.\nThe probability of any one event not occurring is the sum of those remaining.\n\n\nExample: A randomly selected single digit can be either odd (Event \\(O\\)) or even (Event \\(E\\)).\nThe events \\(O\\) and \\(E\\) are mutually exclusive because a number cannot be both odd and even.\nThe sample space is \\(\\{0,1,2,3,4,5,6,7,8,9\\}\\).\n\n\n\n\\(\\rm{Pr(E~\\&~O)=0}\\)\n\\(\\rm{Pr(E~ or~O)=1}\\)\n\\(\\rm{Pr(E)=1-Pr(O)}\\) and \\(\\rm{Pr(O)=1-Pr(E)}\\)"
  },
  {
    "objectID": "slides/Chapter03.html#statistical-independence",
    "href": "slides/Chapter03.html#statistical-independence",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Statistical Independence",
    "text": "Statistical Independence\nIf events \\(A\\) and \\(B\\) are statistically independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\n\n\n\n\n\n\nConditional probability\n\n\n\n\\(P(A|B)\\) is the probability of event \\(A\\) occurring given that event \\(B\\) is has occurred.\nFor example, the probability of a card you’ve drawn being a 5, given that it is a spade.\nThe sample space is reduced to that where \\(B\\) (e.g. the card is a spade) has occurred.\n\n\n\n\n\nWe say that two events (\\(A\\) and \\(B\\)) are independent if \\(P(A | B) = P(A)\\) and \\(P(B | A) = P(B)\\).\nObserving event \\(A\\) doesn’t make event \\(B\\) any more or less likely, and vice versa.\nFor any two events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B ) = P(A|B) \\times P(B)\\) and \\(P(A \\textbf{ and } B ) = P(B|A) \\times P(A)\\)."
  },
  {
    "objectID": "slides/Chapter03.html#blood-group-example",
    "href": "slides/Chapter03.html#blood-group-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Blood Group Example",
    "text": "Blood Group Example\n\n\n\n Two systems for categorising blood are:\n\nthe Rh system (Rh+ and Rh–)\nthe Kell system (K+ and K–)\n\nFor any person, their blood type in any one system\nis independent of their blood type in any other.\nFor Europeans in New Zealand,\nabout 81% are Rh+ and about 8% are K+.\n\nFrom the table:\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\n\n0.0648 + 0.1748 = 0.2396\n\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\n\n0.0152"
  },
  {
    "objectID": "slides/Chapter03.html#bayes-rule",
    "href": "slides/Chapter03.html#bayes-rule",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[P(A\\mid B)=\\frac {P(B\\mid A)P(A)}{P(B)}~~~~~~~~\\rm{s.t}~~ P(B)&gt;0\\]\n\n\\(P(A\\mid B)\\) and \\(P(B\\mid A)\\) are conditional probabilities.\n\\(P(A)\\) and \\(P(B)\\) are marginal or prior probabilities."
  },
  {
    "objectID": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "href": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Prevalence, sensitivity, specificity, PPV, and NPV",
    "text": "Prevalence, sensitivity, specificity, PPV, and NPV\nLet \\(D\\) be the event of a person having the Disease and \\(H\\) be the event of a person being Healthy (i.e., not having the disease). The outcome of a test for the disease can be either positive \\((T_+)\\) or negative \\((T_-)\\).\nConsider the following definitions of conditional probabilities:\n\nprevalence is the overall probability one has the disease, or \\(P(D)\\).\nsensitivity the probability that one tests positive given one has the disease, or \\(P(T_+ | D)\\).\nspecificity the probability that one tests negative given one does not have the disease, or \\(P(T_- | H)\\).\npositive predictive value of a test is the probability one has the disease given that one has tested positive, or \\(P(D \\mid T_{+})\\)\nnegative predictive value of a test is the probability that one is healthy given that one has tested negative, or \\(P(H \\mid T_{-})\\)"
  },
  {
    "objectID": "slides/Chapter03.html#example",
    "href": "slides/Chapter03.html#example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nSay the following were true:\n\nPrevalence: \\(P(D) = 0.03\\) and \\(P(H) = 1-0.03=0.97\\)\nSensitivity: \\(P(T_+\\mid D) = 0.98\\)\nSpecificity: \\(P(T_{-}\\mid H) = 0.95\\)\n\nWe can use Bayes Rule to answer the following questions:\n\nWhat proportion of the overall population will test positive vs negative?\nWhat are the implications of a positive or negative test result?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-tree",
    "href": "slides/Chapter03.html#probability-tree",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability tree",
    "text": "Probability tree\nIt can be useful to visualise the probabilities of the four possible states using a tree diagram.\n\n\n\n\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\n\n\n\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued",
    "href": "slides/Chapter03.html#example-continued",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat proportion of the overall population will test positive vs negative?\nThe overall proportion of positive tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{+}) &= P(T_{+} \\& D) + P(T_{+} \\& H) \\\\\n&= P(T_{+} \\mid D)P(D) + P(T_{+} \\mid H)P(H) \\\\\n&= 0.98 \\times 0.03 + 0.05 \\times 0.97 \\\\\n&= 0.0779\n\\end{aligned}\n\\] The overall proportion of negative tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{-}) &= 1 - P(T_{+}) \\\\ &= 0.9221\n\\end{aligned}\n\\]\nComplete table of probabilities:\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97\n\n\n\n0.0779\n0.9221\n1"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued-1",
    "href": "slides/Chapter03.html#example-continued-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat are the implications of a positive or negative test result?\nAccording to Bayes rule, the probability of a random person having the disease given they’ve tested positive is given by:\n\\[\n\\begin{aligned}\nP(D\\mid T_{+}) &= \\frac {P(T_{+}\\mid D)P(D)} {P(T_{+})} \\\\\n&= \\frac{0.98 \\times 0.03}  {0.0779} \\\\\n&= 0.3774\n\\end{aligned}\n\\]\nAccording to Bayes rule, the probability of a random person not having the disease given they’ve tested negative is given by:\n\\[\n\\begin{aligned}\nP(H \\mid T_{-}) &= \\frac {P(T_{-} \\mid H)P(H)} {P(T_{-})} \\\\\n&= \\frac{0.95 \\times 0.97}  {0.9221} \\\\\n&= 0.9993\n\\end{aligned}\n\\]\nThe positive predictive value of the test is poor—only 38% of the subjects who tested positive will have the disease.\nThe negative predictive value is better—if a random subject tests negative, they’re very unlikely to have the disease."
  },
  {
    "objectID": "slides/Chapter03.html#discrete-probability-distributions-1",
    "href": "slides/Chapter03.html#discrete-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Discrete probability distributions",
    "text": "Discrete probability distributions\nConsider the number of eggs \\((X)\\) in an Adelie penguin’s nest. The values range from \\(1\\) to \\(5\\), each with a certain probability (or relative frequency) of occurrence.\n\n\nNote the probabilities add to \\(1\\) because \\({1,2,3,4,5}\\) is a complete sample space.\n\nThe population mean \\(\\mu_X\\) is simply the sum of each outcome multiplied by its probability.\n\\[\\mu_X = E(X)= \\sum xP(X=x)=\\sum xP(x)\\]\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\n(Mean=sum(X*P))\n\n[1] 3.15\n\n\nThe population variance is given by\n\\[Var(X)= \\sigma_X^2=\\sum (x-\\mu_X)^2 P(x)\\]\nThe population SD is simply the square-root of the variance.\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\nMean=sum(X*P)\n(Variance =sum((X-Mean)^2*P))\n\n[1] 1.4275\n\n(SD=sqrt(Variance))\n\n[1] 1.19478"
  },
  {
    "objectID": "slides/Chapter03.html#binomial-distribution",
    "href": "slides/Chapter03.html#binomial-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nConsider a variable that has two possible outcomes\n(say success and failure, with 50% probabilty each).\nThis can be described as a “Bernoulli” random variable.\nA “Binomial” is just a collection of Bernoulli trials.\nLet \\(X\\) be the number of heads when two coins are tossed.\nThe count of the number of successes \\(X\\) out of a fixed total of\n\\(n\\) independent trials follows the binomial distribution.\nThat is, \\(X \\sim Bin(n, p)\\), where \\(p\\) the probability of a success.\nThe binomial probability function \\(P(X=x)\\) or \\(P(x)\\)\nis given by \\[P(x)={n \\choose x}p^{x}(1-p)^{n-x}\\]\nFor \\(n=10\\), \\(p=0.3\\), the binomial probabilities,\n\\(P(x)\\) for \\(x=0,1,2, \\dots, 10\\), are plotted to the right.\nIf each of 10 basketball shots succeeded with probability 0.3, this describes the probability of your total score out of 10.\n\n\n\n\ndfm &lt;- data.frame(\n  x = as.factor(0:10), \n  Probability = dbinom(x = 0:10, size = 10, prob = 0.3))\nggplot(dfm) + aes(x = x, y = Probability) + geom_col() +\n  xlab(\"Number of successes (x)\") +\n  annotate(geom = \"table\", label = list(dfm), x=11, y=.05)"
  },
  {
    "objectID": "slides/Chapter03.html#example-1",
    "href": "slides/Chapter03.html#example-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nA microbiologist plates out certain bacteria on a plate, and picks out 10 colonies. She knows that the probability of successfully creating a recombinant is 0.15.\nWhat is the probability that if she mixes all 10 colonies in a growth medium with penicillin, something (anything) will grow?\nIn other words:\nIf \\(X \\sim Bin(n = 10, p = 0.15)\\), what is \\(P(x &gt; 0)\\)?\nNote \\(P(x &gt; 0)=1-P(x = 0)\\). So in R, compute this as follows:\n\n1 - dbinom(x=0, size=10, prob=.15)\n\n[1] 0.8031256\n\n\nor\n\n1-pbinom(q=0, size=10, prob=.15)\n\n[1] 0.8031256"
  },
  {
    "objectID": "slides/Chapter03.html#binomial",
    "href": "slides/Chapter03.html#binomial",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial",
    "text": "Binomial\nThe code pbinom(k,size=n,prob=p) gives the cumulative probabilities up to and including the quantile \\(k\\).\nThe Probability Mass Function (PMS) for a binomial random variable is:\n\\[P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}\\]\nThe mean and variance of the binomial random variable is given by\n\\[\\mu_X=np~~~~ \\sigma^2_X=np(1-p)\\]\nIn the last example, the expected number of recombinant strain of bacteria is\n\\[\\mu_X=np=10*0.15=1.5\\]\nwith standard deviation\n\\[\\sigma_X=\\sqrt {np(1-p)}=1.129159\\]"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution",
    "href": "slides/Chapter03.html#poisson-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to obtain the probabilities of counts of relatively rare events that occur independently in space or time.\nSome Examples:\n\nThe number of snails in a quadrat \\((1~m^2)\\)\nFish counts in a visual transect (25m x 5m)\nBacterial colonies in 2 litres of milk"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution-1",
    "href": "slides/Chapter03.html#poisson-distribution-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe random variable \\(X\\), the number of occurrences (count), often follows the Poisson distribution whose probability function is given by\n\\[\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty\\]\nThe parameter \\(\\lambda\\) is the mean which is also equal to the variance.\n\\[\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda\\]\nMain assumptions:\n\nThe events occur at a constant average rate of \\(\\lambda\\) per unit time or space.\nOccurrences are independent of one another as well as they do not happen at exactly the same unit time or space."
  },
  {
    "objectID": "slides/Chapter03.html#poisson-example",
    "href": "slides/Chapter03.html#poisson-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson example",
    "text": "Poisson example\n\n\n\n\n\n\n\n\n\n\n\n Consider the number of changes that accumulate along a\nstretch of a neutrally evolving gene over a given period of time.\nThis is a Poisson random variable with a\npopulation mean of \\(\\lambda=kt\\), where\n\\(k\\) is the number of mutations per generation, and\n\\(t\\) is the time in generations that has elapsed.\n     \nAssume that \\(k = 1\\times10^{-4}\\) and \\(t = 500\\).\nFor \\(\\lambda=kt=0.05\\), the Poisson probabilities are shown in the following plot.\nWhat is the probability that at least one mutation has occurred over this period?\n\\(P(x &gt; 0)=1-P(x = 0)\\) is found in R as follows:\n\n1 - dpois(x=0, lambda=0.05)\n\n[1] 0.04877058"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-1",
    "href": "slides/Chapter03.html#continuous-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nA discrete random variable takes values which are simply points on a real line. In other words, there is an inherent discontinuity in the values a discrete random variable can take.\nIf a random variable, \\(X\\), can take any value (i.e., not just integers) in some interval of the real line, it is called a continuous random variable.\nE.g., height, weight, length, percentage protein\nFor a discrete random variable \\(X\\), the associated probabilities \\(P(X=x)\\) are also just points or masses, and hence the probability function \\(P(x)\\) is also called as the probability mass function (PMF).\nFor continuous random variables, probabilities can be computed when the variable falls in an interval such as \\(5\\) to \\(15\\), but not when it takes a fixed value such as \\(10\\) (which is equal to zero).\nThe Probability Density Function (PDF) gives the relative likelihood of any particular value."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-2",
    "href": "slides/Chapter03.html#continuous-probability-distributions-2",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\n\nFor example, consider a random proportion \\((X)\\) between \\(0\\) and \\(1\\). Here \\(X\\) follows a (standard) continuous uniform distribution whose (probability) density function \\(f(x)\\) is defined as follows:\n\\[f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x&lt;0\\ \\mathrm {or} \\ x&gt;1\\end{cases}\\] This constant density function is the simple one in the graph to the right.\n\n\n\ntibble(x = seq(-.5, 1.5, length=1000),\n       `f(x)` = dunif(x, min=0, max=1)) |&gt; \n  ggplot() + \n  aes(x = x, y = `f(x)`) +\n  geom_area(colour = 1, alpha = .2)\n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-3",
    "href": "slides/Chapter03.html#continuous-probability-distributions-3",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n  \np\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-4",
    "href": "slides/Chapter03.html#continuous-probability-distributions-4",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  annotate(geom = \"path\", \n    x = c(19.3, 19.3, 13), \n    y = c(0, rep(dnorm(19.3,20,2),2) ),\n    arrow = arrow(),\n    colour = \"dodgerblue4\", size = 1.1)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe density at 19.3 is \\(f(19.3) = 0.1876\\)).\n\ndnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.1876202"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-5",
    "href": "slides/Chapter03.html#continuous-probability-distributions-5",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  geom_area(\n    data = d |&gt; filter(x &lt;= 19.3),\n    fill = \"dodgerblue4\",\n    size  = 1.1, alpha = .6)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe area under the curve to the left of the value 19.3 is given by the Cumulative Density Function (CDF), or \\(F(x)\\). It gives the probability that x &lt; 19.3; \\(F(19.3) = 0.3632\\).\n\npnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.3631693"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-6",
    "href": "slides/Chapter03.html#continuous-probability-distributions-6",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe cumulative distribution function, CDF, \\(F(x)\\) gives the left tail area or probability up to \\(x\\). This is probability is found as\n\\[F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt\\] The relationship between the density function \\(f(x)\\) and the distribution function \\(F(x)\\) is given by the Fundamental Theorem of Calculus.\n\\[f(x)={dF(x) \\over dx}\\]"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-7",
    "href": "slides/Chapter03.html#continuous-probability-distributions-7",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe total area under the PDF curve is \\(1\\). The probability of obtaining a value between two points (\\(a\\) and \\(b\\)) is the area under the PDF curve between those two points. This probability is given by \\(F(b)-F(a)\\).\nFor the uniform distribution \\(U(0,1)\\), \\(f(x)=1\\). So\n\\[F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x\\]\nFor example, the probability of a randomly drawn fraction from the interval \\([0,1]\\) to fall below \\(x=0.5\\) is 50%.\nThe probability of a random fraction falling between \\(a=0.2\\) and \\(b=0.8\\) is\n\\[F(b)-F(a)=0.8-0.2=0.6\\]\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "href": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "The Normal (Gaussian) Distribution",
    "text": "The Normal (Gaussian) Distribution\n\n\nThe Gaussian or Normal Distribution is parameterised in terms of the mean \\(\\mu\\) and the variance \\(\\sigma ^{2}\\) and its Probability Density Function (PDF) is given by\n\\[f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\] A Standard Normal Distribution has mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It has a simpler PDF:\n\\[f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}\\] If \\(X \\sim N(\\mu, \\sigma)\\), you can convert the \\(X\\) values into \\(Z\\)-scores by subtracting the mean \\(\\mu\\) and dividing by the standard deviation \\(\\sigma\\).\n\\[Z={\\frac {X-\\mu }{\\sigma }}\\]\nWe often deal with the standard normal because the symmetric bell shape of the normal distribution remains the same for all \\(\\mu\\) and \\(\\sigma\\).\n\n\n \n\ndfn &lt;- tibble(x=seq(-4,4,length=1000), \n              `f(x)` = dnorm(x), \n              `F(x)` = pnorm(x))\np1 &lt;- ggplot(dfn) + aes(x=x,y=`f(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np2 &lt;- ggplot(dfn) + aes(x=x,y=`F(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Cumulative Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#example-of-a-normal",
    "href": "slides/Chapter03.html#example-of-a-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example of a normal",
    "text": "Example of a normal\n\nThe weight of an individual of Amphibola crenata, a marine snail,\nis normally distributed with a mean of \\(40g\\) and variance of \\(20g^2\\).\n\n  \n\ndfs &lt;- tibble(x=seq(20, 60, length=1000), \n    `f(x)` = dnorm(x, mean=40, sd=sqrt(20)))\n\nps &lt;- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs between \\(35g\\) and \\(50g\\)?\n\n\n\n\nIn R, the function pnorm() gives the CDF.\n\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n\n[1] 0.8555501\n\n\n\n\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &lt; 50 & x &gt; 35),\n            fill=\"coral1\", alpha=.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs below \\(35g\\) or over \\(50g\\)?\n\n\n\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n\n[1] 0.1444499\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &gt; 50),\n            fill=\"coral1\", alpha=.5) + \n  geom_area(data = dfs |&gt; filter(x &lt; 35),\n            fill=\"coral1\", alpha=.5)"
  },
  {
    "objectID": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "href": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Areas (probabilities) under the standard normal",
    "text": "Areas (probabilities) under the standard normal\nUnder standard normal, the areas under the PDF curve are shown below for various situations.\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#small-sample-effect",
    "href": "slides/Chapter03.html#small-sample-effect",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Small sample effect",
    "text": "Small sample effect\nFor small samples, the shape might be difficult to judge.\n\n\nset.seed(1234)\ndfm &lt;- data.frame(\n  x=rnorm(50, \n          mean=80, \n          sd=12)\n  )\n\np1 &lt;- ggplot(dfm) + \n  geom_histogram(\n    aes(x=x, y=after_stat(density)), \n    colour=1\n    ) + \n  stat_function(\n    fun = dnorm, \n    args = list(mean = 80, sd = 12), \n    geom = \"line\"\n    ) +\n  xlim(min(dfm), max(dfm))\n\np2 &lt;- ggplot(dfm) + aes(x) + \n  geom_boxplot() +\n  xlim(min(dfm), max(dfm)) +\n  theme_void()\n\nlibrary(patchwork)\np1 / p2 + plot_layout(heights = c(5, 1))\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#normal-quantile-plots",
    "href": "slides/Chapter03.html#normal-quantile-plots",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Normal quantile plots",
    "text": "Normal quantile plots\nIn a normal quantile plot, the quantiles of the sample are plotted against the theoretical quantiles of the fitted normal distribution.\nThe points should roughly lie on a straight line\nWe can also compare the empirical and theoretical CDFs.\n\n\n\n\n\n\n\n\n\nTV viewing time data\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "href": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Which distribution is most appropriate?",
    "text": "Which distribution is most appropriate?\n\nRemember, theoretical distributions aren’t real—they’re just models—but they can be useful. Keep your purpose in mind.\nChoose the simplest distribution that provides an adequate fit.\nData may be best served by a mixture of two or more distributions rather than a single distribution.\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-1",
    "href": "slides/Chapter04.html#statistical-inference-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nThe term statistical inference means that we are using properties of a sample to make statements about the population from which the sample was drawn.\nFor example, say you wanted to know the mean length of the leaves on a tree (\\(\\mu\\)). You wouldn’t want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths (\\(x_i\\)), calculate the sample mean (\\(\\bar x\\)), and use \\(\\bar x\\) as an estimate of \\(\\mu\\)."
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "href": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference for a Population Mean",
    "text": "Statistical Inference for a Population Mean\n\n\nConsider a population \\(X\\) with mean \\(\\mu\\),\nvariance \\(\\text{Var}(X)=\\sigma^2\\), and\nstandard deviation \\(\\sigma\\).\nWe can estimate \\(\\mu\\) by:\n\ntaking a random sample of values \\(\\{x_1, x_2, ..., x_n\\}\\) from the population, \\(X\\), and\ncalculating the sample mean \\(\\bar x = \\frac 1 n \\sum_{i=1}^{n}x_i\\).\n\n\n\n\n\nImportantly:\n\nThe sample mean \\(\\bar x\\) is a single draw from a random variable \\(\\bar X\\). It will never be exactly equal to the population mean, \\(\\mu\\), unless you measure every single leaf.\nThere is uncertainty in our estimate of \\(\\mu\\). Each sample yields a different \\(\\bar x\\). This is called sampling error."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-variation",
    "href": "slides/Chapter04.html#sampling-variation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Variation",
    "text": "Sampling Variation"
  },
  {
    "objectID": "slides/Chapter04.html#sampling-error",
    "href": "slides/Chapter04.html#sampling-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Error",
    "text": "Sampling Error\nThe variability of the sample means from sample to sample, \\(\\text{Var}(\\bar X)\\), depends on two things:\n\nthe variability of the population values, \\(\\text{Var}(x)\\), and\nthe size of the sample, \\(n\\).\n\nThe variability of the sample estimate of a parameter is usually expressed as a standard error (SE), which is simply the theoretical standard deviation of \\(\\bar X\\) from sample to sample.\nThe equation is surprisingly simple!\n\\(\\text{Var}(\\bar X) = \\text{Var}(X)/n\\)\n\\(\\text{SE}(\\bar X) = \\text{SD}(\\bar X) = \\sigma/\\sqrt n\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset",
    "href": "slides/Chapter04.html#quetelets-dataset",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\nIn 1846, a Belgian scholar Adolphe Quetelet published an analysis of the chest sizes of a full population of 5,738 Scottish soldiers.\nThe distribution of the measurements (in inches) in his database has a mean of \\(\\mu\\) = 39.83 inches, a standard deviation of \\(\\sigma\\) = 2.05 inches, and is well approximated by a normal distribution.\n\n\n\n\nAdolphe Quetelet1796-1874\n\n\n\n\n\n\nqd &lt;- tibble(\n  Chest = 33:48, \n  Count = c(3, 18, 81, 185, 420, 749, 1073, 1079, \n            934, 658, 370, 92, 50, 21, 4, 1),\n  Prob = Count / sum(Count)\n  )\n\nqd |&gt; \n  ggplot() +\n  aes(x = Chest, y = Count) +\n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"Chest circumferences of Scottish soldiers\")"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-1",
    "href": "slides/Chapter04.html#quetelets-dataset-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# A single sample\nsample(qd_long, size = 6)\n\n[1] 39 40 42 40 37 37\n\n\n\n\n\n# Ten samples\nmap(1:7, ~ sample(qd_long, size = 6))\n\n[[1]]\n[1] 40 43 46 41 40 43\n\n[[2]]\n[1] 43 38 38 39 39 37\n\n[[3]]\n[1] 37 43 40 41 43 43\n\n[[4]]\n[1] 42 37 39 39 43 40\n\n[[5]]\n[1] 43 39 37 40 41 39\n\n[[6]]\n[1] 41 43 41 36 39 40\n\n[[7]]\n[1] 42 39 41 37 38 38"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-2",
    "href": "slides/Chapter04.html#quetelets-dataset-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\nhead(q_samples, 15)\n\n# A tibble: 15 × 2\n   sample values\n   &lt;fct&gt;   &lt;int&gt;\n 1 1          37\n 2 1          39\n 3 1          40\n 4 1          41\n 5 1          41\n 6 1          38\n 7 2          39\n 8 2          42\n 9 2          39\n10 2          41\n11 2          45\n12 2          38\n13 3          38\n14 3          41\n15 3          35"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-3",
    "href": "slides/Chapter04.html#quetelets-dataset-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\n\n\n# Calculate means of each sample\nsample_means &lt;- q_samples |&gt; \n  group_by(sample) |&gt; \n  summarise(mean = mean(values))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample  mean\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 1       40.3\n 2 2       40.2\n 3 3       40.7\n 4 4       39.7\n 5 5       40  \n 6 6       39.5\n 7 7       41.3\n 8 8       40.7\n 9 9       40.3\n10 10      39.5"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-4",
    "href": "slides/Chapter04.html#quetelets-dataset-4",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\n\n# Make a function to do it all and plot\nplot_samples &lt;- function(dat = qd_long,\n                         no_samples = 12,\n                         sample_size = 6) {\n\n  # Put samples in a tibble\n  q_samples &lt;- tibble( \n    sample = as_factor(1:no_samples),\n    values = map(sample, ~ sample(dat, size = sample_size))\n    ) |&gt; unnest()\n  \n  # Calculate means of each sample\n  sample_means &lt;- q_samples |&gt; group_by(sample) |&gt; \n    summarise(mean = mean(values))\n  \n  ggplot() + \n    xlim(min(dat), max(dat)) +\n    geom_vline(xintercept = mean(dat), alpha = .4) +\n    geom_jitter(\n      data = q_samples,\n      mapping = aes(y = sample, x = values),\n      width = 0, height = 0.1, alpha = .8\n      ) + \n    geom_point(\n      data = sample_means,\n      mapping = aes(y = sample, x = mean),\n      shape = 15, size = 3, \n      colour = \"dark orange\"\n      ) +\n    ggtitle(paste(\"Sample size =\", sample_size))\n}\n\n\n\nplot_samples(sample_size = 6)"
  },
  {
    "objectID": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "href": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sample means and sample sizes",
    "text": "Sample means and sample sizes\n\n\nLet’s compare the distribution of means for different sample sizes across the samples.\n\nplot_samples(sample_size = 3)\nplot_samples(sample_size = 5)\nplot_samples(sample_size = 10)\nplot_samples(sample_size = 20)\nplot_samples(sample_size = 50)\nplot_samples(sample_size = 100)\nplot_samples(sample_size = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe larger the \\(n\\), the less the sample means vary."
  },
  {
    "objectID": "slides/Chapter04.html#standard-error",
    "href": "slides/Chapter04.html#standard-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error",
    "text": "Standard error\nIf is distributed as a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[X \\sim \\text{Normal}(\\mu, \\sigma)\\]\nthen sample means of size \\(n\\) are distributed as\n\\[\\bar X \\sim \\text{Normal}(\\mu_\\bar{X} = \\mu, \\sigma_\\bar{X} = \\sigma/\\sqrt{n})\\] \\(\\text{SE}(\\bar{X}) = \\sigma_\\bar{X}= \\sigma/\\sqrt{n})\\) is known as the standard error of the sample mean.\nMore generally, a standard error is the standard deviation of an estimated parameter over \\(\\infty\\) theoretical samples.\nAccording to the Central Limit Theorem (CLT), raw \\(X\\) values don’t have to be normally distributed for the sample means to be normally distributed (for any decent sample size)!"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Standard error of means of samples",
    "text": "Quetelet’s chests: Standard error of means of samples\nQuestion:\nIf Quetelet’s chest circumferences are distributed as \\(X \\sim \\text{N}(\\mu=39.83, \\sigma=2.05)\\), then how variable are the means of samples of size \\(n\\) = 6?\nAnswer:\n\\[\n\\begin{aligned}\nSE(\\bar{X}_{n=6})&=\\sigma/\\sqrt{n} \\\\\n&=2.05/\\sqrt{6} \\\\\n&=0.8369\n\\end{aligned}\n\\] Sample means of size \\(n\\) = 6 would be distributed as\n\\(\\bar{X}_{n=6} \\sim \\text{N}(\\mu=39.83, \\sigma=0.8369)\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Distribution of means of samples",
    "text": "Quetelet’s chests: Distribution of means of samples"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals",
    "href": "slides/Chapter04.html#confidence-intervals",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nAnother common way of expressing uncertainty when making statistical inferences is to present:\n\na point estimate (e.g., the sample mean) and\na confidence interval around that estimate.\n\nA confidence interval gives an indication of the sampling error.\nA 95% confidence interval is constructed so that intervals from 95% of samples will contain the true population parameter.\nIn reality the interval either contains the true value or not, so you must not interpret a confidence interval as “there’s a 95% probability that the interval contains the true parameter”.\nWe can say:\n\n“95% of so-constructed intervals will contain the true value of the parameter” or\n“with 95% confidence, the interval contains the true value of the parameter”."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-1",
    "href": "slides/Chapter04.html#confidence-intervals-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nBecause we know the population parameters for the Quetelet dataset, we can calculate where 95% of sample means will lie for any particular sample size.\nRecall that, for a normal distribution, 95% of values lie between \\(\\mu \\pm 1.96\\times\\sigma\\)\n(i.e., \\(\\mu - 1.96\\times\\sigma\\) and \\(\\mu + 1.96\\times\\sigma\\)).\nIt follows that 95% of means of samples of size \\(n\\) will lie within \\(\\mu \\pm 1.96 \\times \\sigma/\\sqrt{n}\\).\nFor example, for samples of size 6 from the Quetelet dataset, 95% of means will lie within \\(39.83 \\pm 1.96\\times 2.05 / \\sqrt 6\\),\nso \\(\\{ 37.02 , 42.64\\}\\)."
  },
  {
    "objectID": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "href": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Estimating the distribution of sample means from data",
    "text": "Estimating the distribution of sample means from data\n\n\nIt’s all very well deriving the distribution of sample means when we know the population parameters (\\(\\mu\\) and \\(\\sigma\\)) but in most cases we only have our one sample.\nWe don’t know any of the population parameters. We have to estimate the population mean (usually denoted \\(\\hat\\mu\\) or \\(\\bar x\\)) and estimate of the population standard deviation (usually denoted \\(\\hat\\sigma\\) or \\(s\\)) from the sample data.\nThis additional uncertainty complicates things a bit. In fact, we can’t even use the normal distribution any more!"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution",
    "href": "slides/Chapter04.html#the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\n\n\nIntroducing William Sealy Gosset, who described the t distribution after working with random numbers.\nGosset was a chemist and mathematician who worked for the Guinness brewery in Dublin.\nGuinness forbade anyone from publishing under their own names so that competing breweries wouldn’t know what they were up to, so he published his discovery in 1908 under the penname “Student”. Student’s identity was only revealed when he died. It is therefore often called “Student’s t distribution”.\n\n\n\n\n\nWillam Sealy Gosset1876-1937"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-1",
    "href": "slides/Chapter04.html#the-t-distribution-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\nThe t distribution is like the standard normal. It is bell-shaped and symmetric with mean = 0, but it has fatter tails (greater uncertainty) due to the fact that we do not know \\(\\sigma\\); we have to estimate it.\nThe t distribution is not just a single distribution. It is really an entire series of distributions which is indexed by something called the “degrees of freedom” (or \\(df\\)).\n\n\n\n\nAs \\(df \\rightarrow \\infty\\), \\(t \\rightarrow Z\\).\n\n\np &lt;- expand_grid(\n  df = c(2, 5, Inf),\n  x = seq(-4, 4, by = .01)\n  ) |&gt; \n  mutate(\n    Density = dt(x = x, df = df),\n    `degrees of freedom` = as_factor(df)\n  ) |&gt; \n  ggplot() +\n  aes(x = x, y = Density, \n      group = `degrees of freedom`, \n      colour = `degrees of freedom`) +\n  geom_line()"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-2",
    "href": "slides/Chapter04.html#the-t-distribution-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\nFor a sample of size \\(n\\), if\n\n\\(X\\) is a normal random variable with mean \\(\\mu\\),\n\\(\\bar X\\) is the sample mean, and\n\\(s\\) is the sample standard deviation,\n\nthen the variable:\n\\[\nT = \\frac{\\bar X - \\mu} {s/\\sqrt{n}}\n\\]\nis distributed as a \\(t\\) distribution with \\((n – 1)\\) degrees of freedom."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "href": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals and the t distribution",
    "text": "Confidence intervals and the t distribution\nThe process of using sample data to try and make useful statements about an unknown parameter, \\(\\theta\\), is called statistical inference.\nA confidence interval for the true value of a parameter is often obtained by:\n\\[\n\\hat \\theta \\pm t \\times \\text{SE}(\\hat \\theta)\n\\] where \\(\\hat \\theta\\) is the sample estimate of \\(\\theta\\) and \\(t\\) is a quantile from the \\(t\\) distribution with the appropriate degrees of freedom.\nFor example, to get the \\(t\\) score for a 95% confidence interval with 9 degrees of freedom:\n\nqt(p = 0.975, df = 9)\n\n[1] 2.262157\n\n\nThe piece that is being added and subtracted, \\(t \\times \\text{SE}(\\hat \\theta)\\), is often called the margin of error."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean\nWe can calculate a 95% confidence interval for a sample mean with the following information:\n\n\nsample mean \\(\\bar x\\),\nsample standard deviation \\(s\\),\nthe sample size \\(n\\), and\nthe 0.025th quantile of the \\(t\\) distribution with degrees of freedom \\(df=n-1\\).\n\n\n\n\nA sample of size \\(n\\) = 6 from Quetelet data\n\nn1 &lt;- 6; df1 &lt;- n1-1\n( dq1 &lt;- sample(qd_long, size = n1) )\n\n[1] 39 42 38 36 40 38\n\n( mean1 &lt;- mean(dq1) )\n\n[1] 38.83333\n\n( sd1 &lt;- sd(dq1) )\n\n[1] 2.041241\n\n( t1 &lt;- qt(c(0.025, 0.975), df = df1) )\n\n[1] -2.570582  2.570582\n\nmean1 + t1 * sd1 / sqrt(n1)\n\n[1] 36.69118 40.97548\n\n\n\n\n\n\nOr, more simply…\n\nt.test(dq1)\n\n\n    One Sample t-test\n\ndata:  dq1\nt = 46.6, df = 5, p-value = 8.595e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.69118 40.97548\nsample estimates:\nmean of x \n 38.83333"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "href": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for sample means",
    "text": "Confidence intervals for sample means"
  },
  {
    "objectID": "slides/Chapter04.html#t-as-a-test-statistic",
    "href": "slides/Chapter04.html#t-as-a-test-statistic",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "t as a test statistic",
    "text": "t as a test statistic\nThis method can be used when the estimator \\(\\hat\\theta\\) is approximately normally distributed and\n\\[\n\\frac{\\hat\\theta - \\theta} {\\text{SE}(\\hat \\theta)}\n\\]\nhas approximately a Student’s t distribution.\nThis paves the way for hypothesis testing for specific values of \\(\\theta\\). Many of the methods you will learn in this course are based on this general rule."
  },
  {
    "objectID": "slides/Chapter04.html#testing-hypotheses",
    "href": "slides/Chapter04.html#testing-hypotheses",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Testing hypotheses",
    "text": "Testing hypotheses\n\nTesting hypotheses underpins a lot of scientific work.\nA hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.\nFor logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:\n\nspecify a null hypotheses, which must be true if our hypothesis of interest is false, and\nmeasure the evidence against the null hypothesis, usually in the form of a p-value."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nSay a farmer has 9 paddocks of alfalfa. He’s hired a new manager, and wants to know if, on average, this year’s crop is different to last year’s. He measures the yield for each paddock in year 1 and year 2, and calculates the difference.\n\nyear1 &lt;- c(0.8, 1.3, 1.7, 1.7, 1.8, 2.0, 2.0, 2.0, 2.2)\nyear2 &lt;- c(0.7, 1.4, 1.8, 1.8, 2.0, 2.0, 2.1, 2.1, 2.2)\ndiff &lt;- year2 - year1\n( xbar &lt;- mean(diff) ) ; ( s &lt;- sd(diff) )\n\n[1] 0.06666667\n\n\n[1] 0.08660254\n\n\nThe mean difference is 0.067. On average, yields were 0.067 greater than last year.\nIs that convincing different from zero?\nHow likely is such a difference to have arisen just by chance, and really, if we had a million paddocks, there would be no difference?\nWhat is the probability of seeing a difference of 0.067 or more in our dataset if the true mean were zero?\nThese questions can be addressed with a \\(p\\)-value from a hypothesis test."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nWe can test the null hypothesis using a t statistic \\(t_0 = \\frac{\\bar x - \\mu_0} {\\text{SE}(\\bar x)}\\), where\n\\(\\ \\ \\ \\ \\ \\mu_0 = 0\\) is the hypothesised value of the mean and\n\\(\\ \\ \\ \\ \\ \\text{SE}(\\bar x) = s/\\sqrt{n}\\) is the (estimated) standard error of the sample mean.\n\n( t0 &lt;- (xbar - 0) / ( s / sqrt(9) ) )\n\n[1] 2.309401\n\n\nThe p-value is \\(\\text{Pr}(|t_{df=8}| &gt; t_0)\\)\n\n2 * pt(t0, df = 8, lower = F)\n\n[1] 0.04973556"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nThis can be done more easily:\n\nt.test(diff)\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.3094, df = 8, p-value = 0.04974\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 9.806126e-05 1.332353e-01\nsample estimates:\n mean of x \n0.06666667"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe \\(p\\)-value of 0.05 means that, if the null hypothesis were true, only 5% of sample means would be as or more extreme than the observed value of 0.067. It is the area in orange in the graph below.\nWe can therefore reject the null hypothesis at the conventional 5% level, and conclude that, on average, yields were indeed higher this year.\nThis is an example of a paired t test. They two samples (year 1 and year 2) are not independent of one another because we have the same paddocks in both years. So, we take the differences, treat them like a single sample, and do a one-sample t test for the mean difference being zero.\n\\(p\\)-values are random variables too.\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes",
    "href": "slides/Chapter04.html#truth-and-outcomes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nIn reality, the null hypothesis is either true or false.\nThe outcome of a test is either we reject the null hypothesis, or we fail to reject the null hypothesis (note, we never “confirm” or “accept” the null hypothesis – absence of evidence is not evidence of absence!).\nThis gives us four possibilities:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect result\n\n\nDo not reject \\(H_0\\)\nCorrect result\nType II error\n\n\n\nWith probabilities usually represented by:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\n\\(\\alpha\\)\n\\(1-\\beta\\) = “power”\n\n\nDo not reject \\(H_0\\)\n\\(1 - \\alpha\\)\n\\(\\beta\\)"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes-1",
    "href": "slides/Chapter04.html#truth-and-outcomes-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nThe probability of making a Type I error, rejecting \\(H_0\\) when \\(H_0\\) is true, is set by the experimenter a priori (beforehand) as the significance level, \\(\\alpha\\).\n\nSay we set \\(\\alpha\\) at the conventional 0.05. We know that, if \\(H_0\\) is true, we have a 5% chance of rejecting \\(H_0\\) (the Type I error rate is 0.05) and a 95% chance of not rejecting \\(H_0\\).\n\nThe probability of retaining (not rejecting) \\(H_0\\) when \\(H_0\\) is false is usually represented by \\(\\beta\\) (“beta”).\n\nWe usually do not know \\(\\beta\\) because it depends on \\(\\sigma\\), \\(n\\) and also the effect size under the alternative hypothesis. These must be asserted to calculate \\(\\beta\\) and power, \\(1-\\beta\\)."
  },
  {
    "objectID": "slides/Chapter04.html#calculating-power",
    "href": "slides/Chapter04.html#calculating-power",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Calculating power",
    "text": "Calculating power\nThe power of a hypothesis test is the probability of rejecting the null hypothesis if it is indeed false. Basically, “if we’re right, what is the probability that we’ll be able to show it?”\nThe power of the \\(t\\) test can be evaluated using R if you assert the effect size \\(\\delta\\):\n\n\n\npower.t.test(n = 10, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 10\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.5619846\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.9986074\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nThe bigger the sample size, the bigger more power we have to detect an effect.\nOften, experimenters might aim to have a power of 80% or more, so they will most likely reject the null if it is false."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-distributions",
    "href": "slides/Chapter04.html#sampling-distributions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nThe term sampling distribution means the distribution of the computed statistic such as the sample mean when sampling is repeated many times.\nFor a normal population,\n\nStudent’s \\(t\\) distribution is the sampling distribution of the mean (after rescaling).\n\\(\\chi^2\\) distribution is the sampling distribution of the sample variance \\(S^2\\).\n\\((n-1)S^2/\\sigma^2\\) follows \\(\\chi^2\\) distribution.\n\n\\(F\\) distribution is ratio of two \\(\\chi^2\\) distributions.\n\nIt becomes the sampling distribution of the ratio of two sample variances \\(S_1^2/S_2^2\\) from two normal populations (after scaling).\n\n\\(t\\) distribution is symmetric but \\(\\chi^2\\) and \\(F\\) distributions are right skewed. - For large samples, they become normal - For \\(n&gt;30\\), the skew will diminish\nFor the three sampling distributions, the sample size \\(n\\) becomes the proxy parameter, called the degrees of freedom (df).\n\n\\(t_{n-1}\\), \\(\\chi_{n-1}^2\\) & \\(F_{(n_1-1),(n_2-1) }\\)"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test",
    "href": "slides/Chapter04.html#two-sample-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nThe two-sample t-test is a common statistical test. You have two populations, \\(X_1\\) and \\(X_2\\), with means \\(\\mu_1\\) and \\(\\mu_2\\) and variances \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\), respectively.\nWe wish to test whether the two population means are equal.\nNull hypothesis: \\(H_0:\\mu=\\mu_1=\\mu_2\\)\nTwo-sided alternative hypothesis: \\(H_1:\\mu_1 \\neq \\mu_2\\)\nThe basic t-test assumes equal variances; that is, \\(\\sigma^2_1 = \\sigma^2_2\\).\nIf we make this assumption and it is false, the test may give the wrong conclusion!"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with equal variances",
    "text": "Two sample t-test with equal variances\nUnder the assumption of equal variances \\(\\sigma^2_1 = \\sigma^2_2\\), we can perform a pooled-sample t-test.\nWe calculate the estimated pooled variance as:\n\\[s_{p}^{2} = w_{1} s_{1}^{2} +w_{2} s_{2}^{2}\\]\nwhere the weights are \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\),\nand \\(n_1\\) and \\(n_2\\) are the sample sizes for samples 1 and 2, respectively.\nFor the pooled case, the \\(df\\) for the \\(t\\)-test is simply\n\\[df = n_{1}+n_{2}-2\\]"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with unequal variances",
    "text": "Two sample t-test with unequal variances\nThis is often called the “Welch test”.\nWe do not need to calculated the pooled variance \\(s^2_p\\);\nwe simply use \\(s^2_1\\) and \\(s^2_2\\) as they are.\nFor the unpooled case, the \\(df\\) for the test is smaller: \\[df=\\frac{\\left(\\frac{s_{1}^{2}}{n_{1}} +\\frac{s_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{s_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{s_{2}^{2}}{n_{2} } \\right)^{2}}\\] The \\(df\\) doesn’t have to be an integer in this case."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nWe can test the null hypothesis that the variances are equal using either a Bartlett’s test or Levene’s test. Levene’s is generally favourable over Bartlett’s.\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\ncar::leveneTest(TELETIME~factor(SEX), data=tv)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.1789 0.08149 .\n      44                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHighish p-value means there’s no strong evidence against the null hypothesis that the variances are equal. Therefore, we might feel happy to assume equal variances and use a pooled variance estimate."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nt-test assuming equal variances:\n\nt.test(TELETIME ~ factor(SEX), var.equal = TRUE, data=tv)\n\n\n    Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 44, p-value = 0.4723\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -461.3476  217.2606\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304 \n\n\nt-test not assuming equal variances (Welch test):\n\nt.test(TELETIME ~ factor(SEX), data=tv)\n\n\n    Welch Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 40.653, p-value = 0.4727\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -462.1384  218.0514\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "href": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps (some not currently working)",
    "text": "Shiny apps (some not currently working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/\nFor more on non-parametric tests, see Study Guide."
  },
  {
    "objectID": "slides/Chapter04.html#test-of-proportions",
    "href": "slides/Chapter04.html#test-of-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Test of proportions",
    "text": "Test of proportions\nTesting the null hypothesis that a proportion \\(p=0.5\\)\nAlternative hypothesis: \\(p \\ne 0.5\\)\nSay a survey of 1000 beached whales had 450 females.\n\n\nIs the population 50:50?\n\nprop.test(450, 1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  450 out of 1000, null probability 0.5\nX-squared = 9.801, df = 1, p-value = 0.001744\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4189204 0.4814685\nsample estimates:\n   p \n0.45 \n\n\n\nAn exact version of the test\n\nbinom.test(c(450, 550))\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value = 0.001731\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45 \n\n\n\n\nTo test for a proportion other than 0.5\n\nbinom.test(c(450, 550), p = 3/4)\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45"
  },
  {
    "objectID": "slides/Chapter04.html#comparing-several-proportions",
    "href": "slides/Chapter04.html#comparing-several-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Comparing several proportions",
    "text": "Comparing several proportions\nData on smokers in four group of patients\n(from Fleiss, 1981, Statistical methods for rates and proportions).\n\nsmokers  &lt;- c( 83, 90, 129, 70 )\npatients &lt;- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  smokers out of patients\nX-squared = 12.6, df = 3, p-value = 0.005585\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9651163 0.9677419 0.9485294 0.8536585 \n\n\nMore on chi-squared tests next week."
  },
  {
    "objectID": "slides/Chapter04.html#tests-for-normality",
    "href": "slides/Chapter04.html#tests-for-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Tests for normality",
    "text": "Tests for normality\nTesting the hypothesis that sample data came from a normally distributed population. \\(H_0: X \\sim N(\\mu, \\sigma)\\).\n\nKolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)\nShapiro-Wilk test (based on variance of the difference)\n\nExample: a sample of \\(n\\) = 50 from N(100,1).\n\n\n\nset.seed(123)\nshapiro.test(rnorm(50, mean=100))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(50, mean = 100)\nW = 0.98928, p-value = 0.9279\n\n\n\n\nset.seed(123)\nks.test(rnorm(50), \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  rnorm(50)\nD = 0.073034, p-value = 0.9347\nalternative hypothesis: two-sided\n\n\n\n\nThe large p-values mean that there’s no evidence of non-normality.\nLike all tests, they can give the wrong answer. Try with set.seed(1234)."
  },
  {
    "objectID": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "href": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Can also use plots to assess normality",
    "text": "Can also use plots to assess normality\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\n\nggplot(tv, aes(sample = TELETIME)) + \n  stat_qq() + stat_qq_line() +\n  labs(title = \"Normal quantile plot for TV viewing times\")"
  },
  {
    "objectID": "slides/Chapter04.html#transformations",
    "href": "slides/Chapter04.html#transformations",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Transformations",
    "text": "Transformations\n\nIt can be useful to transform skewed data to make the distribution more like a ‘normal’ if that is required for a particular analysis.\nA linear transformation \\(Y^*= a+bY\\) only changes the scale or centre, not the shape of the distribution.\nTransformations can help to improve symmetry, normality, and stabilise the variance."
  },
  {
    "objectID": "slides/Chapter04.html#example",
    "href": "slides/Chapter04.html#example",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nRight skewed distribution is made roughly symmetric using a log transformation for no. of vehicles variable (rangitikei.* dataset)"
  },
  {
    "objectID": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "href": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "A Ladder of Powers for Transforming Data",
    "text": "A Ladder of Powers for Transforming Data\n\nRight skewed data needs a shrinking transformation\nLeft skewed data needs a stretching transformation\nThe strength or power of the transformation depends on the degree of skew.\n\n\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values"
  },
  {
    "objectID": "slides/Chapter04.html#box-cox-transformation",
    "href": "slides/Chapter04.html#box-cox-transformation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\n\n\nThis is a normalising transformation based on the Box-Cox power parameter, \\(\\lambda\\).\nR gives a point estimate of the Box-Cox power & a confidence interval.\nUsually, we are concerned about whether the residuals from a model are normally distributed – put the model in the lm statement.\nSometimes, no “good” value of \\(\\lambda\\) can be found.\n\n\nlibrary(lindia)\ngg_boxcox(lm(rangitikei$vehicle ~ 1))"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "href": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical inference based on transformed data",
    "text": "Statistical inference based on transformed data\n\nObtain the confidence interval using transformed data and then back transform the limits (to keep the original scale)\n\nThe confidence limits calculated on log-transformed data can be exponentiated back to the raw scale (i.e., \\(e^{\\text{confidence limit}}\\))\nThe confidence limits of square-root-transformed data can be squared back to the raw scale (ie. \\({\\text{confidence limit}^2}\\))\n\nFor hypothesis test, apply the same transformation on the value hypothesised under the null\nExplore https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/ app (not currently working)"
  },
  {
    "objectID": "slides/Chapter04.html#example-1",
    "href": "slides/Chapter04.html#example-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nBrain weights of Animals data are right-skewed.\n\n\ndata(Animals, package = \"MASS\")\n\nbind_cols( \n  # confidence intervals on raw data\n  `raw x` = Animals |&gt; \n    pull(brain) |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data\n  `log x` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data \n  # and then back-transformed\n  `log x, CIs back-transformed` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\") |&gt;\n    exp()\n  ) |&gt; \n  kable()\n\n\n\nLower and upper confidence limits \n\n\nraw x\nlog x\nlog x, CIs back-transformed\n\n\n\n\n56.88993\n3.495101\n32.95362\n\n\n1092.15293\n5.355790\n211.83131"
  },
  {
    "objectID": "slides/Chapter04.html#non-parametric-tests",
    "href": "slides/Chapter04.html#non-parametric-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations).\nMany non-parametric methods rely on replacing the observed data by their ranks."
  },
  {
    "objectID": "slides/Chapter04.html#spearmans-rank-correlation",
    "href": "slides/Chapter04.html#spearmans-rank-correlation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\n\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient.\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\nlibrary(GGally)\n\np &lt;- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n\n\n\n\n\n\n\nFigure 1: Comparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\nt.test(tv$TELETIME, mu=1680)\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter04.html#mann-whitney-test",
    "href": "slides/Chapter04.html#mann-whitney-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\nt.test(rangitikei$people~rangitikei$time)\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter04.html#another-form-of-test",
    "href": "slides/Chapter04.html#another-form-of-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Another form of test",
    "text": "Another form of test\n\n\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter04.html#permutation-tests",
    "href": "slides/Chapter04.html#permutation-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Permutation tests",
    "text": "Permutation tests\nA permutation (or randomisation) test has the following steps:\n\nRandomly permute the observed data many times, thereby destroying any real relationship,\nRecalculate the test statistic \\(T\\) for each random permutation\nCompare the observed value of \\(T\\) (from the actual data) with the values of \\(T\\) under permutation.\n\nOne sample hypothesis test example follows:\n\nlibrary(exactRankTests)\nperm.test(tv$TELETIME, null.value=1500)\n\n\n    1-sample Permutation Test\n\ndata:  tv$TELETIME\nT = 79547, p-value = 2.842e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nFor small samples, this approach is not powerful."
  },
  {
    "objectID": "slides/Chapter04.html#two-group-comparison",
    "href": "slides/Chapter04.html#two-group-comparison",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nperm.test(TELETIME~SEX, distribution ='exact', data=tv)\n\n\n    2-sample Permutation Test\n\ndata:  TELETIME by SEX\nT = 38370, p-value = 0.471\nalternative hypothesis: true mu is not equal to 0\n\n\nAlso using a linear model fit (cover later)\n\nlibrary(lmPerm)\nsummary(lmp(TELETIME~SEX, data=tv))\n\n[1] \"Settings:  unique SS : numeric variables centered\"\n\n\n\nCall:\nlmp(formula = TELETIME ~ SEX, data = tv)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1178.261  -510.793    -7.283   402.989  1130.739 \n\nCoefficients:\n    Estimate Iter Pr(Prob)\nSEX      122   51     0.98\n\nResidual standard error: 570.9 on 44 degrees of freedom\nMultiple R-Squared: 0.0118, Adjusted R-squared: -0.01066 \nF-statistic: 0.5255 on 1 and 44 DF,  p-value: 0.4723 \n\n\nRead the study guide example for bootstrap tests (not examined)"
  },
  {
    "objectID": "slides/Chapter04.html#summary",
    "href": "slides/Chapter04.html#summary",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Summary",
    "text": "Summary\nBasic methods of inference include:\n\nestimating a population parameter (e.g. mean) using a sample of values,\nestimating standard deviation of a sample estimate using the standard error,\nconstructing confidence intervals for an estimate, and\ntesting hypotheses about particular values of the population parameter.\n\nInference is relatively easy for normally distributed populations.\nStudent’s t-tests include:\n\none-sample t-test, including paired-sample t-test for a difference of zero\ntwo-sample t-test assuming equal variances (estimating pooled variance)\ntwo-sample t-test not assuming equal variances (Welch test)\n\nThe t-test is generally robust for non-normal populations (especially for large samples).\nPower transformations, such as square-root, log, or Box-Cox aim to reduce skewness.\nNon-parametric tests can be used for non-normal data, but they are usually less powerful than parametric tests.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter05.html#analysing-frequencies",
    "href": "slides/Chapter05.html#analysing-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Analysing frequencies",
    "text": "Analysing frequencies\n\nThis chapter focuses on data that consist of frequencies or counts of occurrences of some events\nWish to compare observed with what we would have expected\nWe will cover the \\(\\chi ^ 2\\) distribution, “Goodness-of-fit” tests, and test of independence"
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-distribution",
    "href": "slides/Chapter05.html#chi-squared-distribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi squared distribution",
    "text": "Chi squared distribution\n\n\nContinuous 0 to infinity; starts at 0 because it is a square (a square number can’t be negative)\nThe mean of this distribution is its degrees of freedom (k)\nIt is right skewed, mean greater than median and mode; variance is \\(2k\\)\nShape of the \\(\\chi ^ 2\\) distribution is determined by the degrees of freedom (k), at very high k (90 or greater) \\(\\chi ^ 2\\) distribution resembles the normal distribution\nMain purpose is hypothesis testing, not describing real-world distributions"
  },
  {
    "objectID": "slides/Chapter05.html#tables-and-frequencies",
    "href": "slides/Chapter05.html#tables-and-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tables and frequencies",
    "text": "Tables and frequencies\n\nWe often have hypotheses regarding the frequencies of levels of a factor or group of factors.\n\nFor a single two-level factor (e.g., male vs female, survived vs died), we might wish to know how likely the data are to have come from a population with equal proportions (or some other specified proportion).\nFor two factors, we might wish to know whether they are independent.\n\nIn either case, we can specify a null hypothesis and test it using data.\nTo do so, compare observed counts with expected counts, where the expected counts are derived from our null model about the population.\nWe are employing the Chi-squared statistic with data which consists of integers. That is, the data are discrete rather than continuous."
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 1\n\nA dataset contains 40 males and 50 females.\nHow plausible is the null model of these counts coming from a population with 50% males and 50% females?\nThe expected counts in this case would be 45 males and 45 females."
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 2\n\n\n\nDoes the distribution of rejects of metal castings by causes in a particular week vary from the long-term average counts?\nTreat the long-term average counts as the expected counts.\nCompare observed counts with expected counts.\n\n\n\n\n\nCauses of rejection\nRejects during the week\nLong-term average\n\n\n\n\nsand\n90\n82\n\n\nmisrun\n8\n4\n\n\nshift\n16\n10\n\n\ndrop-\n8\n6\n\n\ncorebreak\n23\n21\n\n\nbroken\n21\n20\n\n\nother\n5\n8"
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic",
    "href": "slides/Chapter05.html#chi-squared-test-statistic",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\n\\[\n\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\rm Observed-Expected}\\right)^{{\\rm 2}} }{{\\rm Expected}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E}\n\\]\nIf the number of categories is \\(c\\), then the degrees of freedom is \\(c-1\\)."
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "href": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\nIf the null hypothesis were true, then the value of \\(\\chi ^{2}\\) calculated from our data is a random value from a Chi-squared distribuion: \\[\n\\chi_{0} ^{2} = \\chi_{c-1} ^{2}\n\\] Once we calculate the test statistic, we can compare our observed value with its distribution under \\(H_{0}\\) to calculate a p-value."
  },
  {
    "objectID": "slides/Chapter05.html#assumptions",
    "href": "slides/Chapter05.html#assumptions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe classification of observations into groups must be independent\nNo more than 20% of categories should have expected counts less than 5"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-test",
    "href": "slides/Chapter05.html#goodness-of-fit-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit test",
    "text": "Goodness of fit test\n\nCompare observed frequencies with expected frequencies under some specified null hypothesis.\nMake a hypothesis about the population, what would we expect the frequency to be under that hypothesis?\nFor example:\n\nWish to compare the frequency of occurence of different phenotypes in an organism with the frequencies we would expect under Mendel’s laws of inheritance.\nWish to compare the distribution of a observation with expected count we would obtain from a Poisson distribution.\n\n\nHypotheses of this sort can be tested using the Chi-squared test statistic (\\(\\chi ^ 2\\) = Ki Sq.)"
  },
  {
    "objectID": "slides/Chapter05.html#example",
    "href": "slides/Chapter05.html#example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nA survey of voters included 550 males and 450 females. Will you call this survey as a biased one?\nHere the null hypothesis is that the ratio of males to females is 1:1.\nEquivalent: the proportion of males is equal to the proportion of female in the population\n\n\n\nGender\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\nmale\n550\n500\n\\((550-500)^2/ 500=5\\)\n\n\nfemale\n450\n500\n\\((450-500)^2/ 500=5\\)\n\n\nsum\n1000\n1000\n10\n\n\n\n\\[\n\\chi ^{2} = \\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E} =\n\\frac{(550-500)^2}{500} + \\frac{(450-500)^2}{500} = 10\n\\]"
  },
  {
    "objectID": "slides/Chapter05.html#example-3",
    "href": "slides/Chapter05.html#example-3",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\ndf= (2-1)=1\nAt 5% level (\\(\\alpha =0.05\\)), the critical value is only 3.84. So the sample is a biased one."
  },
  {
    "objectID": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "href": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Mendel’s experiment (see Study Guide)",
    "text": "Mendel’s experiment (see Study Guide)\n\n\nMendel discovered the principles of heredity by breeding garden peas. In one Mendel’s trials ratios of various types of peas (dihybrid-crosses) were 9:3:3:1\nThe observed results are very close to expected results. This results in a small chi squared value. Were experimental results fudged or was there a confirmation bias?"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions",
    "text": "Goodness of fit for distributions\n\nTreat class intervals as categories and obtain the actual counts (O)\nThe assumed distribution gives the expected counts (E)\nPerform a goodness of fit and validate the assumed theoretical distribution\nAdjust the degrees of freedom (df) for the number of estimated parameters of the theoretical distribution\n\nFor example, assume that you have 10 class intervals and test for normal distribution, which has 2 parameters.\n\nSo the df for this test will be 10-1-2=7."
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\nA safety inspector monitors car accidents at a bustling intersection. The inspector enters the counts of monthly accidents.\nNull: The sample data follow the Poisson distribution. Alternative: The sample data do not follow the Poisson distribution.\nNote The Poisson distribution is a discrete probability distribution (integers) that can model counts of events or attributes in a fixed observation space. Many but not all count processes follow this distribution."
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\n\n\n\nAccidents\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\n0\n7\n\n\n\n\n1\n8\n\n\n\n\n2\n13\n\n\n\n\n3\n10\n\n\n\n\n&gt;=4\n12\n\n\n\n\nSum\n40\n\n\n\n\n\nConclusion:"
  },
  {
    "objectID": "slides/Chapter05.html#tests-of-independence",
    "href": "slides/Chapter05.html#tests-of-independence",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tests of Independence",
    "text": "Tests of Independence\n\nIn some cases, we have counts of observations cross-classified in terms of two factors.\nWe are generally interested in determining whether or not the two factors are independent.\nIf we consider the observations falling into each category for factor 1, is this distribution consistent across all levels of factor 2? (or vice versa)"
  },
  {
    "objectID": "slides/Chapter05.html#contingency-table",
    "href": "slides/Chapter05.html#contingency-table",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Contingency table",
    "text": "Contingency table\n\nGiven a two way table of frequency counts, we test whether the row and column variables are independent\nHypotheses:\n\nNull: the two factors are independent\nwritten another way: the row (or column) distributions are the same\n\nThe expected count for cell \\((i,j)\\) is given by \\(E_{ij}\\) = \\((T_i \\times T_j)/n\\) where\n\\(~~~~~T_i\\), the total for row \\(i\\);\n\\(~~~~~T_j\\), the total for column \\(j\\)\n\\(~~~~~n\\), the overall total count\nTest statistic : \\(\\chi ^{2} =\\sum _{{i=1}}^{r}\\sum _{{j=1}}^{{\\rm c}}\\frac{\\left({ O}_{{ ij}} { -E}_{{ij}} \\right)^{{ 2}} }{{ E}_{{ij}} }.\\)\ndegrees of freedom: \\((r-1)(c-1)\\)"
  },
  {
    "objectID": "slides/Chapter05.html#example-4",
    "href": "slides/Chapter05.html#example-4",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nContext: Porcine Stress Syndrome (PSS) result in pale, soft meat in pigs and under conditions of stress- death.\n\nPresence of PPS is a positive reaction to breathing halothane.\n\nSelective breeding for reducing incidence of PSS\n\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#example-5",
    "href": "slides/Chapter05.html#example-5",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343\n\n\n\n\n\n\n\n\nchisq.test(dt)\n\n\n    Pearson's Chi-squared test\n\ndata:  dt\nX-squared = 16.433, df = 8, p-value = 0.03659"
  },
  {
    "objectID": "slides/Chapter05.html#computations",
    "href": "slides/Chapter05.html#computations",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Computations",
    "text": "Computations\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#inference",
    "href": "slides/Chapter05.html#inference",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Inference",
    "text": "Inference\n\nThe tabulated Chisq with (4-1)x(2-1) = 3 d.f are\n\n7.81 at 5% level; 16.27 at 1% level.\n\n\nConclusion: There is a statistical evidence that the breed is not independent of the result of the Halothane test.\n\n\nNote that large counts in the second column (Halothane negative) lead to large expected values but low contributions to chi-squared statistic.\nThis is because of the division by the appropriate expected value. On the other hand, the small observations of first column lead to small expected values but large contributions to the \\(\\chi ^{2}\\)."
  },
  {
    "objectID": "slides/Chapter05.html#significant-cell-contribution",
    "href": "slides/Chapter05.html#significant-cell-contribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Significant cell contribution",
    "text": "Significant cell contribution\n\nCounts follow Poisson distribution for which mean = variance\nHence \\(\\chi^{2}=\\sum \\frac{\\left({\\rm residual}\\right)^{{\\rm 2}} }{{\\rm variance}} =\\sum \\left(\\frac{{\\rm residual}}{{\\rm std\\; dev}} \\right)^{2}.\\)\nIndividual cell contribution is similar to standardized residual. Any standardized residual greater than 2 is regarded as significant.\nSo \\(2^2 = 4\\) is treated as a significant contribution to the \\(\\chi ^{2}\\) statistic."
  },
  {
    "objectID": "slides/Chapter05.html#warnings",
    "href": "slides/Chapter05.html#warnings",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Warnings",
    "text": "Warnings\n\nUse only frequency counts. Use of percentages in place of counts may lead to incorrect conclusions.\nCheck for small expected values. An expected value of less than 5 may lead to concern and a very small value of less than 1 is a warning. Sometimes, you can merge/combine categories in case of small expected counts.\nIf the chi-squared statistic is small enough to be not significant, there is no problem.\nIf chi-squared statistic is significant, check the contributions to each cell. If cells with large expected value (&gt;5) contribute a large amount to chi-squared statistic, again there is no problem.\nIf cells with expected values less than 5 lead large contributions to chi-squared statistic, the significance of the chi-squared statistic should be treated with caution."
  },
  {
    "objectID": "slides/Chapter05.html#simpsons-paradox",
    "href": "slides/Chapter05.html#simpsons-paradox",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nGroup 1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nGroup 2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nAfter amalgamation of both groups\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166"
  },
  {
    "objectID": "slides/Chapter05.html#permutation-test",
    "href": "slides/Chapter05.html#permutation-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Permutation test",
    "text": "Permutation test\nThis test is done maintaining the marginal totals.\nData: Smoking Status vs. Staff  Groupings\n\n\n\nSmoking Status vis-a-vis Staff Groupings\n\n\n\nNone\nModerate\nHeavy\nTotals\n\n\n\n\nJunior employees\n18\n57\n13\n88\n\n\nJunior managers\n4\n10\n4\n18\n\n\nSecretaries\n10\n13\n2\n25\n\n\nSenior employees\n25\n22\n4\n51\n\n\nSenior managers\n4\n5\n2\n11\n\n\nTotals\n61\n107\n25\n193\n\n\n\n\n\n\n\nPermutation test\n\nset.seed(12321)\nchisq.test(tabledata, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tabledata\nX-squared = 15.672, df = NA, p-value = 0.04948\n\n\nRegular Chi-square test\n\nchisq.test(tabledata)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#correspondence-analysis",
    "href": "slides/Chapter05.html#correspondence-analysis",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\nCorrespondence Analysis is an exploratory statistical technique for assessing the interdependence of categorical variables whose data are presented primarily in the form of a two-way table of frequencies\nData: Smoking Status vs. Staff Groupings\n\n\n                 None Moderate Heavy\nJunior employees   18       57    13\nJunior managers     4       10     4\nSecretaries        10       13     2\nSenior employees   25       22     4\nSenior managers     4        5     2\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#row-mass-row-profiles",
    "href": "slides/Chapter05.html#row-mass-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Row mass & row profiles",
    "text": "Row mass & row profiles\n\nrow profiles are found dividing the cell counts by the corresponding row total\nrow mass is found dividing the row totals by the grand total\n\n\n\n                  None Moderate  Heavy\nJunior employees 0.205    0.648 0.1477\nJunior managers  0.222    0.556 0.2222\nSecretaries      0.400    0.520 0.0800\nSenior employees 0.490    0.431 0.0784\nSenior managers  0.364    0.455 0.1818\nrowmass          0.316    0.554 0.1295\n\n\n\nSimilarly column profiles & column masses can be found"
  },
  {
    "objectID": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "href": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Graphical display of row profiles",
    "text": "Graphical display of row profiles\n\n\nNo clear patterns seen"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping\n\nlibrary(\"FactoMineR\")\nCA(tabledata) |&gt; summary()\n\n\n\nCall:\nCA(X = tabledata) \n\nThe chi square of independence between the two variables is equal to 15.7 (p-value =  0.0473 ).\n\nEigenvalues\n                       Dim.1   Dim.2\nVariance               0.074   0.008\n% of var.             90.570   9.430\nCumulative % of var.  90.570 100.000\n\nRows\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nJunior employees |    26.268 | -0.235 34.372  0.962 | -0.047 12.934  0.038 |\nJunior managers  |     8.784 | -0.236  7.042  0.590 |  0.197 47.082  0.410 |\nSecretaries      |     5.618 |  0.195  6.670  0.873 | -0.074  9.301  0.127 |\nSenior employees |    37.894 |  0.379 51.521  1.000 |  0.004  0.045  0.000 |\nSenior managers  |     2.636 |  0.071  0.394  0.110 |  0.203 30.638  0.890 |\n\nColumns\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nNone             |    49.186 |  0.394 66.705  0.997 |  0.020  1.688  0.003 |\nModerate         |    15.679 | -0.157 18.619  0.873 | -0.060 25.941  0.127 |\nHeavy            |    16.335 | -0.289 14.676  0.661 |  0.207 72.371  0.339 |"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping"
  },
  {
    "objectID": "slides/Chapter05.html#summary",
    "href": "slides/Chapter05.html#summary",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Summary",
    "text": "Summary\n\nGoodness of fit is for testing whether the observed counts are from the hypothesised population groups.\n\nFor \\(c\\) categories (groups), the test involves \\(c-1\\) df.\n\nContingency Table (\\(r\\) rows and \\(c\\) columns) data are tested for the independence.\n\nFor \\(r\\times c\\) cells the test involves \\((r-1)(c-1)\\) df\n\n\\(\\chi ^{2}\\) test works well if \\(E&gt; 5\\). Some could be as low as 1.\nDo correspondence analysis (symmetric plots) when independence is rejected.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model",
    "href": "slides/Chapter06.html#what-is-a-statistical-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\nFIT- to explain systematic (non-random) variation in the data\nRESIDUAL - to explain random variation in the data"
  },
  {
    "objectID": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "href": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Analysis of two quantitiative variables",
    "text": "Analysis of two quantitiative variables\n\nWith paired (related) data (X,Y)\nTwo variables: one (Y) is random (response) and the other (X) is considered fixed (predictor)\nInterested in the functional relationship between these two variables \\(Y=f(X)\\) to predict Y, given X\nInterested in estimates of coefficients"
  },
  {
    "objectID": "slides/Chapter06.html#regression",
    "href": "slides/Chapter06.html#regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\nStatistical Model\n\nFitted Model: \\(\\hat{Y}=a+bX\\) (True Model: \\(Y=\\alpha+\\beta X+\\epsilon\\))\nresidual error: \\(e= Y-\\hat{Y}\\) (\\(\\epsilon\\) is not the same as \\(e\\))"
  },
  {
    "objectID": "slides/Chapter06.html#regression-1",
    "href": "slides/Chapter06.html#regression-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\n\nA regression equation is a function that indicates how the average value of one response variable for given values of one or more predictor variables varies with these predictor variables; that is, \\(E(Y|X_1, X_2, ..., X_k)\\)"
  },
  {
    "objectID": "slides/Chapter06.html#simple-regression",
    "href": "slides/Chapter06.html#simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Simple regression",
    "text": "Simple regression\n\nThe term simple means there is a single predictor\nThe fitted model remains as: \\(\\hat{Y}=a+bX_{i}\\)\n\n\n\\(\\hat{Y}\\) is the predicted value of \\(y_{i}\\) given \\(x_{i}\\). Also called ‘fitted’ values, they are the linear funtion of X.\n\\(a\\) the intercept on the y-axis; value of \\(\\hat{Y}\\) when \\(x_{i}=0\\).\n\\(b\\) the slope of the line; the change in \\(\\hat{Y}\\) with a 1-unit increase in \\(x_{i}\\)."
  },
  {
    "objectID": "slides/Chapter06.html#fitting-a-regression",
    "href": "slides/Chapter06.html#fitting-a-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitting a regression",
    "text": "Fitting a regression\n\nWant a line that is as ‘close’ as possible to the existing data points we have\nThe method of least squares is employed to obtain the estimates \\(a\\) and \\(b\\)\n\nThe sum of squared residuals is minimized in the least squares method."
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n\n country Alcohol Death\n       1    22.9  30.0\n       2    15.2  23.6\n       3    12.3  18.9\n       4    11.9   5.0\n       5    10.8  12.3\n       6     9.9  14.2\n       7     8.3   7.4\n       8     7.2   3.0\n       9     6.6   7.2\n      10     5.8  10.6\n      11     5.7   3.7\n      12     5.6   3.4\n      13     4.2   4.3\n      14     3.9   3.6\n      15     3.1   5.4"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data\n\n\\(min \\left( \\sum_{i=1}(y_{i}-\\hat{y_{i}})^2 \\right)\\)\n\n\\(i\\) number of observations\n\\(y_{i}\\) observed value of y\n\\(\\hat{y_{i}}\\) predicted value of y"
  },
  {
    "objectID": "slides/Chapter06.html#r-base-output",
    "href": "slides/Chapter06.html#r-base-output",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "R Base Output",
    "text": "R Base Output\n\nmod1 &lt;- lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nLet us tidy it."
  },
  {
    "objectID": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "href": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitted model and testing its coefficients",
    "text": "Fitted model and testing its coefficients\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n\nFocus on the model, its coefficients\n\nWhat is the scale of these variables? Are these coefficient estimates meaningful in the context?\nWhat is the error around these estimates?"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions",
    "href": "slides/Chapter06.html#assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nModel forming assumptions\n\n\\(X\\) is known without error\n\\(Y\\) is linearly related to \\(X\\)\nThere is a random variability of \\(Y\\) about this line"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions-1",
    "href": "slides/Chapter06.html#assumptions-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nMore assumptions to form \\(t\\) and \\(F\\) statistics:\n\nVariability in \\(Y\\) about the line is constant and independent of \\(X\\) variable.\nThe variability of \\(Y\\) about the line follows normal distribution.\nThe distribution of \\(Y\\) given \\(X = X_i\\) is independent of \\(Y\\) given \\(X = X_j\\)."
  },
  {
    "objectID": "slides/Chapter06.html#residuals-and-assumptions",
    "href": "slides/Chapter06.html#residuals-and-assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals and assumptions",
    "text": "Residuals and assumptions\n\nMost assumptions are on the errors (residuals)\n\nindependent\nnormal\nrandom\n\nIs there a pattern in my residuals?\nDo they suggest what to do?"
  },
  {
    "objectID": "slides/Chapter06.html#residuals",
    "href": "slides/Chapter06.html#residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/Chapter06.html#types-of-residuals",
    "href": "slides/Chapter06.html#types-of-residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Types of residuals",
    "text": "Types of residuals\n\nraw or ordinary residual is just (observation-fit)\nStandardized Residual= Residual/Std.Dev of residual\nThe regression model is influenced by outliers or unusual points because the slope estimate of the regression line is sensitive to these outliers.\nSo we define Studentised or deleted t Residual\nSimilar to Standardized Residual without the observation under consideration. That is,\n\nStudentised residual = residual/std. dev of residual (after omitting the particular observation)."
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\nFor small sample sizes, residual diagnostics is difficult"
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\npar(mfrow=(c(2,2)))\nplot(mod1)"
  },
  {
    "objectID": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "href": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals showing need for transformation",
    "text": "Residuals showing need for transformation\n\nNon-constant Residual Variation"
  },
  {
    "objectID": "slides/Chapter06.html#adding-more-predictors",
    "href": "slides/Chapter06.html#adding-more-predictors",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Adding more predictors",
    "text": "Adding more predictors"
  },
  {
    "objectID": "slides/Chapter06.html#subgrouping-patterns",
    "href": "slides/Chapter06.html#subgrouping-patterns",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Subgrouping patterns",
    "text": "Subgrouping patterns"
  },
  {
    "objectID": "slides/Chapter06.html#outliers",
    "href": "slides/Chapter06.html#outliers",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/Chapter06.html#autocorrelation",
    "href": "slides/Chapter06.html#autocorrelation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nNeighbouring residuals depend on each other"
  },
  {
    "objectID": "slides/Chapter06.html#improving-simple-regression",
    "href": "slides/Chapter06.html#improving-simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Improving simple regression",
    "text": "Improving simple regression\n\nUse a different predictor or explanatory variable\nTransform the \\(Y\\) variable\nAdd other explanatory variables to the model (next week)\nDeletion of invalid (as opposed to outlier) observations\nReconsider the linear relationship"
  },
  {
    "objectID": "slides/Chapter06.html#tests-of-significance",
    "href": "slides/Chapter06.html#tests-of-significance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Tests of significance",
    "text": "Tests of significance\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n-   $t$ tests of intercept and slope\n\n-   $H_0=true~slope=0$; $H_0=true~intercept=0$\n\n    -   For `Alcohol~deaths` model, the slope is significant\n        but not the intercept"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures",
    "href": "slides/Chapter06.html#model-quality-measures",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nModel summary (or Quality) measures\n\n\\(R^2\\) is the proportion of variation explained by the fitted model\n\nA meaningful model must have at least 50% \\(R^2\\)\nA large \\(R^2\\) is important to explain the relationship(s)\n\nResidual standard deviation (error) \\(S\\) has to be small\n\nHow small? Difficult to say. Compare \\(S\\) with the overall spread in \\(Y\\) or with the mean of \\(Y\\)\nA small \\(S\\) is important for prediction"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures-1",
    "href": "slides/Chapter06.html#model-quality-measures-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\n\nout1 &lt;- glance(mod1) |&gt; \n  select(r.squared, sigma, statistic, p.value) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nout1 |&gt; t() |&gt; \n  kable(caption = \"Model summary measures\") |&gt; \n  kable_classic(full_width = T) \n\n\n\nModel summary measures\n\n\n\n\n\n\n\n\nr.squared\n0.79\n\n\nsigma\n3.94\n\n\nstatistic\n48.35\n\n\np.value\n0.00"
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained",
    "href": "slides/Chapter06.html#variance-explained",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\nsummary(ols)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05"
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained-1",
    "href": "slides/Chapter06.html#variance-explained-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\nSS &lt;- anova(ols) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term df    sumsq\n   Alcohol  1 751.4408\n Residuals 13 202.0286\n     Total 14 953.4693\n\n\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\nwhere:\n\\(MS~Error\\) = \\(frac{SSE}{n-k-1}\\)\n\\(MS~Total\\) = \\(frac{SST}{n-1}\\)\ntherefore \\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)"
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test",
    "href": "slides/Chapter06.html#anova-f-test",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n\n\nmod1  &lt;-  lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1) \n\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\n\n\nanova(mod1)\n\n\nAnalysis of Variance Table\n\nResponse: Death\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAlcohol    1 751.44  751.44  48.353 1.001e-05 ***\nResiduals 13 202.03   15.54                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmod1.aov  &lt;-  aov(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1.aov) \n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nAlcohol      1  751.4   751.4   48.35  1e-05 ***\nResiduals   13  202.0    15.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFor the straight line model, the F-test is equivalent to testing the hypothesis that the true slope is zero."
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test-1",
    "href": "slides/Chapter06.html#anova-f-test-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n-   Significant F ratio need not always imply that the\n    straight line is the best fit to the data.\n-   For `Alcohol~deaths` model, the $F$ statistic is \n    significant which means that the fitted model explains significant variation\n\n\nout1 &lt;- anova(mod1) |&gt; tidy() |&gt; mutate_if(is.numeric, round, 2)\n\noptions(knitr.kable.NA = \" \")\n\nkable(out1, caption = \"ANOVA table\") |&gt; \n  kable_classic(full_width = F) \n\n\n\nANOVA table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAlcohol\n1\n751.44\n751.44\n48.35\n0\n\n\nResiduals\n13\n202.03\n15.54"
  },
  {
    "objectID": "slides/Chapter06.html#anova-table-construction",
    "href": "slides/Chapter06.html#anova-table-construction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA table construction",
    "text": "ANOVA table construction\n\nEach source has an associated degrees of freedom.\nFor regression, DF = \\(1\\) as there are two parameters \\(a\\) and \\(b\\) fitted in the model\nFor total, DF = \\(n-1\\) since there are n observations\nFor error, DF = by subtraction = \\((n-1)-1 = n-2\\)\nMean Square (MS) values are obtained as MS = SS/DF\nF ratio for regression =MS(Regression)/MS(Error)\nF ratio follows the \\(F\\) distribution with \\((1, n-2)\\) d.f and provides the significance of the model fitted.\n\nthe ratio of two sample variances (or MS) follows the \\(F\\) distribution (normal case)"
  },
  {
    "objectID": "slides/Chapter06.html#prediction",
    "href": "slides/Chapter06.html#prediction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Prediction",
    "text": "Prediction\n\nThe predicted response at value \\(x=x_0\\) is obtained using the fitted regression equation.\nConfidence & prediction intervals can also be constructed.\nNote that prediction intervals are for individual observations whereas the confidence intervals are for the expected (mean) response for a given \\(x_0\\)\n\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"confidence\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 9.476416 13.97915\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"prediction\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 2.918701 20.53686"
  },
  {
    "objectID": "slides/Chapter06.html#outlier-effect-on-regression",
    "href": "slides/Chapter06.html#outlier-effect-on-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outlier effect on regression",
    "text": "Outlier effect on regression\n\nScatter plot of People vs Vehicle"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance\n\n\nA distant \\(x\\) value has a higher leverage.\n\nThis leverage is often measured by the \\(h_{ii}\\) or hi value\nCheck \\(h_{ii}\\) &gt; \\(\\frac{3p}{n}\\) or not\n\nInfluence of a point on the regression is measured using the Cook’s distance \\(D_i\\)\n\nrelated to difference between the regression coefficients with and without the \\(i^{th}\\) data point.\n\\(D_{i} &gt;0.7\\) can be deemed as being influential (for \\(n&gt;15\\))"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance"
  },
  {
    "objectID": "slides/Chapter06.html#robust-regression-models",
    "href": "slides/Chapter06.html#robust-regression-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust regression models",
    "text": "Robust regression models\nRobust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates.\nFor example, least squares estimates for regression models are highly sensitive to outliers: an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss, and therefore has more leverage over the regression estimates."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models",
    "href": "slides/Chapter06.html#robust-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust models",
    "text": "Robust models\nWe can fit a robust linear model using the functions MASS::rlm() robustbase::lmrob()."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-1",
    "href": "slides/Chapter06.html#robust-models-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\nTo determine if a robust regression model offers a better fit to the data compared to the OLS model, we can calculate the residual standard error of each model.\nThe residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model. The lower the value for RSE, the more closely a model is able to fit the data."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-2",
    "href": "slides/Chapter06.html#robust-models-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\n\n#fit least squares regression model\nols &lt;- lm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of ols model\nsummary(ols)$sigma\n\n[1] 3.942164\n\n#fit robust regression model\nrobust &lt;- rlm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of robust model\nsummary(robust)$sigma\n\n[1] 3.417522"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv",
    "href": "slides/Chapter06.html#cross-validation-cv",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nA tool to avoid overfitting (more important with multiple predictors)\nEvaluate performance of model"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv-1",
    "href": "slides/Chapter06.html#cross-validation-cv-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nSplit the sample data randomly into (equal) k subsets (parts) by resampling.\nFit the model for \\((k − 1\\)) subsets of the data\nPredict for the omitted subsets\nCompare prediction errors\nRepeat for each subsets of the data"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-types",
    "href": "slides/Chapter06.html#cross-validation-types",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation Types",
    "text": "Cross Validation Types\n\nK-fold\nStratified K-fold\n\nSame as K-fold but splitting data governed by criteria so that each subset has the same proportion of obervations of a given categorical value\n\nLeave One Out (LOO)\n\nLeaves out a single data point, then uses the same process as K-fold CV\n\nR has many packages for cross validation"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n# CV using Root Mean Sq Error as measure of prediction error\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit &lt;- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses &lt;- lmfit$resample[,1]\nrlmfit &lt;- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses &lt;- rlmfit$resample[,1]\ndfm  &lt;-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter06.html#choosing-the-best-model",
    "href": "slides/Chapter06.html#choosing-the-best-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Choosing the best model",
    "text": "Choosing the best model\n\nThe best model is not decided purely on statistical grounds.\nIf the main aim is to describe relationships, include all the relevant variables.\nIf the main aim is to predict, prefer the simplest feasible (parsimonious) model with smaller number of predictors.\n\nExamine the literature to discover similar examples, see how they are tackled, discuss the matter with the researcher etc."
  },
  {
    "objectID": "slides/Chapter06.html#main-points",
    "href": "slides/Chapter06.html#main-points",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Main points",
    "text": "Main points\nConcepts and practical skills you should have at the end of this chapter:\n\nUnderstand and be able to perform a simple linear regression on bivariate related data sets\nUse scatter plots or other appropriate plots to visualize the data and regression line\nExamine residual diagnostic plots and test assumptions, then perform appropriate transformations as necessary\nSummarize regression results and appropriate tests of significance. Interpret these results in context of your data\nUse a regression line to predict new data and explain confidence and prediction intervals\nUnderstand and explain the concepts of robust regression modeling, and cross-validation.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives",
    "href": "slides/Chapter07.html#learning-objectives",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nUnderstand and describe a multiple linear regression, the difference from a simple linear regression, and when each are appropriate\nFit and display a multiple regression\nUnderstand and interpret different types of Sums of Squares\nUnderstand and test for multicollinearity and other assumptions"
  },
  {
    "objectID": "slides/Chapter07.html#what-is-a-multiple-regression",
    "href": "slides/Chapter07.html#what-is-a-multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What is a multiple Regression?",
    "text": "What is a multiple Regression?\n\nIn a simple regression, there is only one predictor.\nMultiple regression modelling involves many predictors."
  },
  {
    "objectID": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "href": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When to use multiple predictors",
    "text": "When to use multiple predictors\n\nStatistical control of a confound: controlling treatment for some unwanted variability\nMultiple causation: multiple things are thought to cause changes in the outcome variable\nInteractions: we are interested in how two variables may combine to change our outcome (will cover this in the next chapter)"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression",
    "href": "slides/Chapter07.html#multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWhere to start:\n\nQuestion and hypothesis + domain knowledge\n\nWhat are we interested in testing?\nWhat data is collected? do those variables make sense?\nWhat kind of data are my response and predictor variables?\n\nPerform EDA first\n\nA scatter plot matrix can show nonlinear relationships\nA correlation matrix will only show the strength of pairwise linear relationships\n\nLook for the predictors having the largest correlation with response\n\nLook for inter-correlations between the predictors and choose the one with high correlation with response variable but uncorrelated with the rest."
  },
  {
    "objectID": "slides/Chapter07.html#data-example",
    "href": "slides/Chapter07.html#data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\nBasketball team summary for 2020 regular season\n\nWe have 2118 data points which equates to 1059 games (2 teams per game), 2020 was a short season\nEach team in a standard season plays 82 games. That means in total, an NBA season is comprised of 1,230 games.\n\n\n  Team     Game Spread Outcome MIN PTS       P2p       P3p       FTp OREB DREB\n1  ATL 21900014     17       W  48 117 0.6000000 0.3548387 0.8571429    8   34\n2  ATL 21900028      4       W  48 103 0.6296296 0.3000000 0.5333333    9   43\n3  ATL 21900043     -2       L  48 103 0.4736842 0.3333333 0.6875000    8   37\n4  ATL 21900052    -15       L  48  97 0.5454545 0.2820513 0.6666667    9   24\n5  ATL 21900066     -9       L  48  97 0.5370370 0.2058824 0.6923077   16   34\n6  ATL 21900099      8       W  48 108 0.5333333 0.3666667 0.6875000    9   39\n  AST TOV STL BLK PF\n1  27  13   9   2 15\n2  22  18   5   9 26\n3  23  21  12   3 25\n4  28  20  14   7 29\n5  20  16   5   5 15\n6  22  18   9   5 20\n\n\nSpread: Point difference between winning and losing team\nPTS: Total points scored by a team in a game\nP2p: Percent of 2-pointers made\nP3p: Percent of 3-pointers made\nFTp: Percent of free-throws made\nOREB: Offensive rebounds\nDREB: Defensive rebounds\nAST: Assists\nSTL: Steals\nBLK: Blocks"
  },
  {
    "objectID": "slides/Chapter07.html#data-example-1",
    "href": "slides/Chapter07.html#data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\n\noptions(digits=3)\ncor(teams[,c(3, 6:12, 14:15)])\n\n        Spread     PTS      P2p      P3p     FTp     OREB     DREB     AST\nSpread  1.0000  0.5730  0.36338  0.42719  0.1063 -0.00240  0.46470  0.3477\nPTS     0.5730  1.0000  0.48713  0.55094  0.1897 -0.01487  0.14131  0.5103\nP2p     0.3634  0.4871  1.00000 -0.00601 -0.0105 -0.28585  0.02484  0.3633\nP3p     0.4272  0.5509 -0.00601  1.00000  0.0297 -0.18736  0.00164  0.4054\nFTp     0.1063  0.1897 -0.01053  0.02967  1.0000 -0.08960 -0.01724 -0.0297\nOREB   -0.0024 -0.0149 -0.28585 -0.18736 -0.0896  1.00000  0.03882 -0.0996\nDREB    0.4647  0.1413  0.02484  0.00164 -0.0172  0.03882  1.00000  0.0468\nAST     0.3477  0.5103  0.36333  0.40540 -0.0297 -0.09960  0.04681  1.0000\nSTL     0.1246  0.0352  0.01897 -0.04479  0.0115 -0.00389 -0.18027  0.0518\nBLK     0.1837  0.0623  0.03757  0.00540 -0.0185  0.00311  0.20700  0.0411\n            STL      BLK\nSpread  0.12460  0.18367\nPTS     0.03525  0.06234\nP2p     0.01897  0.03757\nP3p    -0.04479  0.00540\nFTp     0.01153 -0.01849\nOREB   -0.00389  0.00311\nDREB   -0.18027  0.20700\nAST     0.05178  0.04112\nSTL     1.00000  0.03267\nBLK     0.03267  1.00000"
  },
  {
    "objectID": "slides/Chapter07.html#inter-relationships",
    "href": "slides/Chapter07.html#inter-relationships",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Inter-relationships",
    "text": "Inter-relationships"
  },
  {
    "objectID": "slides/Chapter07.html#full-regression-in-r",
    "href": "slides/Chapter07.html#full-regression-in-r",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Full regression in R",
    "text": "Full regression in R\nPlaces all of the predictors in the model\nEquivalent of throwing everything in and hoping something sticks\n\n\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -142.4223     2.8728  -49.58  &lt; 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  &lt; 2e-16 ***\nP3p           74.0109     3.3449   22.13  &lt; 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  &lt; 2e-16 ***\nDREB           1.2033     0.0367   32.81  &lt; 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  &lt; 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#residuals",
    "href": "slides/Chapter07.html#residuals",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residuals",
    "text": "Residuals\nJust like with a simple regression we examine residuals to look for patterns\n\nThese look great, probably because we have a ton of data in this example."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity",
    "href": "slides/Chapter07.html#multicollinearity",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity is where at least two predictor variables are highly correlated.\nMulticollinearity does not affect the residual SD very much, and doesn’t pose a major problem for prediction.\nThe major effects of multicollinearity are:\n\nIt changes the estimates of the coefficients.\nIt inflates the variance of the estimates of the coefficients. That is, it increases the uncertainty about what the slope parameters are.\nTherefore, it matters when testing hypotheses about the effects of specific predictors."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-1",
    "href": "slides/Chapter07.html#multicollinearity-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nThe impact of multicollinearity on the variance of the estimates can be quantified using the Variance Inflation Factor (VIF &lt; 5 is considered ok).\nThere are several ways to deal with multicollinarity, depending on context. We can discard one of highly correlated variable, perform ridge regression, or think more carefully about how the variables relate to each other."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-2",
    "href": "slides/Chapter07.html#multicollinearity-2",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIn R we can examine Multicollinearity using the function vif()\nRemember our basketball regression:\n\nlibrary(car)\nvif(full_reg)\n\n PTS  P2p  P3p  FTp OREB DREB  AST  STL  BLK \n3.07 2.14 2.24 1.14 1.37 1.14 1.50 1.06 1.05 \n\n\nThese values are all small (VIF &lt; 5) so we can continue on in interpreting the regression."
  },
  {
    "objectID": "slides/Chapter07.html#tidy-summary",
    "href": "slides/Chapter07.html#tidy-summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Tidy summary",
    "text": "Tidy summary\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-142.422\n2.873\n-49.575\n0.000\n\n\nPTS\n0.062\n0.027\n2.320\n0.020\n\n\nP2p\n74.117\n3.839\n19.306\n0.000\n\n\nP3p\n74.011\n3.345\n22.126\n0.000\n\n\nFTp\n15.469\n2.027\n7.633\n0.000\n\n\nOREB\n0.719\n0.062\n11.538\n0.000\n\n\nDREB\n1.203\n0.037\n32.809\n0.000\n\n\nAST\n-0.035\n0.048\n-0.729\n0.466\n\n\nSTL\n1.069\n0.068\n15.785\n0.000\n\n\nBLK\n0.350\n0.078\n4.470\n0.000"
  },
  {
    "objectID": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "href": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Variance Explained (Review from Chapter 6)",
    "text": "Variance Explained (Review from Chapter 6)\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\n\\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)\nNow we have more sums of square to add in to our regression\n\n\nSS &lt;- anova(full_reg) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term   df    sumsq\n       PTS    1 141194.6\n       P2p    1   4001.7\n       P3p    1  14465.4\n       FTp    1    583.8\n      OREB    1   7082.2\n      DREB    1  78359.3\n       AST    1     10.1\n       STL    1  20082.0\n       BLK    1   1542.2\n Residuals 2108 162702.8\n     Total 2117 430024.0"
  },
  {
    "objectID": "slides/Chapter07.html#additional-variation-explained",
    "href": "slides/Chapter07.html#additional-variation-explained",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Additional variation explained",
    "text": "Additional variation explained\n\nVariation in \\(Y\\) is separated into two parts SSR and SSE.\n\nThe shaded overlap of two circles represent the variation in \\(Y\\) explained by the \\(X\\) variables.\n\nThe total overlap of \\(X_1\\) and \\(X_2\\), and \\(Y\\) depends on\n\nrelationship of \\(Y\\) with \\(X_1\\) and \\(X_2\\)\ncorrelation between \\(X_1\\) and \\(X_2\\)"
  },
  {
    "objectID": "slides/Chapter07.html#sequential-addition-of-predictors",
    "href": "slides/Chapter07.html#sequential-addition-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Sequential addition of predictors",
    "text": "Sequential addition of predictors\n\nAddition of variables decreases SSE and increases SSR and \\(R^2\\).\n\\(s^2\\) = MSE = SSE/df decreases to a minimum and then increases since addition of variable decreases SSE but adds to df."
  },
  {
    "objectID": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "href": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Significance of Type I or Seq.SS",
    "text": "Significance of Type I or Seq.SS\n\nThe Type I SS is the SS of a predictor after adjusting for the effects of the preceding predictors in the model.\n\nSometimes order matters, particularly with unequal sample sizes\n\nFor unbalanced data, this approach tests for a difference in the weighted marginal means. In practical terms, this means that the results are dependent on the realized sample sizes. In other words, it is testing the first factor without controlling for the other factor, which may not be the hypothesis of interest.\nF test for the significance of the additional variation explained\nR function anova() calculates sequential or Type-I SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-ii",
    "href": "slides/Chapter07.html#type-ii",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type II",
    "text": "Type II\n\nType II SS is based on the principle of marginality.\n\nEach variable effect is adjusted for all other appropriate effects.\n\nequivalent to the Type I SS when the variable is the last predictor entered the model.\n\nOrder matters for Type I SS but not for Type II SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-iii-ss",
    "href": "slides/Chapter07.html#type-iii-ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type III SS",
    "text": "Type III SS\n\nType III SS is the SS added to the regression SS after ALL other predictors including an interaction term.\nThis type tests for the presence of a main effect after other main effects and interaction. This approach is therefore valid in the presence of significant interactions.\nIf the interaction is significant SS for main effects should not be interpreted"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action",
    "href": "slides/Chapter07.html#ss-types-in-action",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\nConsider a model with terms A and B:\nType 1 SS:\nSS(A) for factor A.\nSS(B | A) for factor B.\nType 2 SS:\nSS(A | B) for factor A.\nSS(B | A) for factor B.\nType 3 SS:\nSS(A | B, AB) for factor A.\nSS(B | A, AB) for factor B.\n\nWhen data are balanced and the design is simple, types I, II, and III will give the same results.\nSS explained is not always a good criterion for selection of variables"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action-1",
    "href": "slides/Chapter07.html#ss-types-in-action-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\n\nanova(full_reg)\n\nAnalysis of Variance Table\n\nResponse: Spread\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nPTS          1 141195  141195 1829.34 &lt; 2e-16 ***\nP2p          1   4002    4002   51.85 8.3e-13 ***\nP3p          1  14465   14465  187.42 &lt; 2e-16 ***\nFTp          1    584     584    7.56   0.006 ** \nOREB         1   7082    7082   91.76 &lt; 2e-16 ***\nDREB         1  78359   78359 1015.23 &lt; 2e-16 ***\nAST          1     10      10    0.13   0.718    \nSTL          1  20082   20082  260.18 &lt; 2e-16 ***\nBLK          1   1542    1542   19.98 8.2e-06 ***\nResiduals 2108 162703      77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlibrary(car)\nAnova(full_reg,  type=2)\n\nAnova Table (Type II tests)\n\nResponse: Spread\n          Sum Sq   Df F value  Pr(&gt;F)    \nPTS          416    1    5.38    0.02 *  \nP2p        28767    1  372.71 &lt; 2e-16 ***\nP3p        37787    1  489.57 &lt; 2e-16 ***\nFTp         4497    1   58.26 3.4e-14 ***\nOREB       10276    1  133.13 &lt; 2e-16 ***\nDREB       83083    1 1076.43 &lt; 2e-16 ***\nAST           41    1    0.53    0.47    \nSTL        19232    1  249.17 &lt; 2e-16 ***\nBLK         1542    1   19.98 8.2e-06 ***\nResiduals 162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAnova(full_reg,  type=3)\n\nAnova Table (Type III tests)\n\nResponse: Spread\n            Sum Sq   Df F value  Pr(&gt;F)    \n(Intercept) 189696    1 2457.72 &lt; 2e-16 ***\nPTS            416    1    5.38    0.02 *  \nP2p          28767    1  372.71 &lt; 2e-16 ***\nP3p          37787    1  489.57 &lt; 2e-16 ***\nFTp           4497    1   58.26 3.4e-14 ***\nOREB         10276    1  133.13 &lt; 2e-16 ***\nDREB         83083    1 1076.43 &lt; 2e-16 ***\nAST             41    1    0.53    0.47    \nSTL          19232    1  249.17 &lt; 2e-16 ***\nBLK           1542    1   19.98 8.2e-06 ***\nResiduals   162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter07.html#summary-so-far",
    "href": "slides/Chapter07.html#summary-so-far",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary so far",
    "text": "Summary so far\n\nDefined multiple regression\nWhy we might run a multiple regression\nHow to perform a multiple regression, examine residuals, multicollinearity\nVariation and R squared\nSums of Squares types"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives-1",
    "href": "slides/Chapter07.html#learning-objectives-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nCautionary tales: when correlation misleads us\nHow to make models\nComparing models\nPrinciples of model selection"
  },
  {
    "objectID": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "href": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Does Waffle Houses cause divorce??",
    "text": "Does Waffle Houses cause divorce??"
  },
  {
    "objectID": "slides/Chapter07.html#or-is-it-butter",
    "href": "slides/Chapter07.html#or-is-it-butter",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Or is it butter?",
    "text": "Or is it butter?\n\nAnd if you want more to impress your friends at a BBQ, the source is: http://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/Chapter07.html#spurious-association",
    "href": "slides/Chapter07.html#spurious-association",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Spurious association",
    "text": "Spurious association\n\nYou’ve heard that before: correlation does not imply causation. BUT it doesn’t discard it either\nHope you are seated: causation does not imply correlation.\nCausation implies conditional correlation (up to linearity issues).\nWe need more than just statistical models to answer causal questions."
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "href": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with spurious associations?",
    "text": "How do we deal with spurious associations?\n\nDomain knowledge: you know that waffles and butter don’t cause divorce so why might they be correlated?\n\nIs there another predictor that would be better?\n\nMultiple regressions can disentangle the association between two predictors and an outcome\n\nStatistical control of a confound"
  },
  {
    "objectID": "slides/Chapter07.html#masked-associations",
    "href": "slides/Chapter07.html#masked-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masked associations",
    "text": "Masked associations\n\nAssociation between a predictor variable and an outcome can be masked by another variable.\nYou need to observe both variables to see the “true” influence of either on the outcome.\nHow do we account for the masking variable (seen as a nuisance)?"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "href": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situations tend to arise when:",
    "text": "Masking situations tend to arise when:\n\nBoth predictors are associated with one another.\nHave opposite relationships with the outcome."
  },
  {
    "objectID": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "href": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Headline: Higher ice cream sales increase shark attacks!",
    "text": "Headline: Higher ice cream sales increase shark attacks!\nWe’ll predict ice cream sales from the temperature and the number of shark attacks"
  },
  {
    "objectID": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "href": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "First we will consider simple regressions",
    "text": "First we will consider simple regressions"
  },
  {
    "objectID": "slides/Chapter07.html#single-regression",
    "href": "slides/Chapter07.html#single-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Single regression",
    "text": "Single regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression-1",
    "href": "slides/Chapter07.html#multiple-regression-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple regression",
    "text": "Multiple regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0\n\n\n\n\n\n\n\n\nTemperature and Sharks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.064\n0.000\n1.000\n\n\ntemp\n0.776\n0.094\n8.217\n0.000\n\n\nshark\n-0.008\n0.094\n-0.084\n0.933"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situation",
    "href": "slides/Chapter07.html#masking-situation",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situation",
    "text": "Masking situation\nTend to arise when\n\nBoth predictors are associated with one another.\nHave different relationships with the outcome"
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-this",
    "href": "slides/Chapter07.html#how-do-we-deal-with-this",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with this?",
    "text": "How do we deal with this?\n\nStatistically there is not really an answer\nThe answer lies in the causes and the causes are not in the data; Shark attacks dont cause ice cream sales or vice versa\nRemember that interpreting the (regression) parameter estimates always depends upon what you believe the causal model"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection",
    "href": "slides/Chapter07.html#model-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model Selection",
    "text": "Model Selection\n\nThe first step before selection of the best subset of predictors is to study the correlation matrix\nWe then perform stepwise additions (forward) or subtractions (backward) from the model and compare them\n\nBUT…\n\nWe saw with the illustration of SS how the significance or otherwise of a variable in a multiple regression model depends on the other variables in the model\nTherefore, we cannot fully rely on the t-test and discard a variable because its coefficient is insignificant"
  },
  {
    "objectID": "slides/Chapter07.html#selection-of-predictors",
    "href": "slides/Chapter07.html#selection-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Selection of predictors",
    "text": "Selection of predictors\n\nHeuristic (short-cut) procedures based on criteria such as \\(F\\), \\(R^2_{adj}\\), \\(AIC\\), \\(C_p\\) etc\n\nForward Selection: Add variables sequentially\n\nconvenient to obtain the simplest feasible model\n\nBackward Elimination: Drop variables sequentially\n\nIf difference between two variables is significant but not the variables themselves, forward regression would obtain the wrong model since both may not enter the model.\n\nKnown as suppressor variables case (like masking variables discussed earlier)\n\n\n\n\nExample: (try)\n\n\n\nCall:\nlm(formula = y ~ x1, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1691 -0.6791 -0.0033  0.6441  1.1299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.98876    1.26689    9.46  3.4e-07 ***\nx1           0.00375    0.41608    0.01     0.99    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.832 on 13 degrees of freedom\nMultiple R-squared:  6.24e-06,  Adjusted R-squared:  -0.0769 \nF-statistic: 8.11e-05 on 1 and 13 DF,  p-value: 0.993\n\n\n\nCall:\nlm(formula = y ~ x2, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0900 -0.6334  0.0002  0.6146  1.0403 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.632      0.811   13.11  7.2e-09 ***\nx2             0.195      0.113    1.74     0.11    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.75 on 13 degrees of freedom\nMultiple R-squared:  0.188, Adjusted R-squared:  0.126 \nF-statistic: 3.02 on 1 and 13 DF,  p-value: 0.106\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = suppressor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.01363 -0.00945 -0.00228  0.00863  0.01632 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.51541    0.06114   -73.8   &lt;2e-16 ***\nx1           3.09701    0.01227   252.3   &lt;2e-16 ***\nx2           1.03186    0.00368   280.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0107 on 12 degrees of freedom\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 3.92e+04 on 2 and 12 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#ockhams-razor",
    "href": "slides/Chapter07.html#ockhams-razor",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Ockham’s razor",
    "text": "Ockham’s razor\nNunca ponenda est pluralitas sine necesitate\n(Plurality should never be posited without necessity)\n\nProblem: fit to sample always (*multi-level models can be counter-examples) improves as we add parameters.\nDangers of “stargazing”: selecting variables with low p-values (aka ‘lots of stars’). P-values are not designed to cope with over-/under-fitting."
  },
  {
    "objectID": "slides/Chapter07.html#overfitting",
    "href": "slides/Chapter07.html#overfitting",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Overfitting",
    "text": "Overfitting\n\nOverfitting learning too much from the data, where you are almost just connecting the points rather than estimating.\nUnderfitting is the opposite, i.e. being insensitive to the data.\naka Models with fewer assumptions are to be preferred.\nIn practice, we have to choose between models that differ both in accuracy and simplicity. The razor is not really a useful guidance for this trade off.\n\nWe need tools"
  },
  {
    "objectID": "slides/Chapter07.html#all-possible-models",
    "href": "slides/Chapter07.html#all-possible-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "All possible models:",
    "text": "All possible models:\nAn exhaustive screening of all possible regression models can also be done using software\n\nBest Subsets: Stop at each step and check whether predictors, in the model or outside, are the best combination for that step.\n\n\ntime consuming to perform when the predictor set is large\n\n\nRemember permutations\nFor example:\nIf we fix the number of predictors as 3, then 20 regression models are possible"
  },
  {
    "objectID": "slides/Chapter07.html#what-criteria-do-we-use",
    "href": "slides/Chapter07.html#what-criteria-do-we-use",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What criteria do we use?",
    "text": "What criteria do we use?\nMultiple options\n- R squared\n- Sums of squared (different types, for testing predictor significance)\n- Information criteria (AIC, BIC etc)\n- For prediction: MSD/MSE, MAD, MAPE\n- \\(C_p\\)\n\nRemember its about balance and what you are looking for (fit vs prediction, complexity vs generality)\n\n\nNote\n\nIf a model stands out, it will perform well in terms of all summary measures.\nIf a model does not stand out, summary measures will contradict."
  },
  {
    "objectID": "slides/Chapter07.html#when-r2-becomes-absurd",
    "href": "slides/Chapter07.html#when-r2-becomes-absurd",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When \\(R^2\\) becomes absurd",
    "text": "When \\(R^2\\) becomes absurd"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-1",
    "href": "slides/Chapter07.html#model-selection-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection",
    "text": "Model selection\n\nResidual SD depends on its degrees of freedom\n\nSo comparison of models based on Residual SD is not fully fair\n\nThe following three measures are popular prediction modelling and similar to residual SD\nMean Squared Deviation (MSD): mean of the squared errors (i.e., deviations) (also called MSE)\n\n\\[\\frac{\\sum \\left({\\rm observation-fit}\\right)^{{\\rm 2}} }{{\\rm number~of~ observations}}\\]\n\nMean Absolute Deviation (MAD)\n\n\\[\\frac{\\sum \\left|{\\rm observation-fit}\\right| }{{\\rm number~of~observations}}\\]\n\nMean Absolute Percentage Error (MAPE)\n\n\\[\\frac{\\sum \\frac{\\left|{\\rm observation-fit}\\right|}{{\\rm observation}} }{{\\rm number~of~observations}} {\\times100}\\]"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-continued",
    "href": "slides/Chapter07.html#model-selection-continued",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection (continued)",
    "text": "Model selection (continued)\n\nAvoid over-fitting.\nSo place a penalty for excessive model parameters\nAkaike Information Criterion (AIC; smaller is better)\n\n\\[AIC  =  n\\log \\left(\\frac{SSE}{n} \\right) + 2p\\] - Bayesian Information Criterion (BIC) places a higher penalty that depends on, the number of observations.\n\nAs a result BIC fares well for selecting a model that explains the relationships well while AIC fares well when selecting a model for prediction purposes.\nOther variations: WAIC, AICc, etc (we will not cover them)"
  },
  {
    "objectID": "slides/Chapter07.html#software",
    "href": "slides/Chapter07.html#software",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Software",
    "text": "Software\n\nIn \\(R\\), lm() and step() function will perform the tasks\n\nleaps() and HH packages contain additional functions\ndredge() in MuMIn will produce all the subset models given a full model\nAlso MASS, car, caret, and SignifReg R packages\n\nR base package step-wise selection is based on \\(AIC\\) only."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "href": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validation (review from Chapter 6)",
    "text": "Cross validation (review from Chapter 6)\nIn sample error vs prediction error\n\nFor simpler models, increasing the number of parameters improves the fit to the sample.\nBut it seems to reduce the accuracy of the out-of-sample predictions.\nMost accurate models trade off flexibility (complexity) and overfitting\n\nGeneral idea: - Leave out some observations. - Train the model on the remaining samples; score on those left out. - Average over many left-out sets to get the out-of-sample (future) accuracy."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection-data-example",
    "href": "slides/Chapter07.html#cross-validated-selection-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection: Data example",
    "text": "Cross validated selection: Data example\nConsider the pinetree data set which contains the circumference measurements of pine trees at four positions (First is bottom)"
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection",
    "href": "slides/Chapter07.html#cross-validated-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection",
    "text": "Cross validated selection\n\nModel selection can be done focusing on prediction\n\nmethod = “leapForward” & method = “leapBackward” options\n\n\n\nlibrary(caret);  library(leaps)\nset.seed(123)\nfitControl  &lt;-  trainControl(method  =  \"repeatedcv\",\n                             number  =  5,  repeats  =  100)\nleapBackwardfit  &lt;-  train(Top  ~  .,  data  =  pinetree[, -1],\ntrControl  =  fitControl,  method  =  \"leapBackward\")\nsummary(leapBackwardfit)\n\nSubset selection object\n3 Variables  (and intercept)\n       Forced in Forced out\nThird      FALSE      FALSE\nSecond     FALSE      FALSE\nFirst      FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         Third Second First\n1  ( 1 ) \" \"   \" \"    \"*\"  \n2  ( 1 ) \"*\"   \" \"    \"*\""
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models",
    "href": "slides/Chapter07.html#polynomial-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nA polynomial model includes the square, cube of predictor variables as additional variables.\nHigh correlation (multicollinearity) between the predictor variables may be a problem in polynomial models, but not always."
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example",
    "href": "slides/Chapter07.html#polynomial-models-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\nWe can fit a simple linear regression using the Pine tree data\n\npine1 &lt;- lm(Top ~ First, data = pinetree) \nsummary(pine1)\n\n\nCall:\nlm(formula = Top ~ First, data = pinetree)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.854 -0.881 -0.195  0.630  3.176 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.334      0.765   -8.28  2.1e-11 ***\nFirst          0.763      0.024   31.78  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.29 on 58 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.945 \nF-statistic: 1.01e+03 on 1 and 58 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#look-closer",
    "href": "slides/Chapter07.html#look-closer",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Look closer",
    "text": "Look closer\n\nLooks non-linear"
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example-1",
    "href": "slides/Chapter07.html#polynomial-models-data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                         44.121      7.039    6.27        0\npoly(First, degree = 3, raw = T)1   -3.972      0.695   -5.71        0\npoly(First, degree = 3, raw = T)2    0.142      0.022    6.39        0\npoly(First, degree = 3, raw = T)3   -0.001      0.000   -5.95        0\n\n\n```         \n- For the pinetree example, all the slope coefficients are highly significant for the cubic regression\n- Not so for the quadratic regression\n```\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                           3.85      2.450   1.569    0.122\npoly(First, degree = 2, raw = T)1     0.10      0.155   0.646    0.521\npoly(First, degree = 2, raw = T)2     0.01      0.002   4.319    0.000\n\n\n\nRaw polynomials do not preserve the coefficient estimates but orthogonal polynomials do.\n\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.146  119.26        0\npoly(First, degree = 2)1    41.01      1.130   36.29        0\npoly(First, degree = 2)2     4.88      1.130    4.32        0\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.115  151.03        0\npoly(First, degree = 3)1    41.01      0.892   45.96        0\npoly(First, degree = 3)2     4.88      0.892    5.47        0\npoly(First, degree = 3)3    -5.31      0.892   -5.95        0"
  },
  {
    "objectID": "slides/Chapter07.html#residual-diagnostics",
    "href": "slides/Chapter07.html#residual-diagnostics",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nFor multiple regression fits, including polynomial fits, examine the residuals as usual to-\n- Validate the model assumptions\n- Look for model improvement clues\nQuadratic regression for pinetree data is not satisfactory based on the residual plots shown below:"
  },
  {
    "objectID": "slides/Chapter07.html#categorical-predictors",
    "href": "slides/Chapter07.html#categorical-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nModels can include categorical predictors such as Area in the pinetree dataset\nMake sure that you use the factor() function when numerical codes are assigned to categorical variables.\nArea effect on Top circumference is clear from the following plot"
  },
  {
    "objectID": "slides/Chapter07.html#indicator-variables",
    "href": "slides/Chapter07.html#indicator-variables",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nFactors are employed in a multiple regression using indicator variables which are simply binary variables taking either zero or one\nFor for males and females, indicator variables are defined as follows:\n\nIndicator variable of males: \\(~~~~~~~~\\begin{array}{cccc} I_{\\text {male}} & = & 1 & \\text{for males}\\\\ & & 0& \\text{for females} \\end{array}\\)\nIndicator variable of females \\(~~~~~~~~\\begin{array}{cccc} I_{\\text{female}} & = & 1 & \\text{for females}\\\\ & & 0& \\text{for males} \\end{array}\\)\n\nThere are three different areas of the forest in the pinetree dataset. So we can define three indicator variables.\nOnly two indicator variables are needed because there is only 2 degrees of freedom for the 3 areas."
  },
  {
    "objectID": "slides/Chapter07.html#regression-output",
    "href": "slides/Chapter07.html#regression-output",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Regression output",
    "text": "Regression output\n\n\n\nRegression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.02\n1.11\n17.98\n0.00\n\n\nI2\n-1.96\n1.57\n-1.24\n0.22\n\n\nI3\n-5.92\n1.57\n-3.76\n0.00\n\n\n\n\n\n\n\n\nThe y-intercept is the mean of the response for the omitted category\n\n20.02 is the mean Top circumference for the first Area\n\nslopes are the difference in the mean response\n\n-1.96 is the drop in the mean top circumference in Area 2 when compared to Area 1 (which is not a significant drop)\n-5.92 is the drop in the mean top circumference in Area 3 when compared to Area 1 (which is a highly significant drop)\n\n\nAnalysis of Covariance model employs both numerical and categorical predictors (covered later on).\n\nWe specifically include the interaction between them"
  },
  {
    "objectID": "slides/Chapter07.html#summary",
    "href": "slides/Chapter07.html#summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary",
    "text": "Summary\n\nRegression methods aim to fit a model by least squares to explain the variation in the dependent variable \\(Y\\) by fitting explanatory \\(X\\) variables.\nMatrix plots (EDA) and correlation coefficients provide important clues to the interrelationships.\nFor building a model, the additional variation explained is important. Summary criterion such as \\(AIC\\) is also useful.\nA model is not judged as the best purely on statistical grounds.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives",
    "href": "slides/Chapter08.html#learning-objectives",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine and understand an ANOVA\nUse a one-way ANOVA on example data\nUnderstand and calculate an ANOVA table\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs\nUnderstand the difference between one-way and two-way ANOVAs"
  },
  {
    "objectID": "slides/Chapter08.html#anova",
    "href": "slides/Chapter08.html#anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA",
    "text": "ANOVA\n\nAnalysis of variance, or ANOVA, is an approach to comparing data with multiple means across different groups, and allows us to see patterns and trends within complex and varied data.\nUsed for categorical or grouped data.\nOften data from experiments (treatments make good factors).\nEDA bar, points, or box plots are options to show differences between groups."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model\n\nfabric burn-time data\n\n\n\n\nfabric 1\nfabric 2\nfabric 3\nfabric 4\n\n\n\n\n17.8\n11.2\n11.8\n14.9\n\n\n16.2\n11.4\n11\n10.8\n\n\n17.5\n15.8\n10\n12.8\n\n\n17.4\n10\n9.2\n10.7\n\n\n15\n10.4\n9.2\n10.7\n\n\n\n\nCan we regard the mean burn times of the four fabrics as equal?\n\n\nfabric 1 seems to take longer time to burn\n\nStart with hypotheses and EDA to visualize groups"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-hypotheses",
    "href": "slides/Chapter08.html#one-way-anova-hypotheses",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA Hypotheses",
    "text": "One-way ANOVA Hypotheses\n\n\\(H_0\\): The mean burn times are equal for the four fabrics\n\\(H_a\\): The mean burn time of at least one fabric is different."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova",
    "href": "slides/Chapter08.html#one-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\nfabric=read.table(\"../data/fabric.txt\", header=TRUE, sep=\"\\t\")\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary(fun.data = \"mean_cl_normal\", colour = \"blue\", linewidth = 2, size = 3)\n\n\n\n\n\n\n\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary_bin(fun = \"mean\", geom = \"bar\", orientation = 'x')"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table",
    "href": "slides/Chapter08.html#anova-table",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\n\nobservation = mean + effect + error\n\n\nSS Total = SS Factor + SS Error\n\n\nsummary(aov(burntime ~ fabric, data = fabric))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfabric effect on burntime is highly significant.\n\nIn other words, the null hypothesis of equal mean burntime is rejected.\n\nOr alternatively the mean burntime is different for at least one fabric"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-1",
    "href": "slides/Chapter08.html#anova-table-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nHow are these values calculated?\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-2",
    "href": "slides/Chapter08.html#anova-table-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nCalculating F values of a factor:\n\\(F = \\frac{MS_{Factor}}{MS_{Error}}\\)\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-practice",
    "href": "slides/Chapter08.html#anova-table-practice",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table practice",
    "text": "ANOVA table practice\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\nMSF/MSE\n\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)\n\n\n\n\n\n\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nfabric\n120.5\n\n\n\n\n\n\nResiduals\n46.3\n\n\n\n\n\n\nTOTAL"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-p-values",
    "href": "slides/Chapter08.html#reminder-on-p-values",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on P values",
    "text": "Reminder on P values\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\nA very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\nWe calculate them based on our theoretical sampling distributions (normal, \\(t\\), \\(F\\), \\(\\chi^2\\))"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions\nA sampling distribution is a probabilistic model of sampling variation–it describes the behaviour of some sample statistic\nFor a normal population, when the population parameters and are known, we can easily derive the sampling distributions of the sample mean or sample variance.\nWhen the population parameters are unknown, we have to estimate them from data."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions"
  },
  {
    "objectID": "slides/Chapter08.html#graphical-comparison-of-means",
    "href": "slides/Chapter08.html#graphical-comparison-of-means",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphical comparison of means",
    "text": "Graphical comparison of means\n\nThe graph below shows individual 95% confidence intervals for the fabric means"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "href": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA model Assumptions",
    "text": "One-way ANOVA model Assumptions\n\nResiduals are randomly and normally distributed\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\nThe ANOVA model assumes equal SD for the treatments.\nIf experimental errors are more in some subgroups, divide the problem into separate ones.\nPositive correlation among residuals leads to under estimation of error variance; negative correlation leads to overestimation.\n\n\nThese assumptions are harder to validate to small experimental design data"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions",
    "href": "slides/Chapter08.html#visualize-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nplot(aov(burntime ~ fabric, data = fabric))"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions-1",
    "href": "slides/Chapter08.html#visualize-assumptions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nFigure 1: SD vs mean for four fabrics\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\n\nWith only four fabrics in the sample, it is difficult to make any definitive claim.\nIf the assumptions were valid, we would expect the four points to fall approximately along a horizontal band indicating constant standard deviations, and hence variances, regardless of the means of the groups.\nThis figure suggests that this is the case, so the assumption of equal variances appears to be valid."
  },
  {
    "objectID": "slides/Chapter08.html#equal-variance",
    "href": "slides/Chapter08.html#equal-variance",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Equal variance",
    "text": "Equal variance\nBartlett’s test: - null hypothesis: equal variances - but it has an assumption of its own (response variable must be normally distributed)\nLevene’s test - null hypothesis: equal variances - is applicable for any continuous distribution\n\nbartlett.test(burntime ~ fabric, data = fabric)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  burntime by fabric\nBartlett's K-squared = 2.6606, df = 3, p-value = 0.447\n\ncar::leveneTest(burntime ~ fabric, data = fabric)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.1788 0.9092\n      16"
  },
  {
    "objectID": "slides/Chapter08.html#what-if-the-equal-variance-assumption-is-violated",
    "href": "slides/Chapter08.html#what-if-the-equal-variance-assumption-is-violated",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What if the equal variance assumption is violated?",
    "text": "What if the equal variance assumption is violated?\nANOVA’s are considered to be fairly robust against violations of the equal variances assumption as long as each group has the same sample size.\nIf this assumption is violated, the most common way to deal with it is to transform the response variable using one of the three transformations:\n\nLog Transformation: Transform the response variable from y to log(y).\nSquare Root Transformation: Transform the response variable from y to √y.\nCube Root Transformation: Transform the response variable from y to y1/3.\n\nBy performing these transformations, the problem of heteroscedasticity typically goes away."
  },
  {
    "objectID": "slides/Chapter08.html#normality",
    "href": "slides/Chapter08.html#normality",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Normality",
    "text": "Normality\n\naov_fabric &lt;- aov(burntime ~ fabric, data = fabric)\nshapiro.test(aov_fabric$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov_fabric$residuals\nW = 0.88926, p-value = 0.02606\n\n\nANOVAs are robust to mild issues of non-normality\nBut if have issues with normality and unequal variance try transformations"
  },
  {
    "objectID": "slides/Chapter08.html#when-transformations-do-not-help",
    "href": "slides/Chapter08.html#when-transformations-do-not-help",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When transformations do not help:",
    "text": "When transformations do not help:\n\nWeighted least squares regression: This type of regression assigns a weight to each data point based on the variance of its fitted value.\n\nEssentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.\n\nNon-parametric test:\n\nKruskal-Wallis Test is the non-parametric version of a one-way ANOVA"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd",
    "href": "slides/Chapter08.html#tukey-hsd",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nTukey HSD (Honest Significant Differences) plot allows pairwise comparison of treatment means.\nWhich fabric types have different burn times? (remember the alternative hypothesis of a one-way ANOVA is at least one of the means are different)\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "href": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD (Interval Plot)",
    "text": "Tukey HSD (Interval Plot)"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova",
    "href": "slides/Chapter08.html#two-way-two-factor-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA",
    "text": "Two way (two factor) ANOVA\n\nTwo factors are present.\nA two-way ANOVA is used to estimate how the mean of a continuous variable changes according to the levels of two categorical variables.\nUse a two-way ANOVA when you want to know how two independent variables, in combination, affect a dependent variable.\nVery similar to multiple regression but with categorical variables."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "href": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA example",
    "text": "Two way (two factor) ANOVA example\n\nExample: We will use the built in data ToothGrowth. It contains data from a study evaluating the effect of vitamin C on tooth growth in Guinea pigs. The experiment has been performed on 60 pigs, where each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, (orange juice or ascorbic acid (a form of vitamin C and coded as VC). Tooth length was measured and a sample of the data is shown below.\n\n\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5"
  },
  {
    "objectID": "slides/Chapter08.html#run-a-two-way-anova",
    "href": "slides/Chapter08.html#run-a-two-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Run a Two-way ANOVA",
    "text": "Run a Two-way ANOVA\n\nsummary(aov(len~dose+supp, data=ToothGrowth))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2 2426.4  1213.2   82.81  &lt; 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA two-way ANOVA tests two null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit",
    "href": "slides/Chapter08.html#two-way-model-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nModel:\nObservation = Overall Mean + Factor 1 Effect + Factor 2 Effect + Error\n\n\n\\(H_0\\): factor 1 means are equal; factor 2 means are equal\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2 2426.4  1213.2   82.81  &lt; 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSupplement and dose effects are significant at 5% level."
  },
  {
    "objectID": "slides/Chapter08.html#main-effect-plots",
    "href": "slides/Chapter08.html#main-effect-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Main effect plots",
    "text": "Main effect plots\n\nSimply plot of response means for factor levels"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-1",
    "href": "slides/Chapter08.html#tukey-hsd-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nteeth &lt;- aov(len~supp+dose, data=ToothGrowth)\nTukeyHSD(teeth)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ supp + dose, data = ToothGrowth)\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.679762 -1.720238 0.0004293\n\n$dose\n          diff       lwr       upr p adj\nD1-D0.5  9.130  6.215909 12.044091 0e+00\nD2-D0.5 15.495 12.580909 18.409091 0e+00\nD2-D1    6.365  3.450909  9.279091 7e-06"
  },
  {
    "objectID": "slides/Chapter08.html#summary-so-far",
    "href": "slides/Chapter08.html#summary-so-far",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary so far",
    "text": "Summary so far\n\nANOVA: 1 and 2 factor\nANOVA table\nAssumptions\nTukey’s HSD"
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives-part-2",
    "href": "slides/Chapter08.html#learning-objectives-part-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives: Part 2",
    "text": "Learning Objectives: Part 2\n\nUnderstand the use of interactions in linear models\nUse a two-way ANOVA with interactions on example data\nUnderstand and calculate an ANOVA table when interactions are present\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs with interactions\nUnderstand the difference and similarities between this chapter and previous linear models covered\nReview non-parametric options"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-effect",
    "href": "slides/Chapter08.html#interaction-effect",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction effect",
    "text": "Interaction effect\n\nWhether Factor A effects are constant over Factor B effects or Factor B effects are constant over Factor A effects?\n\nIf the answer is no, then there is an interaction between A & B.\n\nExample:\n\nTemperature and pressure are factors affecting the yield in chemical experiments.\nThey do interact in a mechanistic sense.\n\n\nInteraction may or may not have physical meaning."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "href": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way ANOVA and Interaction effects",
    "text": "Two-way ANOVA and Interaction effects\nA two-way ANOVA with interaction tests three null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable\nThere is no interaction effect between the two variables"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plots",
    "href": "slides/Chapter08.html#interaction-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plots",
    "text": "Interaction Plots\n\nIn the absence of interaction, the plotted means of factor will be roughly parallel\n\nsee Plot 1. A & B do not interact.\n\nIf the the plotted means of factor crossings are far from parallel, then there is interaction\n\nPlot 2 shows extreme (antagonistic) interaction between A & B."
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "href": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plot for Zooplankton data",
    "text": "Interaction Plot for Zooplankton data\n\nlibrary(ggplot2)\np1= ggplot(data = ToothGrowth, aes(x = supp, y = len, group=dose, colour=dose)) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\n\np2= ggplot(data = ToothGrowth, aes(x = dose, y = len, group=supp, colour=supp)) +stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\nlibrary(patchwork)\np1+p2\n\n\n\nInteraction effect may be present"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-1",
    "href": "slides/Chapter08.html#two-way-model-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nobservation = mean + Factor A + Factor B + interaction effect + error\n\n\nThe above model is known as multiplicative model\nIf interaction effect is ignored, we deal with an additive model"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-2",
    "href": "slides/Chapter08.html#two-way-model-fit-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nToothGrowth |&gt; \n  aov(formula = len ~ supp * dose) |&gt; \n  summary()\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  15.572 0.000231 ***\ndose         2 2426.4  1213.2  92.000  &lt; 2e-16 ***\nsupp:dose    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#residual-diagnostics",
    "href": "slides/Chapter08.html#residual-diagnostics",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics"
  },
  {
    "objectID": "slides/Chapter08.html#interpreting-interactions",
    "href": "slides/Chapter08.html#interpreting-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions\n\nIf an interaction term is significant, the effect of Factor A depends on Factor B (and vice versa)\nUse interaction plots to visualize\nPosthoc tests to examine different level combinations\n\n\nTukeyHSD(modl)$`supp:dose`\n\n                 diff        lwr        upr        p adj\nVC:D0.5-OJ:D0.5 -5.25 -10.048124 -0.4518762 2.425209e-02\nOJ:D1-OJ:D0.5    9.47   4.671876 14.2681238 4.612304e-06\nVC:D1-OJ:D0.5    3.54  -1.258124  8.3381238 2.640208e-01\nOJ:D2-OJ:D0.5   12.83   8.031876 17.6281238 2.125153e-09\nVC:D2-OJ:D0.5   12.91   8.111876 17.7081238 1.769939e-09\nOJ:D1-VC:D0.5   14.72   9.921876 19.5181238 2.985978e-11\nVC:D1-VC:D0.5    8.79   3.991876 13.5881238 2.100948e-05\nOJ:D2-VC:D0.5   18.08  13.281876 22.8781238 4.855005e-13\nVC:D2-VC:D0.5   18.16  13.361876 22.9581238 4.821699e-13\nVC:D1-OJ:D1     -5.93 -10.728124 -1.1318762 7.393032e-03\nOJ:D2-OJ:D1      3.36  -1.438124  8.1581238 3.187361e-01\nVC:D2-OJ:D1      3.44  -1.358124  8.2381238 2.936430e-01\nOJ:D2-VC:D1      9.29   4.491876 14.0881238 6.908163e-06\nVC:D2-VC:D1      9.37   4.571876 14.1681238 5.774013e-06\nVC:D2-OJ:D2      0.08  -4.718124  4.8781238 1.000000e+00"
  },
  {
    "objectID": "slides/Chapter08.html#interpreting-interactions-1",
    "href": "slides/Chapter08.html#interpreting-interactions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions"
  },
  {
    "objectID": "slides/Chapter08.html#importance-of-interactions",
    "href": "slides/Chapter08.html#importance-of-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Importance of interactions",
    "text": "Importance of interactions\n\nWhen you have statistically significant interaction effects, you can’t interpret the main effects without considering the interactions\nInteractions can be used to control for unhelpful variation (blocking effect in experiments)\nInteractions can be mechanistically meaningful"
  },
  {
    "objectID": "slides/Chapter08.html#anova-extensions-and-inference",
    "href": "slides/Chapter08.html#anova-extensions-and-inference",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA extensions and inference",
    "text": "ANOVA extensions and inference\nLinear models can theoretically be performed with an infinite amount of predictors\nThe more predictors the more samples needed, your effect sample number is no longer your total sample size but the lowest treatment\nInteraction effects with more than 3 (sometimes even more than 2) become very difficult to interpret and visualize\nGood experimental design and causal thinking can aid in analysis design, design test then collect data, it is much harder the other way around"
  },
  {
    "objectID": "slides/Chapter08.html#indicator-variables",
    "href": "slides/Chapter08.html#indicator-variables",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nIndicator variables are used if the predictor is qualitative rather than quantitative.\n\nConsider gender, a categorical variable.\n\nLet \\(I_1\\) be an indicator variable that takes a value 1 for males and 0 for females.\n\nLet \\(I_2\\) takes 1 for females and 0 for males.\nNote only one of \\(I_1\\) & \\(I_2\\) is sufficient.\n\n\nThe minimum number of indicator variables needed is related to degrees of freedom."
  },
  {
    "objectID": "slides/Chapter08.html#anova-through-regression",
    "href": "slides/Chapter08.html#anova-through-regression",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA through regression",
    "text": "ANOVA through regression\n\nConsider the burn-time data for the four fabrics.\n\nThe four fabric types are categorical.\nDefine-\n\n\\(I_1\\) = 1 for fabric 1 and 0 otherwise\n\\(I_2\\) = 1 for fabric 2 and 0 otherwise\n\\(I_3\\) = 1 for fabric 3 and 0 otherwise\n\\(I_4\\) = 1 for fabric 4 and 0 otherwise\n\n\nNote that any THREE indicator variables are sufficient for the four fabrics.\n\n3 df for 4 fabrics"
  },
  {
    "objectID": "slides/Chapter08.html#regression-summary",
    "href": "slides/Chapter08.html#regression-summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression summary",
    "text": "Regression summary\n\nRegress the burn-time response on the indicator variables \\(I_1\\),\\(I_2\\), \\(I_3\\) & \\(I_4\\)\n\n\n\n   burntime   fabric I1 I2 I3 I4\n1      17.8 Fabric 1  1  0  0  0\n2      16.2 Fabric 1  1  0  0  0\n3      17.5 Fabric 1  1  0  0  0\n4      17.4 Fabric 1  1  0  0  0\n5      15.0 Fabric 1  1  0  0  0\n6      11.2 Fabric 2  0  1  0  0\n7      11.4 Fabric 2  0  1  0  0\n8      15.8 Fabric 2  0  1  0  0\n9      10.0 Fabric 2  0  1  0  0\n10     10.4 Fabric 2  0  1  0  0\n11     11.8 Fabric 3  0  0  1  0\n12     11.0 Fabric 3  0  0  1  0\n13     10.0 Fabric 3  0  0  1  0\n14      9.2 Fabric 3  0  0  1  0\n15      9.2 Fabric 3  0  0  1  0\n16     14.9 Fabric 4  0  0  0  1\n17     10.8 Fabric 4  0  0  0  1\n18     12.8 Fabric 4  0  0  0  1\n19     10.7 Fabric 4  0  0  0  1\n20     10.7 Fabric 4  0  0  0  1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova",
    "href": "slides/Chapter08.html#regression-and-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\n\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n\n\n\nCompare with the one-way output\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova-1",
    "href": "slides/Chapter08.html#regression-and-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\nfabric |&gt; lm(formula = burntime~I1+I2+I3) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: burntime\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nI1         1 111.521 111.521 38.5718 1.248e-05 ***\nI2         1   1.408   1.408  0.4871    0.4952    \nI3         1   7.569   7.569  2.6179    0.1252    \nResiduals 16  46.260   2.891                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCompare with the one-way output\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "href": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Analysis of Covariance (ANCOVA)",
    "text": "Analysis of Covariance (ANCOVA)\nAnalysis of covariance (ANCOVA) is a statistical method that combines linear regression and analysis of variance (ANOVA) to evaluate the relationship between a response variable and various independent variables while controlling for covariates.\n\nIndicator variables are used as additional regressors along with a quantitative predictor (covariate)."
  },
  {
    "objectID": "slides/Chapter08.html#data-example-test-anxiety",
    "href": "slides/Chapter08.html#data-example-test-anxiety",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data example: test anxiety",
    "text": "Data example: test anxiety\nResearchers investigated the effect of exercises in reducing the level of anxiety, where they measured the anxiety score of three groups of individuals practicing physical exercises at different levels (grp1: low, grp2: moderate and grp3: high).\nThe anxiety score was measured pre- and 6-months post-exercise training programs. It is expected that any reduction in the anxiety by the exercises programs would also depend on the participant’s basal level of anxiety score.\nIn this analysis we use the pretest anxiety score (pretest) as the covariate and are interested in possible differences between group with respect to the post-test anxiety scores.\n\n\n# A tibble: 3 × 4\n  id    group pretest posttest\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 15    grp1     19.8     19.4\n2 30    grp2     19.3     17.7\n3 33    grp3     15.5     11"
  },
  {
    "objectID": "slides/Chapter08.html#interaction",
    "href": "slides/Chapter08.html#interaction",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction",
    "text": "Interaction\nAre the regression slopes the same?\n\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \ngroup          2  72.13   36.07 203.973 &lt;2e-16 ***\npretest        1 101.29  101.29 572.828 &lt;2e-16 ***\ngroup:pretest  2   0.04    0.02   0.127  0.881    \nResiduals     39   6.90    0.18                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInteraction term is not significant (at sig. level of 5%)."
  },
  {
    "objectID": "slides/Chapter08.html#check-assumptions",
    "href": "slides/Chapter08.html#check-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Check assumptions",
    "text": "Check assumptions\nLinearity:\n\nAppears to be a linear relationship in each training group"
  },
  {
    "objectID": "slides/Chapter08.html#assumptions",
    "href": "slides/Chapter08.html#assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Assumptions",
    "text": "Assumptions\nResiduals:"
  },
  {
    "objectID": "slides/Chapter08.html#assumptions-1",
    "href": "slides/Chapter08.html#assumptions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Assumptions",
    "text": "Assumptions\nNormality:\n\n\n# A tibble: 1 × 3\n  variable                 statistic p.value\n  &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n1 summary(model$residuals)     0.964   0.849\n\n\nThe Shapiro Wilk test was not significant (\\(\\alpha = 0.05\\)), so we can assume normality of residuals\nEqual variance:\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  0.2604  0.772\n      42               \n\n\nThe Levene’s test was not significant (\\(\\alpha = 0.05\\)), so we can assume homogeneity of the residual variances for all groups."
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit",
    "href": "slides/Chapter08.html#ancova-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\nEstimates are the slope in each treatment (group). Remember that R assigns the first (alphabetical) level of the treatment to Intercept\n\n\n\nCall:\nlm(formula = posttest ~ pretest + group, data = anxiety)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00467 -0.29485 -0.02866  0.33846  0.68799 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.94088    0.72574  -1.296 0.202074    \npretest      1.02775    0.04202  24.461  &lt; 2e-16 ***\ngroupgrp2   -0.64112    0.15137  -4.235 0.000126 ***\ngroupgrp3   -2.98463    0.15027 -19.862  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4114 on 41 degrees of freedom\nMultiple R-squared:  0.9615,    Adjusted R-squared:  0.9587 \nF-statistic: 341.5 on 3 and 41 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit-1",
    "href": "slides/Chapter08.html#ancova-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\nUse ANOVA table to display\n\n\nAnalysis of Variance Table\n\nResponse: posttest\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npretest    1 99.400  99.400  587.16 &lt; 2.2e-16 ***\ngroup      2 74.023  37.011  218.63 &lt; 2.2e-16 ***\nResiduals 41  6.941   0.169                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPost-hoc tests to see which group is different\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = posttest ~ group + pretest, data = anxiety)\n\n$group\n               diff       lwr        upr p adj\ngrp2-grp1 -1.093333 -1.458662 -0.7280048     0\ngrp3-grp1 -3.060000 -3.425329 -2.6946715     0\ngrp3-grp2 -1.966667 -2.331995 -1.6013381     0"
  },
  {
    "objectID": "slides/Chapter08.html#data-example-fast-food-resturants",
    "href": "slides/Chapter08.html#data-example-fast-food-resturants",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data example: fast-food resturants",
    "text": "Data example: fast-food resturants\n\n\n Restaurant  Sales Households Location I1 I2\n          1 135.27        155  Highway  0  0\n          2  72.74         93  Highway  0  0\n          3 114.95        128  Highway  0  0\n          4 102.93        114  Highway  0  0\n          5 131.77        158  Highway  0  0\n          6 160.91        183  Highway  0  0\n          7 179.86        178     Mall  1  0\n          8 220.14        215     Mall  1  0\n          9 179.64        172     Mall  1  0\n         10 185.92        197     Mall  1  0\n         11 207.82        207     Mall  1  0\n         12 113.51         95     Mall  1  0\n         13 203.98        224   Street  0  1\n         14 174.48        199   Street  0  1\n         15 220.43        240   Street  0  1\n         16  93.19        100   Street  0  1\n\n\n\nDo we need three separate models?"
  },
  {
    "objectID": "slides/Chapter08.html#ancova",
    "href": "slides/Chapter08.html#ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA",
    "text": "ANCOVA\n\n\nIn order to allow for different slopes for each location, we define the product (or interaction) variables"
  },
  {
    "objectID": "slides/Chapter08.html#data-set-up",
    "href": "slides/Chapter08.html#data-set-up",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Data set up",
    "text": "Data set up\n\n\n  Restaurant  Sales Households Location I1 I2\n1          1 135.27        155  Highway  0  0\n2          2  72.74         93  Highway  0  0\n3          3 114.95        128  Highway  0  0\n4          4 102.93        114  Highway  0  0\n5          5 131.77        158  Highway  0  0\n6          6 160.91        183  Highway  0  0"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model",
    "href": "slides/Chapter08.html#ancova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\nWe examine the relationship between restaurant sales (response variable, in thousands of dollars) and the number of households (H) in the restaurant’s trading area and the location of the restaurant (Mall, Street, and Highway). We can use the indicator variables (\\(I_1\\) and \\(I_2\\)) to define our three locations uniquely.\n\\(Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\nThis model provides a separate model for each location as well as allows for the interaction between location of the restaurant and the number of households through the slope coefficient"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model-1",
    "href": "slides/Chapter08.html#ancova-model-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\n\\(Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\n\nFor Highway Locations: \\(I_1=0\\) & \\(I_2=0\\) hence our model simplifies to\n\\(Sales = \\beta_{0} + (\\beta_3)Households\\)\nFor Mall Locations: \\(I_1=1\\) & \\(I_2=0\\) so the model becomes\n\\(Sales = \\beta_{0} + \\beta_{1} + (\\beta_3 + \\beta_4)Households\\)\nFor Street Locations: \\(I_1=0\\) & \\(I_2=1\\) so the model becomes \\(Sales = \\beta_{0} + \\beta_2 + (\\beta_3 + \\beta_5) Households\\)\n\nnote R does not require us to code Location as an indicator variable"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit-2",
    "href": "slides/Chapter08.html#ancova-fit-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\n\nmodl = lm(Sales~Households*Location, data = restaurant)\nouts &lt;- summary(modl)$coefficients\nround(outs[, 1:4], digits = 3)\n\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 -6.203     11.818  -0.525    0.611\nHouseholds                   0.909      0.083  10.906    0.000\nLocationMall                39.223     16.451   2.384    0.038\nLocationStreet               8.036     16.273   0.494    0.632\nHouseholds:LocationMall     -0.074      0.104  -0.710    0.494\nHouseholds:LocationStreet   -0.012      0.101  -0.120    0.907\n\n\nWe can plug these numbers into our equations \\(Sales = \\beta_{0} + \\beta_{1} I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households\\)\n\\(Sales = -6.2 + 39.22 + 8.04 + ( 0.909 -0.074 -0.012) Households\\)"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-model-2",
    "href": "slides/Chapter08.html#ancova-model-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA model",
    "text": "ANCOVA model\n\\(Sales = -6.2 + 39.22 + 8.04 + ( 0.909 -0.074 -0.012) Households\\)\n\nFor Highway Locations: \\(I_1=0\\) & \\(I_2=0\\) hence our model simplifies to\n\\(Sales = -6.2 + (0.909)Households\\)\nFor Mall Locations: \\(I_1=1\\) & \\(I_2=0\\) so the model becomes\n\\(Sales = -6.2 + 39.22 + (0.909 -0.074)Households\\)\nFor Street Locations: \\(I_1=0\\) & \\(I_2=1\\) so the model becomes \\(Sales = -6.2 + 8.04 + (0.909 -0.012) Households\\)"
  },
  {
    "objectID": "slides/Chapter08.html#graphing-the-model",
    "href": "slides/Chapter08.html#graphing-the-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphing the model",
    "text": "Graphing the model"
  },
  {
    "objectID": "slides/Chapter08.html#summary",
    "href": "slides/Chapter08.html#summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary",
    "text": "Summary\nANOVA models study categorical predictors (factors).\n-   Interaction between factors is important. \n-   ANOVA models and regression models are related and fall under a general family of linear models.\nANCOVA models employs both numerical variables (covariates) and qualitative factors for modelling.\n-   Interaction between factors and covariates is important."
  },
  {
    "objectID": "slides/Chapter08.html#review-of-non-parametric-tests",
    "href": "slides/Chapter08.html#review-of-non-parametric-tests",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Review of non-parametric tests",
    "text": "Review of non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations).\nMany non-parametric methods rely on replacing the observed data by their ranks."
  },
  {
    "objectID": "slides/Chapter08.html#spearmans-rank-correlation",
    "href": "slides/Chapter08.html#spearmans-rank-correlation",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\n\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient.\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\nlibrary(GGally)\n\np &lt;- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n\n\n\n\n\n\n\nFigure 2: Comparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter08.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter08.html#wilcoxon-signed-rank-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\nt.test(tv$TELETIME, mu=1680)\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter08.html#non-parametric-anova",
    "href": "slides/Chapter08.html#non-parametric-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Non-parametric ANOVA",
    "text": "Non-parametric ANOVA\nKruskal-Wallis test: allows to compare three or more groups\nMann-Whitney test: allows to compare 2 groups under the non-normality assumption."
  },
  {
    "objectID": "slides/Chapter08.html#kruskal-wallis-test",
    "href": "slides/Chapter08.html#kruskal-wallis-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Kruskal-Wallis test",
    "text": "Kruskal-Wallis test\nThe null and alternative hypotheses of the Kruskal-Wallis test are:\nH0: The 3 groups are equal in terms of the variable H1: At least one group is different from the other 2 groups in terms of variable\nSimilar to an ANOVA the alternative hypothesis is not that all groups are different. The opposite of all groups being equal (H0) is that at least one group is different from the others (H1).\nIn this sense, if the null hypothesis is rejected, it means that at least one group is different from the other 2, but not necessarily that all 3 groups are different from each other. Post-hoc tests must be performed to test whether all 3 groups differ."
  },
  {
    "objectID": "slides/Chapter08.html#mann-whitney-test",
    "href": "slides/Chapter08.html#mann-whitney-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\nload(\"../data/rangitikei.Rdata\")\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\nt.test(rangitikei$people~rangitikei$time)\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter08.html#another-form-of-test",
    "href": "slides/Chapter08.html#another-form-of-test",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Another form of test",
    "text": "Another form of test\n\n\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter08.html#linear-models-review",
    "href": "slides/Chapter08.html#linear-models-review",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Linear models Review",
    "text": "Linear models Review\nFamily tree\nT-tests, regressions, ANOVAs, and ANCOVAs are related\nSimilarities:\n-   1 continuous response variable\n-   Assumptions: normality, equal variance, and independence of residuals\nDifferences:\n- EDA\n- test statistics\n- number and type of predictors"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nA t-test is used to determine whether or not there is a statistically significant difference between the means of two groups\n\nsample group vs hypothetical population One-sample t-test\ntwo independent samples Two-sample t-test\n\nalso called independent t-test because the two samples are considered independent\n\nsamples linked in some why, not independent Paired t-test\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nThe main difference between a t-test and an ANOVA is in how the two tests calculate their test statistic to determine if there is a statistically significant difference between groups.\nT-test:\n\nT statistic: ratio of the mean difference between two groups relative to overal standard deviation of the differences\nOne-sample t-test: \\(t = \\frac{x-\\mu}{s/\\sqrt n}\\)\nTwo-sample t-test: \\(t = \\frac{(\\bar{x_{1}}-\\bar{x_{2}})-d}{\\frac{s_1}{\\sqrt n} + \\frac{s_2}{\\sqrt n}}\\)\n\nPaired t-test: \\(t=\\frac{\\bar{d}}{\\frac{S_{d}}{\\sqrt n}}\\)\n\n\\(s =\\) sample standard deviation \\(d =\\) difference\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\) Where \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\nMSF/MSE"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a t.test or an ANOVA?",
    "text": "When to use a t.test or an ANOVA?\nIn practice, when we want to compare the means of two groups, we use a t-test. When we want to compare the means of three or more groups, we use an ANOVA.\nSuppose we have three groups we wish to compare the means between: group A, group B, and group C.\nIf you did the following t-tests:\n\nA t-test to compare the difference in means between group A and group B\nA t-test to compare the difference in means between group A and group C\nA t-test to compare the difference in means between group B and group C"
  },
  {
    "objectID": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "href": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What if we just did many t-tests?",
    "text": "What if we just did many t-tests?\nFor each t-test there is a chance that we will commit a type I error, which is the probability that we reject the null hypothesis when it is actually true. Let’s say we set this to 0.05 (\\(\\alpha = 0.05\\)). This means that when we perform multiple t-tests, this error rate increases.\n\nThe probability that we commit a type I error with one t-test is 1 – 0.95 = 0.05.\nThe probability that we commit a type I error with two t-tests is 1 – (0.95^2) = 0.0975.\nThe probability that we commit a type I error with three t-tests is 1 – (0.95^3) = 0.1427.\n\nThe type I error just increases!\nWe use a post-hoc (after) test to see which group is driving differences, these pairwise comparisons have a correction factor to adjust our Type I error"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nA regression is used to understand the relationship between predictor variable(s) and a response variable\n\none predictor Simple Regression\ntwo or more predictors Multiple Regression\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…\n\nNot too much difference there"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nRegression:\n\nT statistic: test if each predictor’s estimate is different from 0\n\\(R^2\\): proportion of variance explained \\(\\frac{SSR}{SST}\\)\nF statistic: overall F statistic for the regression model, \\(\\frac{MS_{reg}}{MS_{error}}\\) where \\(MS = \\frac{SS}{df}\\)\n\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\)\n\nwhere \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\n\nMSF/MSE\n\nThe F statistic is the same, thats why you can produce an ANOVA table from your regression"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a regression or an ANOVA?",
    "text": "When to use a regression or an ANOVA?\nConventionally, a regression refers to continuous predictors and ANOVAs are used for discrete variables.\nBUT …\nYou can code discrete variable as indicator variables and then treat them as continuous and run a regression!\nGenerally, if you have 1 continuous and 1 discrete variable you should generate indicator variables and run a multiple regression\nIf you are using the continuous predictor as a covariate it would be called an ANCOVA.\nYou can code a discrete variable into continuous but if you do the reverse you lose statistical inference."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-hypotheses",
    "href": "slides/Chapter08.html#reminder-on-hypotheses",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on Hypotheses",
    "text": "Reminder on Hypotheses\nT-Test: difference between the means of two groups\n\nNull: \\(\\mu_{1} - \\mu_{2}\\) is equal to \\(0\\) Can be \\(=\\), \\(\\le\\), or \\(\\ge\\)\nAlt: \\(\\mu_{1} - \\mu_{2}\\) is not equal to \\(0\\) Can be \\(\\ne\\), \\(&lt;\\), or \\(&gt;\\)\n\nRegressions: understand the relationship between predictor variable(s) and a response variable\n\nNull: true slope coefficient is \\(= 0\\) (i.e. no relationship)\nAlt: true slope coefficient \\(\\ne 0\\) (i.e. relationship)\n\nANOVA: difference between the means of three or more groups\n\nNull: group means are equal\nAlt: At least one mean is different"
  },
  {
    "objectID": "slides/Chapter08.html#where-to-go-from-here",
    "href": "slides/Chapter08.html#where-to-go-from-here",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nLinear models form the basis of a lot more statistics tests.\n\n\n\n161250 Data Analysis"
  }
]